{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from itertools import chain\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from crossvalidation import multilabel_label_combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, confusion_matrix\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, InputLayer, Embedding, Conv1D, LSTM, Bidirectional, GlobalMaxPool1D, Add, Dropout, Dense\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain = pd.read_csv(\"input/train.csv\")\n",
    "dftrain['comment_text'] = dftrain['comment_text'].apply(str) # some values parsed as float\n",
    "dftest = pd.read_csv(\"input/test.csv\")\n",
    "dftest['comment_text'] = dftest['comment_text'].apply(str) # some values parsed as float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  toxic  \\\n",
       "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
       "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
       "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
       "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4  79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6044863</td>\n",
       "      <td>==Orphaned non-free media (Image:41cD1jboEvL. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6102620</td>\n",
       "      <td>::Kentuckiana is colloquial.  Even though the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14563293</td>\n",
       "      <td>Hello fellow Wikipedians,\\nI have just modifie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21086297</td>\n",
       "      <td>AKC Suspensions \\nThe Morning Call - Feb 24, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22982444</td>\n",
       "      <td>== [WIKI_LINK: Talk:Celts] ==</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text\n",
       "0   6044863  ==Orphaned non-free media (Image:41cD1jboEvL. ...\n",
       "1   6102620  ::Kentuckiana is colloquial.  Even though the ...\n",
       "2  14563293  Hello fellow Wikipedians,\\nI have just modifie...\n",
       "3  21086297  AKC Suspensions \\nThe Morning Call - Feb 24, 2...\n",
       "4  22982444                      == [WIKI_LINK: Talk:Celts] =="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    delimeter = \"([?\\\\/.,`~!@#4%^&*()-+\\[\\]{}<>'\\\"]*[ \\s\\n\\t\\r]+)\"\n",
    "    tokens = re.split(delimeter, text + \" \")\n",
    "    stripped_tokens = map(str.strip, tokens)\n",
    "    noempty_tokens = filter(bool, stripped_tokens)\n",
    "    return list(noempty_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    return \" \".join(tokenize(text.replace(\"'ll\", \" will\").replace(\"n't\", \" not\")))\n",
    "\n",
    "\n",
    "def preprocess_texts(texts):\n",
    "    return [preprocess_text(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain['preprocessed_text'] = preprocess_texts(dftrain['comment_text'])\n",
    "dftest['preprocessed_text'] = preprocess_texts(dftest['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Nonsense ? kiss off , geek . what I said is tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" Please do not vandalize pages , as you did w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" \"\"Points of interest \"\" I removed the \"\"poin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  toxic  \\\n",
       "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
       "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
       "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
       "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4  79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  Nonsense ? kiss off , geek . what I said is tr...  \n",
       "1  \" Please do not vandalize pages , as you did w...  \n",
       "2  \" \"\"Points of interest \"\" I removed the \"\"poin...  \n",
       "3  Asking some his nationality is a Racial offenc...  \n",
       "4  The reader here is not going by my say so for ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_idx():\n",
    "    def get_label_combination_indices():\n",
    "        labels = np.array(dftrain[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']])\n",
    "        label_combinations = multilabel_label_combinations(labels, 2)\n",
    "        label_combination_indices = np.zeros([len(dftrain)])\n",
    "        for i, row in enumerate(label_combinations):\n",
    "            idx = np.all(labels == row, axis=1)\n",
    "            label_combination_indices[idx] = i\n",
    "        return label_combination_indices\n",
    "\n",
    "    label_combination_indices = get_label_combination_indices()\n",
    "    train_idx, val_idx, _, _ = train_test_split(np.arange(len(dftrain), dtype=np.int), \n",
    "                                                label_combination_indices, \n",
    "                                                stratify=label_combination_indices,\n",
    "                                                random_state=42)\n",
    "    \n",
    "    return train_idx, val_idx\n",
    "\n",
    "train_idx, val_idx = get_train_val_idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fasttext-train.txt', 'w', encoding='utf-8') as target:\n",
    "    for text in list(dftrain['preprocessed_text']) + list(dftest['preprocessed_text']):\n",
    "        target.write(\"__label__0__\\t{0}\\n\".format(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fasttext skipgram -input fasttext-train.txt -output fasttext-vector-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordset = set(chain(*map(lambda val: val.split(' '), dftrain['preprocessed_text']))) | set(chain(*map(lambda val: val.split(' '), dftest['preprocessed_text'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fasttext-words.txt', 'w', encoding='utf-8') as target:\n",
    "    for word in wordset:\n",
    "        target.write(\"{0}\\n\".format(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fasttext print-word-vectors fasttext-vector-model.bin < fasttext-words.txt > fasttext-word-vectors.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "word2index = {}\n",
    "with open('fasttext-word-vectors.txt', 'r', encoding='utf-8') as src:\n",
    "    for row in filter(lambda row: len(row) > 0, map(lambda line: line.strip().split(' '), src)):\n",
    "        word = row[0]\n",
    "        vector = np.fromiter(map(float, row[1:]), dtype=np.float)\n",
    "        word2index[word] = len(word2index)\n",
    "        vectors.append(vector)\n",
    "vectors = np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(text):\n",
    "    sequence = [word2index[word] for word in text.split(' ') if word in word2index]\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def texts_to_sequence(texts):\n",
    "    return [text_to_sequence(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = texts_to_sequence(np.array(dftrain['preprocessed_text'])[train_idx])\n",
    "val_sequences = texts_to_sequence(np.array(dftrain['preprocessed_text'])[val_idx])\n",
    "test_sequences = texts_to_sequence(np.array(dftest['preprocessed_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 100\n",
    "train_X = np.array(pad_sequences(train_sequences, maxlen=MAXLEN))\n",
    "val_X = np.array(pad_sequences(val_sequences, maxlen=MAXLEN))\n",
    "test_X = np.array(pad_sequences(test_sequences, maxlen=MAXLEN))\n",
    "del train_sequences\n",
    "del val_sequences\n",
    "del test_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array(dftrain[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']])[train_idx]\n",
    "val_y = np.array(dftrain[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']])[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(721620, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(trainable=False):\n",
    "    embedding = Embedding(len(word2index), 100, weights=[vectors], trainable=trainable)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion(y_true, y_pred):\n",
    "    false_positive_count = np.logical_and(y_true == 0, y_pred == 1).sum()\n",
    "    false_negative_count = np.logical_and(y_true == 1, y_pred == 0).sum()\n",
    "    true_positive_count = np.logical_and(y_true == 1, y_pred == 1).sum()\n",
    "    true_negative_count = np.logical_and(y_true == 0, y_pred == 0).sum()\n",
    "    \n",
    "    false_positive_rate = false_positive_count / (false_positive_count + true_negative_count)\n",
    "    false_negative_rate = false_negative_count / (false_negative_count + true_positive_count)\n",
    "    true_negative_rate = true_negative_count / (false_positive_count + true_negative_count)\n",
    "    true_positive_rate = true_positive_count / (false_negative_count + true_positive_count)\n",
    "    \n",
    "    return np.array([\n",
    "        [true_negative_rate, false_positive_rate],\n",
    "        [false_negative_rate, true_positive_rate],\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_4 (Embedding)         (None, 100, 100)     72162000    input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_10 (Conv1D)              (None, 100, 50)      5050        embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_11 (Conv1D)              (None, 99, 50)       10050       embedding_4[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_10 (Global (None, 50)           0           conv1d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_11 (Global (None, 50)           0           conv1d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 50)           0           global_max_pooling1d_10[0][0]    \n",
      "                                                                 global_max_pooling1d_11[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_7 (Dropout)             (None, 50)           0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_7 (Dense)                 (None, 100)          5100        dropout_7[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_8 (Dropout)             (None, 100)          0           dense_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_8 (Dense)                 (None, 6)            606         dropout_8[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 72,182,806\n",
      "Trainable params: 20,806\n",
      "Non-trainable params: 72,162,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    input = Input(shape=(MAXLEN,), dtype='int32')\n",
    "    embedding = get_embedding() (input)\n",
    "    \n",
    "    conv1 = Conv1D(50, 1, activation='relu') (embedding)\n",
    "    pool1 = GlobalMaxPool1D() (conv1)\n",
    "    \n",
    "    conv2 = Conv1D(50, 2, activation='relu') (embedding)\n",
    "    pool2 = GlobalMaxPool1D() (conv2)\n",
    "    \n",
    "    merge = Add() ([pool1, pool2])\n",
    "    drop1 = Dropout(0.3) (merge)\n",
    "    fc1 = Dense(100, activation='relu') (drop1)\n",
    "    drop2 = Dropout(0.3) (fc1)\n",
    "    fc2 = Dense(6, activation='sigmoid') (drop2)\n",
    "    \n",
    "    model = Model(input, fc2)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam', 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71888 samples, validate on 23963 samples\n",
      "Epoch 1/200\n",
      "71888/71888 [==============================] - 33s 464us/step - loss: 0.0764 - val_loss: 0.0548\n",
      "Epoch 2/200\n",
      "71888/71888 [==============================] - 30s 414us/step - loss: 0.0574 - val_loss: 0.0526\n",
      "Epoch 3/200\n",
      "71888/71888 [==============================] - 28s 394us/step - loss: 0.0544 - val_loss: 0.0520\n",
      "Epoch 4/200\n",
      "71888/71888 [==============================] - 30s 419us/step - loss: 0.0529 - val_loss: 0.0522\n",
      "Epoch 5/200\n",
      "71888/71888 [==============================] - 32s 447us/step - loss: 0.0515 - val_loss: 0.0516\n",
      "Epoch 6/200\n",
      "71888/71888 [==============================] - 32s 449us/step - loss: 0.0502 - val_loss: 0.0509\n",
      "Epoch 7/200\n",
      "71888/71888 [==============================] - 27s 378us/step - loss: 0.0494 - val_loss: 0.0513\n",
      "Epoch 8/200\n",
      "71888/71888 [==============================] - 30s 416us/step - loss: 0.0488 - val_loss: 0.0509\n",
      "Epoch 9/200\n",
      "71888/71888 [==============================] - 28s 396us/step - loss: 0.0479 - val_loss: 0.0503\n",
      "Epoch 10/200\n",
      "71888/71888 [==============================] - 27s 372us/step - loss: 0.0475 - val_loss: 0.0508\n",
      "Epoch 11/200\n",
      "71888/71888 [==============================] - 27s 378us/step - loss: 0.0468 - val_loss: 0.0512\n",
      "Epoch 12/200\n",
      "71888/71888 [==============================] - 27s 381us/step - loss: 0.0463 - val_loss: 0.0504\n",
      "Epoch 13/200\n",
      "71888/71888 [==============================] - 28s 391us/step - loss: 0.0458 - val_loss: 0.0506\n",
      "Epoch 14/200\n",
      "71888/71888 [==============================] - 26s 362us/step - loss: 0.0460 - val_loss: 0.0509\n",
      "Epoch 15/200\n",
      "71888/71888 [==============================] - 23s 318us/step - loss: 0.0451 - val_loss: 0.0516\n",
      "Epoch 16/200\n",
      "71888/71888 [==============================] - 28s 391us/step - loss: 0.0447 - val_loss: 0.0527\n",
      "Epoch 17/200\n",
      "71888/71888 [==============================] - 28s 385us/step - loss: 0.0442 - val_loss: 0.0528\n",
      "Epoch 18/200\n",
      "71888/71888 [==============================] - 27s 381us/step - loss: 0.0441 - val_loss: 0.0513\n",
      "Epoch 19/200\n",
      "71888/71888 [==============================] - 27s 372us/step - loss: 0.0436 - val_loss: 0.0526\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2337e9afa90>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_y, \n",
    "          epochs=200,\n",
    "          validation_data=(val_X, val_y), \n",
    "          verbose=True, \n",
    "          callbacks=[\n",
    "              ModelCheckpoint('model-conv.h5', save_best_only=True),\n",
    "              EarlyStopping(patience=10),\n",
    "          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23963/23963 [==============================] - 3s 116us/step\n",
      "toxic\n",
      "loss:  0.103945565169\n",
      "[[ 0.98794569  0.01205431]\n",
      " [ 0.29684119  0.70315881]]\n",
      "severe_toxic\n",
      "loss:  0.0248819026766\n",
      "[[  9.99831387e-01   1.68612739e-04]\n",
      " [  9.87500000e-01   1.25000000e-02]]\n",
      "obscene\n",
      "loss:  0.0600789274654\n",
      "[[ 0.99391695  0.00608305]\n",
      " [ 0.32498042  0.67501958]]\n",
      "threat\n",
      "loss:  0.0126833524881\n",
      "[[ 1.  0.]\n",
      " [ 1.  0.]]\n",
      "insult\n",
      "loss:  0.0743594396471\n",
      "[[ 0.99047034  0.00952966]\n",
      " [ 0.43036913  0.56963087]]\n",
      "identity_hate\n",
      "loss:  0.0257490880308\n",
      "[[  9.99957909e-01   4.20910851e-05]\n",
      " [  9.36585366e-01   6.34146341e-02]]\n",
      "Total loss:  0.0502830459129\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('model-conv.h5')\n",
    "\n",
    "def val_result(model):\n",
    "    val_prediction = model.predict(val_X, verbose=True)\n",
    "    losses = []\n",
    "    for i, label in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n",
    "        print(label)\n",
    "        loss = log_loss(val_y[:, i], val_prediction[:, i])\n",
    "        losses.append(loss)\n",
    "        print('loss: ', loss)\n",
    "        print(confusion(val_y[:, i], 1.0 * (val_prediction[:, i] > 0.5)))\n",
    "    print('Total loss: ', np.array(losses).mean())\n",
    "    \n",
    "val_result(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    eps = 1e-10\n",
    "    toxic_pos_weight         = 1.0\n",
    "    toxic_neg_weight         = 1.0\n",
    "    severe_toxic_pos_weight  = 1.5\n",
    "    severe_toxic_neg_weight  = 1.0\n",
    "    obscene_pos_weight       = 1.0\n",
    "    obscene_neg_weight       = 1.0\n",
    "    threat_pos_weight        = 1.5\n",
    "    threat_neg_weight        = 1.0\n",
    "    insult_pos_weight        = 1.0\n",
    "    insult_neg_weight        = 1.0\n",
    "    identity_hate_pos_weight = 1.5\n",
    "    identity_hate_neg_weight = 1.0\n",
    "    toxic         =         toxic_pos_weight *      y_true[:, 0]  * K.log(    y_pred[:, 0] + eps) + \\\n",
    "                            toxic_neg_weight * (1 - y_true[:, 0]) * K.log(1 - y_pred[:, 0] + eps)\n",
    "    severe_toxic  =  severe_toxic_pos_weight *      y_true[:, 1]  * K.log(    y_pred[:, 1] + eps) + \\\n",
    "                     severe_toxic_neg_weight * (1 - y_true[:, 1]) * K.log(1 - y_pred[:, 1] + eps)\n",
    "    obscene       =       obscene_pos_weight *      y_true[:, 2]  * K.log(    y_pred[:, 2] + eps) + \\\n",
    "                          obscene_neg_weight * (1 - y_true[:, 2]) * K.log(1 - y_pred[:, 2] + eps)\n",
    "    threat        =        threat_pos_weight *      y_true[:, 3]  * K.log(    y_pred[:, 3] + eps) + \\\n",
    "                           threat_neg_weight * (1 - y_true[:, 3]) * K.log(1 - y_pred[:, 3] + eps)\n",
    "    insult        =        insult_pos_weight *      y_true[:, 4]  * K.log(    y_pred[:, 4] + eps) + \\\n",
    "                           insult_neg_weight * (1 - y_true[:, 4]) * K.log(1 - y_pred[:, 4] + eps)\n",
    "    identity_hate = identity_hate_pos_weight *      y_true[:, 5]  * K.log(    y_pred[:, 5] + eps) + \\\n",
    "                    identity_hate_neg_weight * (1 - y_true[:, 5]) * K.log(1 - y_pred[:, 5] + eps)\n",
    "    return - (toxic + severe_toxic + obscene + threat + insult + identity_hate) / 6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_6 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_6 (Embedding)         (None, 100, 100)     72162000    input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_12 (Conv1D)              (None, 100, 50)      5050        embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_13 (Conv1D)              (None, 99, 50)       10050       embedding_6[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_12 (Global (None, 50)           0           conv1d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_13 (Global (None, 50)           0           conv1d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 50)           0           global_max_pooling1d_12[0][0]    \n",
      "                                                                 global_max_pooling1d_13[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)             (None, 50)           0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_9 (Dense)                 (None, 100)          5100        dropout_9[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)            (None, 100)          0           dense_9[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 6)            606         dropout_10[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 72,182,806\n",
      "Trainable params: 20,806\n",
      "Non-trainable params: 72,162,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('adam', custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71888 samples, validate on 23963 samples\n",
      "Epoch 1/200\n",
      "71888/71888 [==============================] - 38s 525us/step - loss: 0.0851 - val_loss: 0.0600\n",
      "Epoch 2/200\n",
      "71888/71888 [==============================] - 32s 452us/step - loss: 0.0619 - val_loss: 0.0589\n",
      "Epoch 3/200\n",
      "71888/71888 [==============================] - 34s 473us/step - loss: 0.0589 - val_loss: 0.0554\n",
      "Epoch 4/200\n",
      "71888/71888 [==============================] - 33s 464us/step - loss: 0.0560 - val_loss: 0.0549\n",
      "Epoch 5/200\n",
      "71888/71888 [==============================] - 33s 466us/step - loss: 0.0545 - val_loss: 0.0549\n",
      "Epoch 6/200\n",
      "71888/71888 [==============================] - 36s 498us/step - loss: 0.0540 - val_loss: 0.0536\n",
      "Epoch 7/200\n",
      "71888/71888 [==============================] - 38s 527us/step - loss: 0.0529 - val_loss: 0.0547\n",
      "Epoch 8/200\n",
      "71888/71888 [==============================] - 34s 474us/step - loss: 0.0521 - val_loss: 0.0551\n",
      "Epoch 9/200\n",
      "71888/71888 [==============================] - 32s 450us/step - loss: 0.0514 - val_loss: 0.0552\n",
      "Epoch 10/200\n",
      "71888/71888 [==============================] - 33s 456us/step - loss: 0.0501 - val_loss: 0.0544\n",
      "Epoch 11/200\n",
      "71888/71888 [==============================] - 32s 439us/step - loss: 0.0503 - val_loss: 0.0543\n",
      "Epoch 12/200\n",
      "71888/71888 [==============================] - 29s 410us/step - loss: 0.0496 - val_loss: 0.0538\n",
      "Epoch 13/200\n",
      "71888/71888 [==============================] - 28s 396us/step - loss: 0.0492 - val_loss: 0.0553\n",
      "Epoch 14/200\n",
      "71888/71888 [==============================] - 35s 484us/step - loss: 0.0486 - val_loss: 0.0550\n",
      "Epoch 15/200\n",
      "71888/71888 [==============================] - 30s 419us/step - loss: 0.0484 - val_loss: 0.0570\n",
      "Epoch 16/200\n",
      "71888/71888 [==============================] - 34s 475us/step - loss: 0.0476 - val_loss: 0.0546\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2337eb95d68>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_y, \n",
    "          batch_size=32,\n",
    "          epochs=200,\n",
    "          validation_data=(val_X, val_y), \n",
    "          verbose=True, \n",
    "          callbacks=[\n",
    "              ModelCheckpoint('model-conv.h5', save_best_only=True),\n",
    "              EarlyStopping(patience=10),\n",
    "          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23963/23963 [==============================] - 3s 110us/step\n",
      "toxic\n",
      "loss:  0.105332165325\n",
      "[[ 0.9878995   0.0121005 ]\n",
      " [ 0.31241887  0.68758113]]\n",
      "severe_toxic\n",
      "loss:  0.0253344406112\n",
      "[[ 0.99898832  0.00101168]\n",
      " [ 0.8375      0.1625    ]]\n",
      "obscene\n",
      "loss:  0.0608212333743\n",
      "[[ 0.99316759  0.00683241]\n",
      " [ 0.33202819  0.66797181]]\n",
      "threat\n",
      "loss:  0.0116147358343\n",
      "[[  9.99832545e-01   1.67455101e-04]\n",
      " [  9.34210526e-01   6.57894737e-02]]\n",
      "insult\n",
      "loss:  0.0742947426321\n",
      "[[ 0.99060208  0.00939792]\n",
      " [ 0.44043624  0.55956376]]\n",
      "identity_hate\n",
      "loss:  0.0248569240586\n",
      "[[  9.99200269e-01   7.99730617e-04]\n",
      " [  7.95121951e-01   2.04878049e-01]]\n",
      "Total loss:  0.0503757069727\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('model-conv.h5')\n",
    "\n",
    "val_result(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kaggle-toxic]",
   "language": "python",
   "name": "conda-env-kaggle-toxic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
