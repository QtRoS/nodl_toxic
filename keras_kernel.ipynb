{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 9)\n",
    "sns.set(context='paper', style='darkgrid', rc={'figure.facecolor':'white'}, font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "ERROR (theano.gpuarray): Could not initialize pygpu, support disabled\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/theano/gpuarray/__init__.py\", line 223, in <module>\n",
      "    use(config.device)\n",
      "  File \"/home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/theano/gpuarray/__init__.py\", line 210, in use\n",
      "    init_dev(device, preallocate=preallocate)\n",
      "  File \"/home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/theano/gpuarray/__init__.py\", line 96, in init_dev\n",
      "    **args)\n",
      "  File \"pygpu/gpuarray.pyx\", line 651, in pygpu.gpuarray.init\n",
      "  File \"pygpu/gpuarray.pyx\", line 587, in pygpu.gpuarray.pygpu_init\n",
      "pygpu.gpuarray.GpuArrayException: b'cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected'\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout, GRU, LeakyReLU\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 24000  # TODO\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"input/train.csv\")\n",
    "test = pd.read_csv(\"input/test.csv\")\n",
    "train = train.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path=\"weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor='val_loss', mode='min', patience=20)\n",
    "callbacks = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, output_dim=128)(inp)\n",
    "    x = Bidirectional(LSTM(50, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(50, activation='relu')(x)  # Leaky relu?\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                 optimizer='nadam', # adam\n",
    "                 metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86240/86265 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9789Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 523s 6ms/step - loss: 0.0634 - acc: 0.9790 - val_loss: 0.0519 - val_acc: 0.9809\n",
      "Epoch 2/2\n",
      "86240/86265 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9831Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 511s 6ms/step - loss: 0.0448 - acc: 0.9831 - val_loss: 0.0518 - val_acc: 0.9811\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "history = model.fit(X_t, y, batch_size=32, epochs=2, validation_split=0.1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(file_path)\n",
    "y_test = model.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv(\"output/keras_baseline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_with_params(p):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, output_dim=p['embedding_size'])(inp)\n",
    "    if p['cell_type']=='lstm':\n",
    "        cell = LSTM(int(p['units']), return_sequences=True, dropout=p['dropout_r'], recurrent_dropout=p['dropout_r'])\n",
    "    else:\n",
    "        cell = GRU(int(p['units']), return_sequences=True, dropout=p['dropout_r'], recurrent_dropout=p['dropout_r'])\n",
    "    x = Bidirectional(cell)(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(p['dropout_1'])(x)\n",
    "    x = Dense(p['dense_1'])(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(p['dropout_2'])(x)\n",
    "    x = Dense(6, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=p['opt_algo'], metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "def score(p):\n",
    "    print(\"Training with params:\", p)\n",
    "    model = get_model_with_params(p)\n",
    "    h = model.fit(X_t, y, batch_size=p['batch_size'], epochs=p['epochs'], validation_split=0.1, callbacks=callbacks)\n",
    "    score = min(h.history['val_loss']) #h.history['val_loss'][-1]\n",
    "    print(\"\\tScore {0}\\n\".format(score))\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "def optimize():\n",
    "    trials = Trials()\n",
    "    space = {\n",
    "        'batch_size' : hp.choice('batch_size', np.arange(12, 25, dtype=int)),\n",
    "        'dropout_1': hp.quniform('dropout_1', 0.00, 0.15, 0.025),\n",
    "        'dropout_2': hp.quniform('dropout_2', 0.025, 0.2, 0.025),\n",
    "        'dropout_r': 0, # hp.quniform('dropout_r', 0.00, 0.15, 0.025),\n",
    "        'dense_1': hp.choice('dense_1', np.arange(40, 65, dtype=int)),\n",
    "        'cell_type': hp.choice('cell_type', ['lstm', 'gru']),\n",
    "        'embedding_size': hp.choice('embedding_size', np.arange(64, 81, dtype=int)), # [64, 96, 128]\n",
    "        'units': hp.choice('units', np.arange(32, 48, dtype=int)),\n",
    "        'opt_algo': hp.choice('opt_algo', ['rmsprop', 'nadam', 'adam']),\n",
    "        'epochs': 2,\n",
    "    }\n",
    "\n",
    "    best = fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=16)\n",
    "\n",
    "    print(\"Best:\", best)\n",
    "    return best, trials\n",
    "\n",
    "b, t = optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 128)          3072000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 48)           22032     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 54)                2646      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 54)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 54)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 330       \n",
      "=================================================================\n",
      "Total params: 3,097,008\n",
      "Trainable params: 3,097,008\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86256/86265 [============================>.] - ETA: 0s - loss: 0.0733 - acc: 0.9763Epoch 00001: val_loss improved from inf to 0.05263, saving model to weights_base.best.hdf5\n",
      "86265/86265 [==============================] - 363s 4ms/step - loss: 0.0733 - acc: 0.9763 - val_loss: 0.0526 - val_acc: 0.9809\n",
      "Epoch 2/2\n",
      "86256/86265 [============================>.] - ETA: 0s - loss: 0.0490 - acc: 0.9819Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 366s 4ms/step - loss: 0.0490 - acc: 0.9819 - val_loss: 0.0527 - val_acc: 0.9808\n"
     ]
    }
   ],
   "source": [
    "p = {'cell_type': 'gru', 'opt_algo': 'adam', 'units': 24, 'batch_size': 24, 'embedding_size': 128,  \n",
    "     'dense_1': 54, 'dropout_1': 0.2, 'dropout_2': 0.2, 'dropout_r': 0.0, 'epochs': 2}\n",
    "manual_model = get_model_with_params(p)\n",
    "manual_model.summary()\n",
    "h = manual_model.fit(X_t, y, batch_size=p['batch_size'], epochs=p['epochs'], validation_split=0.1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from crossvalidation import multilabel_cross_validation, multilabel_label_combinations\n",
    "from multilabel_classifier import MultilabelClassifier\n",
    "from transform_pipeline import TransformPipeline\n",
    "\n",
    "#from nltk.tokenize import wordpunct_tokenize\n",
    "#from nltk.stem.snowball import EnglishStemmer\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#from functools import lru_cache\n",
    "\n",
    "#from textblob import TextBlob\n",
    "\n",
    "#from collections import OrderedDict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "from visualizations import topn_features, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class ModelTransformer(TransformerMixin):\n",
    "\n",
    "#     def __init__(self, model):\n",
    "#         self.model = model\n",
    "\n",
    "#     def fit(self, *args, **kwargs):\n",
    "#         self.model.fit(*args, **kwargs)\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X, **transform_params):\n",
    "#         return DataFrame(self.model.predict(X))\n",
    "    \n",
    "\n",
    "class KerasAnswerExtractor(TransformerMixin):\n",
    "\n",
    "    def __init__(self, model, column_name):\n",
    "        self.model = model\n",
    "        self.column_name = column_name\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        #self.model.fit(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X, *args, **kwargs):\n",
    "        prediction = self.model.predict(X)[:, list_classes.index(self.column_name)]\n",
    "        return prediction.reshape(len(prediction), -1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', FeatureUnion([\n",
    "        ('words', TransformPipeline([\n",
    "            ('stemmed', FunctionTransformer(lambda X: X[:, 0], validate=False)),\n",
    "            ('vec', TfidfVectorizer()),\n",
    "        ])),\n",
    "        ('chars', TransformPipeline([\n",
    "            ('text', FunctionTransformer(lambda X: X[:, 1], validate=False)),\n",
    "            ('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(1)\n",
    "    ]))\n",
    "])\n",
    "confusion_matrix(clf,\n",
    "                dftrain[['comment_text_stemmed', 'comment_text']],\n",
    "                np.array(dftrain[['identity_hate']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted negative</th>\n",
       "      <th>predicted positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          predicted negative  predicted positive\n",
       "negative                 NaN                 NaN\n",
       "positive                 NaN                 NaN"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name = 'toxic'\n",
    "class_y = train[class_name].values\n",
    "confusion_matrix(KerasAnswerExtractor(manual_model, class_name),\n",
    "                 X_t,\n",
    "                 class_y.reshape(len(class_y), -1), \n",
    "                 do_fit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KerasAnswerExtractor(manual_model, class_name)\n",
    "pred = clf.predict(X_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54601"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred.argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!Fuck off you ass!'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_sentences_train[pred.argmax()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 100, 128)          3072000   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 100, 100)          53700     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 51)                5151      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 51)                0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 51)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 6)                 312       \n",
      "=================================================================\n",
      "Total params: 3,131,163\n",
      "Trainable params: 3,131,163\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_model = load_model(file_path, custom_objects={ })\n",
    "final_model.summary()\n",
    "y_test = final_model.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv(\"output/keras_tuned_0.0457.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plots\n",
    "# my_plots = ['loss', 'acc']\n",
    "# for plot in my_plots:\n",
    "#     plt.plot(history.history[plot])\n",
    "#     plt.plot(history.history['val_' + plot])\n",
    "#     plt.title('model ' + plot)\n",
    "#     plt.ylabel(plot)\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.legend(['train', 'test'], loc='upper left')\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
