{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 9)\n",
    "sns.set(context='paper', style='darkgrid', rc={'figure.facecolor':'white'}, font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using Theano backend.\n",
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "ERROR (theano.gpuarray): Could not initialize pygpu, support disabled\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/theano/gpuarray/__init__.py\", line 223, in <module>\n",
      "    use(config.device)\n",
      "  File \"/home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/theano/gpuarray/__init__.py\", line 210, in use\n",
      "    init_dev(device, preallocate=preallocate)\n",
      "  File \"/home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/theano/gpuarray/__init__.py\", line 96, in init_dev\n",
      "    **args)\n",
      "  File \"pygpu/gpuarray.pyx\", line 651, in pygpu.gpuarray.init\n",
      "  File \"pygpu/gpuarray.pyx\", line 587, in pygpu.gpuarray.pygpu_init\n",
      "pygpu.gpuarray.GpuArrayException: b'cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected'\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout, GRU, LeakyReLU\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 24000  # TODO\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"input/train.csv\")\n",
    "test = pd.read_csv(\"input/test.csv\")\n",
    "train = train.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73760</th>\n",
       "      <td>769245648177</td>\n",
       "      <td>\"\\n\\n First sentence  painfully misguided \\n\\n...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21000</th>\n",
       "      <td>216962050962</td>\n",
       "      <td>yeah well, I don't subscribe to your opinion, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67630</th>\n",
       "      <td>704475052283</td>\n",
       "      <td>Because I'm wordy when I write (grins). Reword...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27429</th>\n",
       "      <td>284254550115</td>\n",
       "      <td>Claims the US planned false flag chemical atta...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53532</th>\n",
       "      <td>558271161037</td>\n",
       "      <td>\"\\n\\n Japanese city naming \\n\\nHi - I've revis...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "73760  769245648177  \"\\n\\n First sentence  painfully misguided \\n\\n...      0   \n",
       "21000  216962050962  yeah well, I don't subscribe to your opinion, ...      1   \n",
       "67630  704475052283  Because I'm wordy when I write (grins). Reword...      0   \n",
       "27429  284254550115  Claims the US planned false flag chemical atta...      0   \n",
       "53532  558271161037  \"\\n\\n Japanese city naming \\n\\nHi - I've revis...      0   \n",
       "\n",
       "       severe_toxic  obscene  threat  insult  identity_hate  \n",
       "73760             0        0       0       0              0  \n",
       "21000             0        0       0       0              0  \n",
       "67630             0        0       0       0              0  \n",
       "27429             0        0       0       0              0  \n",
       "53532             0        0       0       0              0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.903632\n",
      "1    0.096368\n",
      "Name: toxic, dtype: float64\n",
      "0    0.989932\n",
      "1    0.010068\n",
      "Name: severe_toxic, dtype: float64\n",
      "0    0.946699\n",
      "1    0.053301\n",
      "Name: obscene, dtype: float64\n",
      "0    0.996818\n",
      "1    0.003182\n",
      "Name: threat, dtype: float64\n",
      "0    0.950287\n",
      "1    0.049713\n",
      "Name: insult, dtype: float64\n",
      "0    0.991508\n",
      "1    0.008492\n",
      "Name: identity_hate, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for col in list_classes:\n",
    "    print(train[col].value_counts(normalize=True))\n",
    "#     train[col].value_counts(normalize=True).plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path=\"weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor='val_loss', mode='min', patience=20)\n",
    "callbacks = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, output_dim=128)(inp)\n",
    "    x = Bidirectional(LSTM(50, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(50, activation='relu')(x)  # Leaky relu?\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                 optimizer='nadam', # adam\n",
    "                 metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86240/86265 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9789Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 523s 6ms/step - loss: 0.0634 - acc: 0.9790 - val_loss: 0.0519 - val_acc: 0.9809\n",
      "Epoch 2/2\n",
      "86240/86265 [============================>.] - ETA: 0s - loss: 0.0447 - acc: 0.9831Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 511s 6ms/step - loss: 0.0448 - acc: 0.9831 - val_loss: 0.0518 - val_acc: 0.9811\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "history = model.fit(X_t, y, batch_size=32, epochs=2, validation_split=0.1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(file_path)\n",
    "y_test = model.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv(\"output/keras_baseline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_with_params(p):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, output_dim=p['embedding_size'])(inp)\n",
    "    if p['cell_type']=='lstm':\n",
    "        cell = LSTM(int(p['units']), return_sequences=True, dropout=p['dropout_r'], recurrent_dropout=p['dropout_r'])\n",
    "    else:\n",
    "        cell = GRU(int(p['units']), return_sequences=True, dropout=p['dropout_r'], recurrent_dropout=p['dropout_r'])\n",
    "    x = Bidirectional(cell)(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(p['dropout_1'])(x)\n",
    "    x = Dense(p['dense_1'])(x)\n",
    "    x = LeakyReLU()(x)\n",
    "    x = Dropout(p['dropout_2'])(x)\n",
    "    x = Dense(6, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=p['opt_algo'], metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "def score(p):\n",
    "    print(\"Training with params:\", p)\n",
    "    model = get_model_with_params(p)\n",
    "    h = model.fit(X_t, y, batch_size=p['batch_size'], epochs=p['epochs'], validation_split=0.1, callbacks=callbacks)\n",
    "    score = min(h.history['val_loss']) #h.history['val_loss'][-1]\n",
    "    print(\"\\tScore {0}\\n\".format(score))\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "def optimize():\n",
    "    trials = Trials()\n",
    "    space = {\n",
    "        'batch_size' : hp.choice('batch_size', np.arange(12, 25, dtype=int)),\n",
    "        'dropout_1': hp.quniform('dropout_1', 0.00, 0.15, 0.025),\n",
    "        'dropout_2': hp.quniform('dropout_2', 0.025, 0.2, 0.025),\n",
    "        'dropout_r': 0, # hp.quniform('dropout_r', 0.00, 0.15, 0.025),\n",
    "        'dense_1': hp.choice('dense_1', np.arange(40, 65, dtype=int)),\n",
    "        'cell_type': hp.choice('cell_type', ['lstm', 'gru']),\n",
    "        'embedding_size': hp.choice('embedding_size', np.arange(64, 81, dtype=int)), # [64, 96, 128]\n",
    "        'units': hp.choice('units', np.arange(32, 48, dtype=int)),\n",
    "        'opt_algo': hp.choice('opt_algo', ['rmsprop', 'nadam', 'adam']),\n",
    "        'epochs': 2,\n",
    "    }\n",
    "\n",
    "    best = fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=16)\n",
    "\n",
    "    print(\"Best:\", best)\n",
    "    return best, trials\n",
    "\n",
    "b, t = optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_2 (Embedding)      (None, 100, 80)           1920000   \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 100, 48)           15120     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_2 (Glob (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 48)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 54)                2646      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 54)                0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 54)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 330       \n",
      "=================================================================\n",
      "Total params: 1,938,096\n",
      "Trainable params: 1,938,096\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86256/86265 [============================>.] - ETA: 0s - loss: 0.0696 - acc: 0.9775Epoch 00001: val_loss improved from 0.05263 to 0.05143, saving model to weights_base.best.hdf5\n",
      "86265/86265 [==============================] - 2053s 24ms/step - loss: 0.0696 - acc: 0.9775 - val_loss: 0.0514 - val_acc: 0.9811\n",
      "Epoch 2/2\n",
      "54000/86265 [=================>............] - ETA: 12:58 - loss: 0.0451 - acc: 0.9834"
     ]
    }
   ],
   "source": [
    "p = {'cell_type': 'gru', 'opt_algo': 'adam', 'units': 24, 'batch_size': 24, 'embedding_size': 80,  \n",
    "     'dense_1': 54, 'dropout_1': 0.0, 'dropout_2': 0.2, 'dropout_r': 0.1, 'epochs': 2}\n",
    "manual_model = get_model_with_params(p)\n",
    "manual_model.summary()\n",
    "h = manual_model.fit(X_t, y, batch_size=p['batch_size'], epochs=p['epochs'], validation_split=0.1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from crossvalidation import multilabel_cross_validation, multilabel_label_combinations\n",
    "from multilabel_classifier import MultilabelClassifier\n",
    "from transform_pipeline import TransformPipeline\n",
    "\n",
    "#from nltk.tokenize import wordpunct_tokenize\n",
    "#from nltk.stem.snowball import EnglishStemmer\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#from functools import lru_cache\n",
    "\n",
    "#from textblob import TextBlob\n",
    "\n",
    "#from collections import OrderedDict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "from visualizations import topn_features, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class ModelTransformer(TransformerMixin):\n",
    "\n",
    "#     def __init__(self, model):\n",
    "#         self.model = model\n",
    "\n",
    "#     def fit(self, *args, **kwargs):\n",
    "#         self.model.fit(*args, **kwargs)\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X, **transform_params):\n",
    "#         return DataFrame(self.model.predict(X))\n",
    "    \n",
    "\n",
    "class KerasAnswerExtractor(TransformerMixin):\n",
    "\n",
    "    def __init__(self, model, column_name):\n",
    "        self.model = model\n",
    "        self.column_name = column_name\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        #self.model.fit(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X, *args, **kwargs):\n",
    "        prediction = (self.model.predict(X)[:, list_classes.index(self.column_name)])\n",
    "        return prediction.reshape(len(prediction), -1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# clf = Pipeline([\n",
    "#     ('vec', FeatureUnion([\n",
    "#         ('words', TransformPipeline([\n",
    "#             ('stemmed', FunctionTransformer(lambda X: X[:, 0], validate=False)),\n",
    "#             ('vec', TfidfVectorizer()),\n",
    "#         ])),\n",
    "#         ('chars', TransformPipeline([\n",
    "#             ('text', FunctionTransformer(lambda X: X[:, 1], validate=False)),\n",
    "#             ('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)),\n",
    "#         ])),\n",
    "#     ])),\n",
    "#     ('clf', MultilabelClassifier([\n",
    "#         LogisticRegression(penalty='l1')\n",
    "#         for _ in range(1)\n",
    "#     ]))\n",
    "# ])\n",
    "# confusion_matrix(clf,\n",
    "#                 dftrain[['comment_text_stemmed', 'comment_text']],\n",
    "#                 np.array(dftrain[['identity_hate']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted negative</th>\n",
       "      <th>predicted positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.986284</td>\n",
       "      <td>0.013716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.137722</td>\n",
       "      <td>0.862278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          predicted negative  predicted positive\n",
       "negative            0.986284            0.013716\n",
       "positive            0.137722            0.862278"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name = 'toxic'\n",
    "class_y = train[class_name].values\n",
    "confusion_matrix(KerasAnswerExtractor(manual_model, class_name),\n",
    "                 X_t,\n",
    "                 class_y.reshape(len(class_y), -1), \n",
    "                 do_fit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic log loss: 0.07290494285026734\n",
      "[[0.98549888 0.01450112]\n",
      " [0.13467576 0.86532424]]\n",
      "severe_toxic log loss: 0.023505014466099847\n",
      "[[0.99798706 0.00201294]\n",
      " [0.68082902 0.31917098]]\n",
      "obscene log loss: 0.04956906263386151\n",
      "[[0.98674263 0.01325737]\n",
      " [0.1233118  0.8766882 ]]\n",
      "threat log loss: 0.013953790726962609\n",
      "[[1. 0.]\n",
      " [1. 0.]]\n",
      "insult log loss: 0.06100324271304353\n",
      "[[0.98399315 0.01600685]\n",
      " [0.20587618 0.79412382]]\n",
      "identity_hate log loss: 0.028804451678192445\n",
      "[[1. 0.]\n",
      " [1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "for class_name in list_classes:\n",
    "    clf = KerasAnswerExtractor(manual_model, class_name)\n",
    "    pred = clf.predict(X_t)\n",
    "    class_y = train[class_name].values\n",
    "    cm = confusion_matrix(class_y.reshape(len(class_y), -1), (pred > 0.5).astype(int))\n",
    "    print(class_name, \"log loss:\", log_loss(class_y.reshape(len(class_y), -1), pred))\n",
    "    print(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9008559668824437"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(class_y.reshape(len(class_y), -1), pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 100, 128)          3072000   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 100, 100)          53700     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 51)                5151      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 51)                0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 51)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 6)                 312       \n",
      "=================================================================\n",
      "Total params: 3,131,163\n",
      "Trainable params: 3,131,163\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_model = load_model(file_path, custom_objects={ })\n",
    "final_model.summary()\n",
    "y_test = final_model.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv(\"output/keras_tuned_0.0457.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plots\n",
    "# my_plots = ['loss', 'acc']\n",
    "# for plot in my_plots:\n",
    "#     plt.plot(history.history[plot])\n",
    "#     plt.plot(history.history['val_' + plot])\n",
    "#     plt.title('model ' + plot)\n",
    "#     plt.ylabel(plot)\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.legend(['train', 'test'], loc='upper left')\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
