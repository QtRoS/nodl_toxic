{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 9)\n",
    "sns.set(context='paper', style='darkgrid', rc={'figure.facecolor':'white'}, font_scale=1.2)\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout, GRU, LeakyReLU, CuDNNGRU, CuDNNLSTM\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 24000  # TODO\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"input/train.csv\")\n",
    "test = pd.read_csv(\"input/test.csv\")\n",
    "train = train.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>80515</th>\n",
       "      <td>d76b51560386b138</td>\n",
       "      <td>Upgrade works delayed\\nI don't have a referenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114630</th>\n",
       "      <td>6519da8b639605ae</td>\n",
       "      <td>How come i have seen many useless articles abo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82524</th>\n",
       "      <td>dcc4ab5422620087</td>\n",
       "      <td>Any tendencies in the naming of other articles...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34900</th>\n",
       "      <td>5d38f140a929b08c</td>\n",
       "      <td>\"\\n\\n poise between legislation and constituti...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71059</th>\n",
       "      <td>be326db76147c9fa</td>\n",
       "      <td>\"\\n\\n Dogs \\n\\nOne question, how many dogs do ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "80515   d76b51560386b138  Upgrade works delayed\\nI don't have a referenc...   \n",
       "114630  6519da8b639605ae  How come i have seen many useless articles abo...   \n",
       "82524   dcc4ab5422620087  Any tendencies in the naming of other articles...   \n",
       "34900   5d38f140a929b08c  \"\\n\\n poise between legislation and constituti...   \n",
       "71059   be326db76147c9fa  \"\\n\\n Dogs \\n\\nOne question, how many dogs do ...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "80515       0             0        0       0       0              0  \n",
       "114630      0             0        0       0       0              0  \n",
       "82524       0             0        0       0       0              0  \n",
       "34900       0             0        0       0       0              0  \n",
       "71059       0             0        0       0       0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path=\"weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor='val_loss', mode='min', patience=20)\n",
    "callbacks = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.904156\n",
      "1    0.095844\n",
      "Name: toxic, dtype: float64\n",
      "0    0.990004\n",
      "1    0.009996\n",
      "Name: severe_toxic, dtype: float64\n",
      "0    0.947052\n",
      "1    0.052948\n",
      "Name: obscene, dtype: float64\n",
      "0    0.997004\n",
      "1    0.002996\n",
      "Name: threat, dtype: float64\n",
      "0    0.950636\n",
      "1    0.049364\n",
      "Name: insult, dtype: float64\n",
      "0    0.991195\n",
      "1    0.008805\n",
      "Name: identity_hate, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for col in list_classes:\n",
    "    print(train[col].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, output_dim=128)(inp)\n",
    "    x = Bidirectional(LSTM(50, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                 optimizer='adam', # nadam\n",
    "                 metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1213: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9781Epoch 00001: val_loss improved from inf to 0.05502, saving model to weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 843s 6ms/step - loss: 0.0725 - acc: 0.9781 - val_loss: 0.0550 - val_acc: 0.9809\n",
      "Epoch 2/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9822Epoch 00002: val_loss improved from 0.05502 to 0.05048, saving model to weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 841s 6ms/step - loss: 0.0501 - acc: 0.9822 - val_loss: 0.0505 - val_acc: 0.9824\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "history = model.fit(X_t, y, batch_size=32, epochs=2, validation_split=0.1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train on 86265 samples, validate on 9586 samples\n",
    "Epoch 1/2\n",
    "86240/86265 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9765Epoch 00001: val_loss did not improve\n",
    "86265/86265 [==============================] - 652s 8ms/step - loss: 0.0829 - acc: 0.9765 - val_loss: 0.0556 - val_acc: 0.9812\n",
    "Epoch 2/2\n",
    "86240/86265 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9823Epoch 00002: val_loss did not improve\n",
    "86265/86265 [==============================] - 703s 8ms/step - loss: 0.0504 - acc: 0.9823 - val_loss: 0.0549 - val_acc: 0.9817"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(file_path)\n",
    "y_test = model.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv(\"output/keras_baseline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_with_params(p):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, output_dim=p['embedding_size'])(inp)\n",
    "    if p['cell_type']=='lstm':\n",
    "        cell = CuDNNLSTM(int(p['units']), return_sequences=True)  # , dropout=p['dropout_r'], recurrent_dropout=p['dropout_r']\n",
    "    else:\n",
    "        cell = CuDNNGRU(int(p['units']), return_sequences=True)  # , dropout=p['dropout_r'], recurrent_dropout=p['dropout_r']\n",
    "    x = Bidirectional(cell)(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(p['dropout_1'])(x)\n",
    "    x = Dense(p['dense_1'], activation='relu')(x)  #     x = LeakyReLU()(x)\n",
    "    x = Dropout(p['dropout_2'])(x)\n",
    "    x = Dense(6, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=p['opt_algo'], metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params: {'batch_size': 21, 'cell_type': 'lstm', 'dense_1': 63, 'dropout_1': 0.15000000000000002, 'dropout_2': 0.17500000000000002, 'dropout_r': 0, 'embedding_size': 87, 'epochs': 2, 'opt_algo': 'adam', 'units': 55}\n",
      "WARNING:tensorflow:From /home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1213: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143577/143613 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9792Epoch 00001: val_loss improved from inf to 0.04636, saving model to weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 99s 686us/step - loss: 0.0623 - acc: 0.9792 - val_loss: 0.0464 - val_acc: 0.9830\n",
      "Epoch 2/2\n",
      "143577/143613 [============================>.] - ETA: 0s - loss: 0.0458 - acc: 0.9830Epoch 00002: val_loss improved from 0.04636 to 0.04463, saving model to weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 97s 678us/step - loss: 0.0458 - acc: 0.9830 - val_loss: 0.0446 - val_acc: 0.9836\n",
      "\tScore 0.04463030758149928\n",
      "\n",
      "Training with params: {'batch_size': 29, 'cell_type': 'lstm', 'dense_1': 54, 'dropout_1': 0.15000000000000002, 'dropout_2': 0.025, 'dropout_r': 0, 'embedding_size': 85, 'epochs': 2, 'opt_algo': 'adam', 'units': 47}\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143579/143613 [============================>.] - ETA: 0s - loss: 0.0638 - acc: 0.9787Epoch 00001: val_loss did not improve\n",
      "143613/143613 [==============================] - 73s 507us/step - loss: 0.0638 - acc: 0.9787 - val_loss: 0.0496 - val_acc: 0.9819\n",
      "Epoch 2/2\n",
      "143579/143613 [============================>.] - ETA: 0s - loss: 0.0461 - acc: 0.9830Epoch 00002: val_loss did not improve\n",
      "143613/143613 [==============================] - 71s 492us/step - loss: 0.0461 - acc: 0.9830 - val_loss: 0.0448 - val_acc: 0.9833\n",
      "\tScore 0.04483571296712747\n",
      "\n",
      "Training with params: {'batch_size': 30, 'cell_type': 'gru', 'dense_1': 54, 'dropout_1': 0.025, 'dropout_2': 0.15000000000000002, 'dropout_r': 0, 'embedding_size': 72, 'epochs': 2, 'opt_algo': 'adam', 'units': 56}\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143610/143613 [============================>.] - ETA: 0s - loss: 0.0615 - acc: 0.9792Epoch 00001: val_loss did not improve\n",
      "143613/143613 [==============================] - 63s 435us/step - loss: 0.0615 - acc: 0.9792 - val_loss: 0.0465 - val_acc: 0.9828\n",
      "Epoch 2/2\n",
      "143580/143613 [============================>.] - ETA: 0s - loss: 0.0439 - acc: 0.9835Epoch 00002: val_loss improved from 0.04463 to 0.04398, saving model to weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 63s 439us/step - loss: 0.0439 - acc: 0.9835 - val_loss: 0.0440 - val_acc: 0.9835\n",
      "\tScore 0.04397768202142482\n",
      "\n",
      "Training with params: {'batch_size': 19, 'cell_type': 'gru', 'dense_1': 51, 'dropout_1': 0.07500000000000001, 'dropout_2': 0.025, 'dropout_r': 0, 'embedding_size': 114, 'epochs': 2, 'opt_algo': 'adam', 'units': 53}\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143583/143613 [============================>.] - ETA: 0s - loss: 0.0574 - acc: 0.9805Epoch 00001: val_loss did not improve\n",
      "143613/143613 [==============================] - 132s 919us/step - loss: 0.0574 - acc: 0.9805 - val_loss: 0.0467 - val_acc: 0.9831\n",
      "Epoch 2/2\n",
      "143602/143613 [============================>.] - ETA: 0s - loss: 0.0418 - acc: 0.9840Epoch 00002: val_loss improved from 0.04398 to 0.04320, saving model to weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 132s 916us/step - loss: 0.0418 - acc: 0.9840 - val_loss: 0.0432 - val_acc: 0.9836\n",
      "\tScore 0.043195044269181554\n",
      "\n",
      "Training with params: {'batch_size': 29, 'cell_type': 'gru', 'dense_1': 61, 'dropout_1': 0.05, 'dropout_2': 0.15000000000000002, 'dropout_r': 0, 'embedding_size': 74, 'epochs': 2, 'opt_algo': 'adam', 'units': 60}\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143521/143613 [============================>.] - ETA: 0s - loss: 0.0609 - acc: 0.9795Epoch 00001: val_loss did not improve\n",
      "143613/143613 [==============================] - 64s 445us/step - loss: 0.0609 - acc: 0.9795 - val_loss: 0.0460 - val_acc: 0.9829\n",
      "Epoch 2/2\n",
      "143521/143613 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9836Epoch 00002: val_loss improved from 0.04320 to 0.04310, saving model to weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 64s 443us/step - loss: 0.0436 - acc: 0.9836 - val_loss: 0.0431 - val_acc: 0.9841\n",
      "\tScore 0.0431007441215338\n",
      "\n",
      "Training with params: {'batch_size': 30, 'cell_type': 'gru', 'dense_1': 64, 'dropout_1': 0.025, 'dropout_2': 0.025, 'dropout_r': 0, 'embedding_size': 107, 'epochs': 2, 'opt_algo': 'adam', 'units': 58}\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143550/143613 [============================>.] - ETA: 0s - loss: 0.0610 - acc: 0.9796Epoch 00001: val_loss did not improve\n",
      "143613/143613 [==============================] - 86s 596us/step - loss: 0.0609 - acc: 0.9796 - val_loss: 0.0453 - val_acc: 0.9834\n",
      "Epoch 2/2\n",
      "143610/143613 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9838Epoch 00002: val_loss did not improve\n",
      "143613/143613 [==============================] - 84s 588us/step - loss: 0.0426 - acc: 0.9838 - val_loss: 0.0451 - val_acc: 0.9834\n",
      "\tScore 0.04509735859118482\n",
      "\n",
      "Training with params: {'batch_size': 21, 'cell_type': 'lstm', 'dense_1': 62, 'dropout_1': 0.05, 'dropout_2': 0.17500000000000002, 'dropout_r': 0, 'embedding_size': 77, 'epochs': 2, 'opt_algo': 'adam', 'units': 53}\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143535/143613 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9794Epoch 00001: val_loss did not improve\n",
      "143613/143613 [==============================] - 97s 679us/step - loss: 0.0623 - acc: 0.9794 - val_loss: 0.0478 - val_acc: 0.9825\n",
      "Epoch 2/2\n",
      "143535/143613 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9829Epoch 00002: val_loss did not improve\n",
      "143613/143613 [==============================] - 98s 685us/step - loss: 0.0456 - acc: 0.9829 - val_loss: 0.0449 - val_acc: 0.9830\n",
      "\tScore 0.044903031306638336\n",
      "\n",
      "Training with params: {'batch_size': 27, 'cell_type': 'lstm', 'dense_1': 48, 'dropout_1': 0.15000000000000002, 'dropout_2': 0.15000000000000002, 'dropout_r': 0, 'embedding_size': 109, 'epochs': 2, 'opt_algo': 'adam', 'units': 60}\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143586/143613 [============================>.] - ETA: 0s - loss: 0.0619 - acc: 0.9794Epoch 00001: val_loss did not improve\n",
      "143613/143613 [==============================] - 74s 514us/step - loss: 0.0619 - acc: 0.9794 - val_loss: 0.0466 - val_acc: 0.9827\n",
      "Epoch 2/2\n",
      "143586/143613 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9831Epoch 00002: val_loss did not improve\n",
      "143613/143613 [==============================] - 73s 511us/step - loss: 0.0455 - acc: 0.9831 - val_loss: 0.0458 - val_acc: 0.9831\n",
      "\tScore 0.045751451545707006\n",
      "\n",
      "Training with params: {'batch_size': 17, 'cell_type': 'lstm', 'dense_1': 60, 'dropout_1': 0.07500000000000001, 'dropout_2': 0.17500000000000002, 'dropout_r': 0, 'embedding_size': 124, 'epochs': 2, 'opt_algo': 'adam', 'units': 41}\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143548/143613 [============================>.] - ETA: 0s - loss: 0.0617 - acc: 0.9793Epoch 00001: val_loss did not improve\n",
      "143613/143613 [==============================] - 130s 905us/step - loss: 0.0617 - acc: 0.9793 - val_loss: 0.0469 - val_acc: 0.9828\n",
      "Epoch 2/2\n",
      "143565/143613 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9832Epoch 00002: val_loss did not improve\n",
      "143613/143613 [==============================] - 129s 899us/step - loss: 0.0454 - acc: 0.9832 - val_loss: 0.0442 - val_acc: 0.9834\n",
      "\tScore 0.04415319834608059\n",
      "\n",
      "Training with params: {'batch_size': 17, 'cell_type': 'lstm', 'dense_1': 60, 'dropout_1': 0.125, 'dropout_2': 0.17500000000000002, 'dropout_r': 0, 'embedding_size': 120, 'epochs': 2, 'opt_algo': 'adam', 'units': 41}\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143582/143613 [============================>.] - ETA: 0s - loss: 0.0601 - acc: 0.9798Epoch 00001: val_loss did not improve\n",
      "143613/143613 [==============================] - 126s 880us/step - loss: 0.0601 - acc: 0.9798 - val_loss: 0.0482 - val_acc: 0.9826\n",
      "Epoch 2/2\n",
      "143565/143613 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9833Epoch 00002: val_loss did not improve\n",
      "143613/143613 [==============================] - 129s 895us/step - loss: 0.0444 - acc: 0.9833 - val_loss: 0.0446 - val_acc: 0.9836\n",
      "\tScore 0.04456322384780706\n",
      "\n",
      "Training with params: {'batch_size': 18, 'cell_type': 'lstm', 'dense_1': 60, 'dropout_1': 0.1, 'dropout_2': 0.2, 'dropout_r': 0, 'embedding_size': 66, 'epochs': 2, 'opt_algo': 'adam', 'units': 50}\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143586/143613 [============================>.] - ETA: 0s - loss: 0.0625 - acc: 0.9790Epoch 00001: val_loss did not improve\n",
      "143613/143613 [==============================] - 111s 771us/step - loss: 0.0625 - acc: 0.9790 - val_loss: 0.0467 - val_acc: 0.9830\n",
      "Epoch 2/2\n",
      "143550/143613 [============================>.] - ETA: 0s - loss: 0.0457 - acc: 0.9832Epoch 00002: val_loss did not improve\n",
      "143613/143613 [==============================] - 110s 765us/step - loss: 0.0457 - acc: 0.9832 - val_loss: 0.0451 - val_acc: 0.9835\n",
      "\tScore 0.04509355138040266\n",
      "\n",
      "Training with params: {'batch_size': 29, 'cell_type': 'gru', 'dense_1': 49, 'dropout_1': 0.125, 'dropout_2': 0.05, 'dropout_r': 0, 'embedding_size': 126, 'epochs': 2, 'opt_algo': 'adam', 'units': 54}\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143579/143613 [============================>.] - ETA: 0s - loss: 0.0605 - acc: 0.9799Epoch 00001: val_loss did not improve\n",
      "143613/143613 [==============================] - 92s 639us/step - loss: 0.0605 - acc: 0.9799 - val_loss: 0.0446 - val_acc: 0.9835\n",
      "Epoch 2/2\n",
      "143579/143613 [============================>.] - ETA: 0s - loss: 0.0426 - acc: 0.9837Epoch 00002: val_loss improved from 0.04310 to 0.04267, saving model to weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 91s 634us/step - loss: 0.0426 - acc: 0.9837 - val_loss: 0.0427 - val_acc: 0.9840\n",
      "\tScore 0.04266818907590267\n",
      "\n",
      "Training with params: {'batch_size': 27, 'cell_type': 'lstm', 'dense_1': 44, 'dropout_1': 0.07500000000000001, 'dropout_2': 0.15000000000000002, 'dropout_r': 0, 'embedding_size': 114, 'epochs': 2, 'opt_algo': 'adam', 'units': 64}\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143586/143613 [============================>.] - ETA: 0s - loss: 0.0640 - acc: 0.9789Epoch 00001: val_loss did not improve\n",
      "143613/143613 [==============================] - 76s 531us/step - loss: 0.0640 - acc: 0.9789 - val_loss: 0.0477 - val_acc: 0.9822\n",
      "Epoch 2/2\n",
      "143505/143613 [============================>.] - ETA: 0s - loss: 0.0460 - acc: 0.9830Epoch 00002: val_loss did not improve\n",
      "143613/143613 [==============================] - 75s 525us/step - loss: 0.0461 - acc: 0.9830 - val_loss: 0.0463 - val_acc: 0.9830\n",
      "\tScore 0.04632683281006327\n",
      "\n",
      "Training with params: {'batch_size': 16, 'cell_type': 'lstm', 'dense_1': 57, 'dropout_1': 0.025, 'dropout_2': 0.025, 'dropout_r': 0, 'embedding_size': 120, 'epochs': 2, 'opt_algo': 'adam', 'units': 58}\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143568/143613 [============================>.] - ETA: 0s - loss: 0.0578 - acc: 0.9802Epoch 00001: val_loss did not improve\n",
      "143613/143613 [==============================] - 126s 878us/step - loss: 0.0578 - acc: 0.9802 - val_loss: 0.0460 - val_acc: 0.9824\n",
      "Epoch 2/2\n",
      "143552/143613 [============================>.] - ETA: 0s - loss: 0.0429 - acc: 0.9837Epoch 00002: val_loss did not improve\n",
      "143613/143613 [==============================] - 125s 870us/step - loss: 0.0429 - acc: 0.9837 - val_loss: 0.0436 - val_acc: 0.9835\n",
      "\tScore 0.043617860676346876\n",
      "\n",
      "Training with params: {'batch_size': 18, 'cell_type': 'gru', 'dense_1': 54, 'dropout_1': 0.07500000000000001, 'dropout_2': 0.15000000000000002, 'dropout_r': 0, 'embedding_size': 81, 'epochs': 2, 'opt_algo': 'adam', 'units': 64}\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143604/143613 [============================>.] - ETA: 0s - loss: 0.0592 - acc: 0.9800Epoch 00001: val_loss did not improve\n",
      "143613/143613 [==============================] - 110s 769us/step - loss: 0.0592 - acc: 0.9800 - val_loss: 0.0480 - val_acc: 0.9824\n",
      "Epoch 2/2\n",
      "143604/143613 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9834Epoch 00002: val_loss did not improve\n",
      "143613/143613 [==============================] - 107s 743us/step - loss: 0.0443 - acc: 0.9834 - val_loss: 0.0442 - val_acc: 0.9837\n",
      "\tScore 0.0441663986329053\n",
      "\n",
      "Training with params: {'batch_size': 26, 'cell_type': 'lstm', 'dense_1': 45, 'dropout_1': 0.125, 'dropout_2': 0.15000000000000002, 'dropout_r': 0, 'embedding_size': 120, 'epochs': 2, 'opt_algo': 'adam', 'units': 52}\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143546/143613 [============================>.] - ETA: 0s - loss: 0.0623 - acc: 0.9791Epoch 00001: val_loss did not improve\n",
      "143613/143613 [==============================] - 79s 550us/step - loss: 0.0623 - acc: 0.9791 - val_loss: 0.0472 - val_acc: 0.9823\n",
      "Epoch 2/2\n",
      "143598/143613 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9829Epoch 00002: val_loss did not improve\n",
      "143613/143613 [==============================] - 80s 557us/step - loss: 0.0455 - acc: 0.9829 - val_loss: 0.0449 - val_acc: 0.9835\n",
      "\tScore 0.04492902493024443\n",
      "\n",
      "Training with params: {'batch_size': 19, 'cell_type': 'lstm', 'dense_1': 62, 'dropout_1': 0.07500000000000001, 'dropout_2': 0.2, 'dropout_r': 0, 'embedding_size': 69, 'epochs': 2, 'opt_algo': 'adam', 'units': 50}\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143602/143613 [============================>.] - ETA: 0s - loss: 0.0631 - acc: 0.9788Epoch 00001: val_loss did not improve\n",
      "143613/143613 [==============================] - 110s 769us/step - loss: 0.0631 - acc: 0.9788 - val_loss: 0.0472 - val_acc: 0.9824\n",
      "Epoch 2/2\n",
      " 25460/143613 [====>.........................] - ETA: 1:25 - loss: 0.0477 - acc: 0.9824"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36moptimize\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[1;32m    305\u001b[0m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_argmin\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         )\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/lib/python3.6/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(self, fn, space, algo, max_evals, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin)\u001b[0m\n\u001b[1;32m    633\u001b[0m             \u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpass_expr_memo_ctrl\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    634\u001b[0m             \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 635\u001b[0;31m             return_argmin=return_argmin)\n\u001b[0m\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[0;34m(fn, space, algo, max_evals, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin)\u001b[0m\n\u001b[1;32m    318\u001b[0m                     verbose=verbose)\n\u001b[1;32m    319\u001b[0m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcatch_eval_exceptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcatch_eval_exceptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 320\u001b[0;31m     \u001b[0mrval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    321\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, N, block_until_done)\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m                 \u001b[0;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mstopped\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/lib/python3.6/site-packages/hyperopt/fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[0;34m(self, N)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'job exception: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/lib/python3.6/site-packages/hyperopt/base.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[1;32m    838\u001b[0m                 \u001b[0mmemo\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmemo\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m                 print_node_on_error=self.rec_eval_print_node_on_error)\n\u001b[0;32m--> 840\u001b[0;31m             \u001b[0mrval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    841\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    842\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36mscore\u001b[0;34m(p)\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1655\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1656\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1657\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1659\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/Software/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2355\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2356\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2357\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2358\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1126\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1128\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1129\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1130\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1343\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1344\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1345\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1346\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1348\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Software/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1327\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1328\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1329\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1330\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1331\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "def score(p):\n",
    "    print(\"Training with params:\", p)\n",
    "    model = get_model_with_params(p)\n",
    "    h = model.fit(X_t, y, batch_size=p['batch_size'], epochs=p['epochs'], validation_split=0.1, callbacks=callbacks)\n",
    "    score = min(h.history['val_loss']) #h.history['val_loss'][-1]\n",
    "    print(\"\\tScore {0}\\n\".format(score))\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "def optimize():\n",
    "    trials = Trials()\n",
    "    space = {\n",
    "        'batch_size' : hp.choice('batch_size', np.arange(16, 33, dtype=int)),\n",
    "        'dropout_1': hp.quniform('dropout_1', 0.00, 0.15, 0.025),\n",
    "        'dropout_2': hp.quniform('dropout_2', 0.025, 0.2, 0.025),\n",
    "        'dropout_r': 0, # hp.quniform('dropout_r', 0.00, 0.15, 0.025),\n",
    "        'dense_1': hp.choice('dense_1', np.arange(44, 65, dtype=int)),\n",
    "        'cell_type': hp.choice('cell_type', ['lstm', 'gru']),\n",
    "        'embedding_size': hp.choice('embedding_size', np.arange(64, 129, dtype=int)), # [64, 96, 128]\n",
    "        'units': hp.choice('units', np.arange(40, 65, dtype=int)),\n",
    "        'opt_algo': hp.choice('opt_algo', ['adam']),  # 'rmsprop', 'nadam', \n",
    "        'epochs': 2,\n",
    "    }\n",
    "\n",
    "    best = fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=24)\n",
    "\n",
    "    print(\"Best:\", best)\n",
    "    return best, trials\n",
    "\n",
    "b, t = optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1213: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 80)           1920000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 100)          39300     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                6464      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 1,966,154\n",
      "Trainable params: 1,966,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86240/86265 [============================>.] - ETA: 0s - loss: 0.0684 - acc: 0.9781Epoch 00001: val_loss improved from inf to 0.05107, saving model to weights_base.best.hdf5\n",
      "86265/86265 [==============================] - 187s 2ms/step - loss: 0.0684 - acc: 0.9781 - val_loss: 0.0511 - val_acc: 0.9819\n",
      "Epoch 2/2\n",
      "86240/86265 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9835Epoch 00002: val_loss improved from 0.05107 to 0.04911, saving model to weights_base.best.hdf5\n",
      "86265/86265 [==============================] - 183s 2ms/step - loss: 0.0443 - acc: 0.9835 - val_loss: 0.0491 - val_acc: 0.9826\n"
     ]
    }
   ],
   "source": [
    "p = {'cell_type': 'gru', 'opt_algo': 'adam', 'units': 50, 'batch_size': 32, 'embedding_size': 80,  \n",
    "     'dense_1': 64, 'dropout_1': 0.05, 'dropout_2': 0.05, 'dropout_r': 0.0, 'epochs': 2}\n",
    "manual_model = get_model_with_params(p)\n",
    "manual_model.summary()\n",
    "h = manual_model.fit(X_t, y, batch_size=p['batch_size'], epochs=p['epochs'], validation_split=0.1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from crossvalidation import multilabel_cross_validation, multilabel_label_combinations\n",
    "from multilabel_classifier import MultilabelClassifier\n",
    "from transform_pipeline import TransformPipeline\n",
    "#from nltk.tokenize import wordpunct_tokenize\n",
    "#from nltk.stem.snowball import EnglishStemmer\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#from functools import lru_cache\n",
    "#from textblob import TextBlob\n",
    "#from collections import OrderedDict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "from visualizations import topn_features, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class ModelTransformer(TransformerMixin):\n",
    "#     def __init__(self, model):\n",
    "#         self.model = model\n",
    "\n",
    "#     def fit(self, *args, **kwargs):\n",
    "#         self.model.fit(*args, **kwargs)\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X, **transform_params):\n",
    "#         return DataFrame(self.model.predict(X))\n",
    "    \n",
    "class KerasAnswerExtractor(TransformerMixin):\n",
    "\n",
    "    def __init__(self, model, column_name):\n",
    "        self.model = model\n",
    "        self.column_name = column_name\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        #self.model.fit(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X, *args, **kwargs):\n",
    "        prediction = (self.model.predict(X)[:, list_classes.index(self.column_name)])\n",
    "        return prediction.reshape(len(prediction), -1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_name = 'toxic'\n",
    "class_y = train[class_name].values\n",
    "confusion_matrix(KerasAnswerExtractor(manual_model, class_name),\n",
    "                 X_t,\n",
    "                 class_y.reshape(len(class_y), -1), \n",
    "                 do_fit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Final model & prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 100, 126)          3024000   \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 100, 108)          58968     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_12 (Glo (None, 108)               0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 108)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 49)                5341      \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 49)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 6)                 300       \n",
      "=================================================================\n",
      "Total params: 3,088,609\n",
      "Trainable params: 3,088,609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "toxic log loss: 0.07081675314289335\n",
      "[[0.99045586 0.00954414]\n",
      " [0.18968223 0.81031777]]\n",
      "severe_toxic log loss: 0.0208816917491589\n",
      "[[9.99949359e-01 5.06406036e-05]\n",
      " [9.64890282e-01 3.51097179e-02]]\n",
      "obscene log loss: 0.04063262647422096\n",
      "[[0.99464671 0.00535329]\n",
      " [0.19493431 0.80506569]]\n",
      "threat log loss: 0.007973142431074444\n",
      "[[9.99956001e-01 4.39994217e-05]\n",
      " [9.30962343e-01 6.90376569e-02]]\n",
      "insult log loss: 0.051490818153220104\n",
      "[[0.99243213 0.00756787]\n",
      " [0.29325885 0.70674115]]\n",
      "identity_hate log loss: 0.018821650964018926\n",
      "[[0.99871022 0.00128978]\n",
      " [0.60640569 0.39359431]]\n"
     ]
    }
   ],
   "source": [
    "final_model = load_model(file_path, custom_objects={ })\n",
    "final_model.summary()\n",
    "pred_train = final_model.predict(X_t)\n",
    "for class_name in list_classes:\n",
    "    one_column_answer = pred_train[:, list_classes.index(class_name)]\n",
    "    pred = one_column_answer.reshape(len(one_column_answer), -1)\n",
    "    class_y = train[class_name].values\n",
    "    cm = confusion_matrix(class_y.reshape(len(class_y), -1), (pred > 0.5).astype(int))\n",
    "    print(class_name, \"log loss:\", log_loss(class_y.reshape(len(class_y), -1), pred))\n",
    "    print(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for class_name in list_classes:\n",
    "#     clf = KerasAnswerExtractor(manual_model, class_name)\n",
    "#     pred = clf.predict(X_t)\n",
    "#     class_y = train[class_name].values\n",
    "#     cm = confusion_matrix(class_y.reshape(len(class_y), -1), (pred > 0.5).astype(int))\n",
    "#     print(class_name, \"log loss:\", log_loss(class_y.reshape(len(class_y), -1), pred))\n",
    "#     print(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toxic log loss: 0.07620896772834636\n",
    "[[0.98964371 0.01035629]\n",
    " [0.20082278 0.79917722]]\n",
    "severe_toxic log loss: 0.024454393614495473\n",
    "[[0.99581603 0.00418397]\n",
    " [0.53678756 0.46321244]]\n",
    "obscene log loss: 0.04788283939460444\n",
    "[[0.98856098 0.01143902]\n",
    " [0.13114112 0.86885888]]\n",
    "threat log loss: 0.0137781501289922\n",
    "[[9.99989534e-01 1.04661629e-05]\n",
    " [9.96721311e-01 3.27868852e-03]]\n",
    "insult log loss: 0.06003329518399972\n",
    "[[0.98591441 0.01408559]\n",
    " [0.20986359 0.79013641]]\n",
    "identity_hate log loss: 0.02840215769807751\n",
    "[[9.99905300e-01 9.46999590e-05]\n",
    " [9.87714988e-01 1.22850123e-02]]\n",
    "\n",
    "\n",
    "toxic log loss: 0.07290494285026734\n",
    "[[0.98549888 0.01450112]\n",
    " [0.13467576 0.86532424]]\n",
    "severe_toxic log loss: 0.023505014466099847\n",
    "[[0.99798706 0.00201294]\n",
    " [0.68082902 0.31917098]]\n",
    "obscene log loss: 0.04956906263386151\n",
    "[[0.98674263 0.01325737]\n",
    " [0.1233118  0.8766882 ]]\n",
    "threat log loss: 0.013953790726962609\n",
    "[[1. 0.]\n",
    " [1. 0.]]\n",
    "insult log loss: 0.06100324271304353\n",
    "[[0.98399315 0.01600685]\n",
    " [0.20587618 0.79412382]]\n",
    "identity_hate log loss: 0.028804451678192445\n",
    "[[1. 0.]\n",
    " [1. 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_12 (Embedding)     (None, 100, 126)          3024000   \n",
      "_________________________________________________________________\n",
      "bidirectional_12 (Bidirectio (None, 100, 108)          58968     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_12 (Glo (None, 108)               0         \n",
      "_________________________________________________________________\n",
      "dropout_23 (Dropout)         (None, 108)               0         \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 49)                5341      \n",
      "_________________________________________________________________\n",
      "dropout_24 (Dropout)         (None, 49)                0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 6)                 300       \n",
      "=================================================================\n",
      "Total params: 3,088,609\n",
      "Trainable params: 3,088,609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_model = load_model(file_path, custom_objects={ })\n",
    "final_model.summary()\n",
    "y_test = final_model.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv(\"output/keras_tuned_0.0426.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plots\n",
    "# my_plots = ['loss', 'acc']\n",
    "# for plot in my_plots:\n",
    "#     plt.plot(history.history[plot])\n",
    "#     plt.plot(history.history['val_' + plot])\n",
    "#     plt.title('model ' + plot)\n",
    "#     plt.ylabel(plot)\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.legend(['train', 'test'], loc='upper left')\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
