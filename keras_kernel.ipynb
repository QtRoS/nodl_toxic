{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns \n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 9)\n",
    "sns.set(context='paper', style='darkgrid', rc={'figure.facecolor':'white'}, font_scale=1.2)\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout, GRU, LeakyReLU\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras import regularizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_features = 24000  # TODO\n",
    "maxlen = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"input/train.csv\")\n",
    "test = pd.read_csv(\"input/test.csv\")\n",
    "train = train.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 2)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159571, 8)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>136211</th>\n",
       "      <td>d8a64df4f787e25c</td>\n",
       "      <td>\"similar statement was made about methane. I r...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158053</th>\n",
       "      <td>e8009916c2ed8733</td>\n",
       "      <td>\"\\n\\n The difference between a first serializa...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44702</th>\n",
       "      <td>77728ec3cd3557bf</td>\n",
       "      <td>\"\\n\\nThat was already discussed above. Just re...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65845</th>\n",
       "      <td>b015b87aa218d5e4</td>\n",
       "      <td>Concerning Peter Cushing\\nThere is a local son...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104842</th>\n",
       "      <td>30ecfea816cf7821</td>\n",
       "      <td>Okay, bin Laden. You just keep concentrating o...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "136211  d8a64df4f787e25c  \"similar statement was made about methane. I r...   \n",
       "158053  e8009916c2ed8733  \"\\n\\n The difference between a first serializa...   \n",
       "44702   77728ec3cd3557bf  \"\\n\\nThat was already discussed above. Just re...   \n",
       "65845   b015b87aa218d5e4  Concerning Peter Cushing\\nThere is a local son...   \n",
       "104842  30ecfea816cf7821  Okay, bin Laden. You just keep concentrating o...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "136211      0             0        0       0       0              0  \n",
       "158053      0             0        0       0       0              0  \n",
       "44702       0             0        0       0       0              0  \n",
       "65845       0             0        0       0       0              0  \n",
       "104842      1             0        0       0       0              0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train[list_classes].values\n",
    "list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = text.Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "X_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "file_path=\"weights_base.best.hdf5\"\n",
    "checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "early = EarlyStopping(monitor='val_loss', mode='min', patience=20)\n",
    "callbacks = [checkpoint, early]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    0.904156\n",
      "1    0.095844\n",
      "Name: toxic, dtype: float64\n",
      "0    0.990004\n",
      "1    0.009996\n",
      "Name: severe_toxic, dtype: float64\n",
      "0    0.947052\n",
      "1    0.052948\n",
      "Name: obscene, dtype: float64\n",
      "0    0.997004\n",
      "1    0.002996\n",
      "Name: threat, dtype: float64\n",
      "0    0.950636\n",
      "1    0.049364\n",
      "Name: insult, dtype: float64\n",
      "0    0.991195\n",
      "1    0.008805\n",
      "Name: identity_hate, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "for col in list_classes:\n",
    "    print(train[col].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, output_dim=128)(inp)\n",
    "    x = Bidirectional(LSTM(50, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(50, activation='relu', kernel_regularizer=regularizers.l2(0.001))(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                 optimizer='adam', # nadam\n",
    "                 metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1213: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0725 - acc: 0.9781Epoch 00001: val_loss improved from inf to 0.05502, saving model to weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 843s 6ms/step - loss: 0.0725 - acc: 0.9781 - val_loss: 0.0550 - val_acc: 0.9809\n",
      "Epoch 2/2\n",
      "143584/143613 [============================>.] - ETA: 0s - loss: 0.0501 - acc: 0.9822Epoch 00002: val_loss improved from 0.05502 to 0.05048, saving model to weights_base.best.hdf5\n",
      "143613/143613 [==============================] - 841s 6ms/step - loss: 0.0501 - acc: 0.9822 - val_loss: 0.0505 - val_acc: 0.9824\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "history = model.fit(X_t, y, batch_size=32, epochs=2, validation_split=0.1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Train on 86265 samples, validate on 9586 samples\n",
    "Epoch 1/2\n",
    "86240/86265 [============================>.] - ETA: 0s - loss: 0.0829 - acc: 0.9765Epoch 00001: val_loss did not improve\n",
    "86265/86265 [==============================] - 652s 8ms/step - loss: 0.0829 - acc: 0.9765 - val_loss: 0.0556 - val_acc: 0.9812\n",
    "Epoch 2/2\n",
    "86240/86265 [============================>.] - ETA: 0s - loss: 0.0505 - acc: 0.9823Epoch 00002: val_loss did not improve\n",
    "86265/86265 [==============================] - 703s 8ms/step - loss: 0.0504 - acc: 0.9823 - val_loss: 0.0549 - val_acc: 0.9817"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.load_weights(file_path)\n",
    "y_test = model.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv(\"output/keras_baseline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 7)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_submission.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 100)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_te.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model_with_params(p):\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, output_dim=p['embedding_size'])(inp)\n",
    "    if p['cell_type']=='lstm':\n",
    "        cell = LSTM(int(p['units']), return_sequences=True)  # , dropout=p['dropout_r'], recurrent_dropout=p['dropout_r']\n",
    "    else:\n",
    "        cell = GRU(int(p['units']), return_sequences=True)  # , dropout=p['dropout_r'], recurrent_dropout=p['dropout_r']\n",
    "    x = Bidirectional(cell)(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(p['dropout_1'])(x)\n",
    "    x = Dense(p['dense_1'], activation='relu')(x)  #     x = LeakyReLU()(x)\n",
    "    x = Dropout(p['dropout_2'])(x)\n",
    "    x = Dense(6, activation='sigmoid')(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy', optimizer=p['opt_algo'], metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with params: {'batch_size': 20, 'cell_type': 'lstm', 'dense_1': 52, 'dropout_1': 0.0, 'dropout_2': 0.15000000000000002, 'dropout_r': 0, 'embedding_size': 121, 'epochs': 2, 'opt_algo': 'adam', 'units': 44}\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86260/86265 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9781Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 349s 4ms/step - loss: 0.0673 - acc: 0.9781 - val_loss: 0.0512 - val_acc: 0.9821\n",
      "Epoch 2/2\n",
      "86260/86265 [============================>.] - ETA: 0s - loss: 0.0455 - acc: 0.9830Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 352s 4ms/step - loss: 0.0455 - acc: 0.9830 - val_loss: 0.0513 - val_acc: 0.9820\n",
      "\tScore 0.051197376069030846\n",
      "\n",
      "Training with params: {'batch_size': 16, 'cell_type': 'lstm', 'dense_1': 46, 'dropout_1': 0.15000000000000002, 'dropout_2': 0.17500000000000002, 'dropout_r': 0, 'embedding_size': 66, 'epochs': 2, 'opt_algo': 'adam', 'units': 51}\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86256/86265 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.9773Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 369s 4ms/step - loss: 0.0700 - acc: 0.9773 - val_loss: 0.0512 - val_acc: 0.9817\n",
      "Epoch 2/2\n",
      "86256/86265 [============================>.] - ETA: 0s - loss: 0.0482 - acc: 0.9823Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 375s 4ms/step - loss: 0.0482 - acc: 0.9823 - val_loss: 0.0515 - val_acc: 0.9821\n",
      "\tScore 0.05123978088192693\n",
      "\n",
      "Training with params: {'batch_size': 21, 'cell_type': 'gru', 'dense_1': 64, 'dropout_1': 0.05, 'dropout_2': 0.025, 'dropout_r': 0, 'embedding_size': 83, 'epochs': 2, 'opt_algo': 'adam', 'units': 43}\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86247/86265 [============================>.] - ETA: 0s - loss: 0.0647 - acc: 0.9786Epoch 00001: val_loss improved from 0.04911 to 0.04893, saving model to weights_base.best.hdf5\n",
      "86265/86265 [==============================] - 268s 3ms/step - loss: 0.0647 - acc: 0.9786 - val_loss: 0.0489 - val_acc: 0.9830\n",
      "Epoch 2/2\n",
      "86247/86265 [============================>.] - ETA: 0s - loss: 0.0437 - acc: 0.9834Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 262s 3ms/step - loss: 0.0437 - acc: 0.9834 - val_loss: 0.0510 - val_acc: 0.9822\n",
      "\tScore 0.04893448421454191\n",
      "\n",
      "Training with params: {'batch_size': 21, 'cell_type': 'gru', 'dense_1': 53, 'dropout_1': 0.15000000000000002, 'dropout_2': 0.05, 'dropout_r': 0, 'embedding_size': 99, 'epochs': 2, 'opt_algo': 'adam', 'units': 62}\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86247/86265 [============================>.] - ETA: 0s - loss: 0.0650 - acc: 0.9783Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 289s 3ms/step - loss: 0.0650 - acc: 0.9783 - val_loss: 0.0518 - val_acc: 0.9811\n",
      "Epoch 2/2\n",
      "86247/86265 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9831Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 290s 3ms/step - loss: 0.0453 - acc: 0.9831 - val_loss: 0.0531 - val_acc: 0.9827\n",
      "\tScore 0.051779852909776276\n",
      "\n",
      "Training with params: {'batch_size': 26, 'cell_type': 'gru', 'dense_1': 50, 'dropout_1': 0.05, 'dropout_2': 0.17500000000000002, 'dropout_r': 0, 'embedding_size': 69, 'epochs': 2, 'opt_algo': 'adam', 'units': 60}\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86242/86265 [============================>.] - ETA: 0s - loss: 0.0686 - acc: 0.9775Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 224s 3ms/step - loss: 0.0686 - acc: 0.9775 - val_loss: 0.0499 - val_acc: 0.9823\n",
      "Epoch 2/2\n",
      "86242/86265 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9831Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 223s 3ms/step - loss: 0.0451 - acc: 0.9831 - val_loss: 0.0503 - val_acc: 0.9825\n",
      "\tScore 0.049941981030475945\n",
      "\n",
      "Training with params: {'batch_size': 20, 'cell_type': 'gru', 'dense_1': 52, 'dropout_1': 0.1, 'dropout_2': 0.05, 'dropout_r': 0, 'embedding_size': 104, 'epochs': 2, 'opt_algo': 'adam', 'units': 56}\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86260/86265 [============================>.] - ETA: 0s - loss: 0.0642 - acc: 0.9788Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 304s 4ms/step - loss: 0.0642 - acc: 0.9788 - val_loss: 0.0503 - val_acc: 0.9823\n",
      "Epoch 2/2\n",
      "86260/86265 [============================>.] - ETA: 0s - loss: 0.0451 - acc: 0.9831Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 304s 4ms/step - loss: 0.0451 - acc: 0.9831 - val_loss: 0.0493 - val_acc: 0.9829\n",
      "\tScore 0.049310520171415545\n",
      "\n",
      "Training with params: {'batch_size': 25, 'cell_type': 'lstm', 'dense_1': 59, 'dropout_1': 0.025, 'dropout_2': 0.07500000000000001, 'dropout_r': 0, 'embedding_size': 79, 'epochs': 2, 'opt_algo': 'adam', 'units': 49}\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86250/86265 [============================>.] - ETA: 0s - loss: 0.0700 - acc: 0.9776Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 268s 3ms/step - loss: 0.0700 - acc: 0.9776 - val_loss: 0.0498 - val_acc: 0.9828\n",
      "Epoch 2/2\n",
      "86250/86265 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9834Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 263s 3ms/step - loss: 0.0449 - acc: 0.9834 - val_loss: 0.0492 - val_acc: 0.9825\n",
      "\tScore 0.04917899466115043\n",
      "\n",
      "Training with params: {'batch_size': 20, 'cell_type': 'lstm', 'dense_1': 62, 'dropout_1': 0.07500000000000001, 'dropout_2': 0.2, 'dropout_r': 0, 'embedding_size': 96, 'epochs': 2, 'opt_algo': 'adam', 'units': 56}\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86260/86265 [============================>.] - ETA: 0s - loss: 0.0679 - acc: 0.9779Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 327s 4ms/step - loss: 0.0679 - acc: 0.9779 - val_loss: 0.0514 - val_acc: 0.9818\n",
      "Epoch 2/2\n",
      "86260/86265 [============================>.] - ETA: 0s - loss: 0.0467 - acc: 0.9828Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 332s 4ms/step - loss: 0.0467 - acc: 0.9828 - val_loss: 0.0504 - val_acc: 0.9825\n",
      "\tScore 0.05038965380900554\n",
      "\n",
      "Training with params: {'batch_size': 32, 'cell_type': 'gru', 'dense_1': 61, 'dropout_1': 0.07500000000000001, 'dropout_2': 0.15000000000000002, 'dropout_r': 0, 'embedding_size': 75, 'epochs': 2, 'opt_algo': 'adam', 'units': 44}\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86240/86265 [============================>.] - ETA: 0s - loss: 0.0691 - acc: 0.9780Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 183s 2ms/step - loss: 0.0691 - acc: 0.9780 - val_loss: 0.0507 - val_acc: 0.9819\n",
      "Epoch 2/2\n",
      "86240/86265 [============================>.] - ETA: 0s - loss: 0.0448 - acc: 0.9833Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 178s 2ms/step - loss: 0.0448 - acc: 0.9833 - val_loss: 0.0499 - val_acc: 0.9822\n",
      "\tScore 0.04992404733192826\n",
      "\n",
      "Training with params: {'batch_size': 28, 'cell_type': 'lstm', 'dense_1': 52, 'dropout_1': 0.07500000000000001, 'dropout_2': 0.1, 'dropout_r': 0, 'embedding_size': 82, 'epochs': 2, 'opt_algo': 'adam', 'units': 61}\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86240/86265 [============================>.] - ETA: 0s - loss: 0.0709 - acc: 0.9773Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 250s 3ms/step - loss: 0.0709 - acc: 0.9773 - val_loss: 0.0505 - val_acc: 0.9818\n",
      "Epoch 2/2\n",
      "86240/86265 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9830Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 247s 3ms/step - loss: 0.0454 - acc: 0.9830 - val_loss: 0.0499 - val_acc: 0.9825\n",
      "\tScore 0.049907292968803585\n",
      "\n",
      "Training with params: {'batch_size': 32, 'cell_type': 'gru', 'dense_1': 49, 'dropout_1': 0.0, 'dropout_2': 0.125, 'dropout_r': 0, 'embedding_size': 99, 'epochs': 2, 'opt_algo': 'adam', 'units': 63}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86240/86265 [============================>.] - ETA: 0s - loss: 0.0681 - acc: 0.9779Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 211s 2ms/step - loss: 0.0681 - acc: 0.9779 - val_loss: 0.0510 - val_acc: 0.9824\n",
      "Epoch 2/2\n",
      "86240/86265 [============================>.] - ETA: 0s - loss: 0.0442 - acc: 0.9834Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 209s 2ms/step - loss: 0.0442 - acc: 0.9834 - val_loss: 0.0495 - val_acc: 0.9828\n",
      "\tScore 0.04949153492787972\n",
      "\n",
      "Training with params: {'batch_size': 18, 'cell_type': 'gru', 'dense_1': 53, 'dropout_1': 0.025, 'dropout_2': 0.1, 'dropout_r': 0, 'embedding_size': 75, 'epochs': 2, 'opt_algo': 'adam', 'units': 52}\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86256/86265 [============================>.] - ETA: 0s - loss: 0.0639 - acc: 0.9789Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 300s 3ms/step - loss: 0.0639 - acc: 0.9789 - val_loss: 0.0494 - val_acc: 0.9826\n",
      "Epoch 2/2\n",
      "86256/86265 [============================>.] - ETA: 0s - loss: 0.0441 - acc: 0.9834Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 304s 4ms/step - loss: 0.0441 - acc: 0.9834 - val_loss: 0.0491 - val_acc: 0.9831\n",
      "\tScore 0.049074465629109665\n",
      "\n",
      "Training with params: {'batch_size': 31, 'cell_type': 'lstm', 'dense_1': 55, 'dropout_1': 0.025, 'dropout_2': 0.1, 'dropout_r': 0, 'embedding_size': 116, 'epochs': 2, 'opt_algo': 'adam', 'units': 59}\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86242/86265 [============================>.] - ETA: 0s - loss: 0.0689 - acc: 0.9778Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 259s 3ms/step - loss: 0.0689 - acc: 0.9778 - val_loss: 0.0520 - val_acc: 0.9814\n",
      "Epoch 2/2\n",
      "86242/86265 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9830Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 255s 3ms/step - loss: 0.0456 - acc: 0.9830 - val_loss: 0.0497 - val_acc: 0.9823\n",
      "\tScore 0.04971459474542928\n",
      "\n",
      "Training with params: {'batch_size': 16, 'cell_type': 'lstm', 'dense_1': 64, 'dropout_1': 0.125, 'dropout_2': 0.125, 'dropout_r': 0, 'embedding_size': 80, 'epochs': 2, 'opt_algo': 'adam', 'units': 48}\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86256/86265 [============================>.] - ETA: 0s - loss: 0.0707 - acc: 0.9771Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 382s 4ms/step - loss: 0.0707 - acc: 0.9771 - val_loss: 0.0504 - val_acc: 0.9820\n",
      "Epoch 2/2\n",
      "86256/86265 [============================>.] - ETA: 0s - loss: 0.0470 - acc: 0.9826Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 383s 4ms/step - loss: 0.0470 - acc: 0.9826 - val_loss: 0.0514 - val_acc: 0.9824\n",
      "\tScore 0.050394979134184906\n",
      "\n",
      "Training with params: {'batch_size': 31, 'cell_type': 'lstm', 'dense_1': 45, 'dropout_1': 0.1, 'dropout_2': 0.05, 'dropout_r': 0, 'embedding_size': 84, 'epochs': 2, 'opt_algo': 'adam', 'units': 44}\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86242/86265 [============================>.] - ETA: 0s - loss: 0.0728 - acc: 0.9769Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 217s 3ms/step - loss: 0.0728 - acc: 0.9769 - val_loss: 0.0505 - val_acc: 0.9819\n",
      "Epoch 2/2\n",
      "86242/86265 [============================>.] - ETA: 0s - loss: 0.0468 - acc: 0.9825Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 216s 3ms/step - loss: 0.0468 - acc: 0.9825 - val_loss: 0.0509 - val_acc: 0.9823\n",
      "\tScore 0.050495151119862795\n",
      "\n",
      "Training with params: {'batch_size': 18, 'cell_type': 'lstm', 'dense_1': 60, 'dropout_1': 0.05, 'dropout_2': 0.025, 'dropout_r': 0, 'embedding_size': 78, 'epochs': 2, 'opt_algo': 'adam', 'units': 61}\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86256/86265 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9775Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 360s 4ms/step - loss: 0.0673 - acc: 0.9775 - val_loss: 0.0503 - val_acc: 0.9821\n",
      "Epoch 2/2\n",
      "86256/86265 [============================>.] - ETA: 0s - loss: 0.0454 - acc: 0.9830Epoch 00002: val_loss improved from 0.04893 to 0.04851, saving model to weights_base.best.hdf5\n",
      "86265/86265 [==============================] - 359s 4ms/step - loss: 0.0454 - acc: 0.9830 - val_loss: 0.0485 - val_acc: 0.9830\n",
      "\tScore 0.048508304528513235\n",
      "\n",
      "Training with params: {'batch_size': 22, 'cell_type': 'gru', 'dense_1': 50, 'dropout_1': 0.07500000000000001, 'dropout_2': 0.07500000000000001, 'dropout_r': 0, 'embedding_size': 73, 'epochs': 2, 'opt_algo': 'adam', 'units': 63}\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86262/86265 [============================>.] - ETA: 0s - loss: 0.0657 - acc: 0.9786Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 265s 3ms/step - loss: 0.0657 - acc: 0.9786 - val_loss: 0.0507 - val_acc: 0.9821\n",
      "Epoch 2/2\n",
      "86262/86265 [============================>.] - ETA: 0s - loss: 0.0456 - acc: 0.9829Epoch 00002: val_loss improved from 0.04851 to 0.04845, saving model to weights_base.best.hdf5\n",
      "86265/86265 [==============================] - 265s 3ms/step - loss: 0.0456 - acc: 0.9829 - val_loss: 0.0484 - val_acc: 0.9828\n",
      "\tScore 0.048448401929657575\n",
      "\n",
      "Training with params: {'batch_size': 19, 'cell_type': 'lstm', 'dense_1': 57, 'dropout_1': 0.07500000000000001, 'dropout_2': 0.07500000000000001, 'dropout_r': 0, 'embedding_size': 117, 'epochs': 2, 'opt_algo': 'adam', 'units': 40}\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86260/86265 [============================>.] - ETA: 0s - loss: 0.0665 - acc: 0.9783Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 352s 4ms/step - loss: 0.0665 - acc: 0.9783 - val_loss: 0.0503 - val_acc: 0.9822\n",
      "Epoch 2/2\n",
      "86260/86265 [============================>.] - ETA: 0s - loss: 0.0464 - acc: 0.9826Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 350s 4ms/step - loss: 0.0464 - acc: 0.9826 - val_loss: 0.0504 - val_acc: 0.9825\n",
      "\tScore 0.050333319082383525\n",
      "\n",
      "Training with params: {'batch_size': 22, 'cell_type': 'gru', 'dense_1': 59, 'dropout_1': 0.025, 'dropout_2': 0.125, 'dropout_r': 0, 'embedding_size': 93, 'epochs': 2, 'opt_algo': 'adam', 'units': 60}\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86262/86265 [============================>.] - ETA: 0s - loss: 0.0673 - acc: 0.9778Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 274s 3ms/step - loss: 0.0673 - acc: 0.9778 - val_loss: 0.0487 - val_acc: 0.9822\n",
      "Epoch 2/2\n",
      "86262/86265 [============================>.] - ETA: 0s - loss: 0.0453 - acc: 0.9830Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 271s 3ms/step - loss: 0.0453 - acc: 0.9830 - val_loss: 0.0506 - val_acc: 0.9814\n",
      "\tScore 0.048742454771063023\n",
      "\n",
      "Training with params: {'batch_size': 21, 'cell_type': 'gru', 'dense_1': 46, 'dropout_1': 0.125, 'dropout_2': 0.07500000000000001, 'dropout_r': 0, 'embedding_size': 89, 'epochs': 2, 'opt_algo': 'adam', 'units': 49}\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86247/86265 [============================>.] - ETA: 0s - loss: 0.0654 - acc: 0.9785Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 271s 3ms/step - loss: 0.0654 - acc: 0.9785 - val_loss: 0.0524 - val_acc: 0.9820\n",
      "Epoch 2/2\n",
      "86247/86265 [============================>.] - ETA: 0s - loss: 0.0449 - acc: 0.9832Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 268s 3ms/step - loss: 0.0449 - acc: 0.9832 - val_loss: 0.0501 - val_acc: 0.9828\n",
      "\tScore 0.05012524252503863\n",
      "\n",
      "Training with params: {'batch_size': 22, 'cell_type': 'gru', 'dense_1': 60, 'dropout_1': 0.05, 'dropout_2': 0.025, 'dropout_r': 0, 'embedding_size': 78, 'epochs': 2, 'opt_algo': 'adam', 'units': 64}\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86262/86265 [============================>.] - ETA: 0s - loss: 0.0630 - acc: 0.9790Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 263s 3ms/step - loss: 0.0630 - acc: 0.9790 - val_loss: 0.0494 - val_acc: 0.9826\n",
      "Epoch 2/2\n",
      "86262/86265 [============================>.] - ETA: 0s - loss: 0.0436 - acc: 0.9835Epoch 00002: val_loss improved from 0.04845 to 0.04780, saving model to weights_base.best.hdf5\n",
      "86265/86265 [==============================] - 264s 3ms/step - loss: 0.0436 - acc: 0.9835 - val_loss: 0.0478 - val_acc: 0.9825\n",
      "\tScore 0.047803207477315195\n",
      "\n",
      "Training with params: {'batch_size': 22, 'cell_type': 'gru', 'dense_1': 47, 'dropout_1': 0.1, 'dropout_2': 0.025, 'dropout_r': 0, 'embedding_size': 120, 'epochs': 2, 'opt_algo': 'adam', 'units': 64}\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86262/86265 [============================>.] - ETA: 0s - loss: 0.0635 - acc: 0.9790Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 295s 3ms/step - loss: 0.0635 - acc: 0.9790 - val_loss: 0.0494 - val_acc: 0.9824\n",
      "Epoch 2/2\n",
      "86262/86265 [============================>.] - ETA: 0s - loss: 0.0438 - acc: 0.9836Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 293s 3ms/step - loss: 0.0438 - acc: 0.9836 - val_loss: 0.0487 - val_acc: 0.9830\n",
      "\tScore 0.048722916858535706\n",
      "\n",
      "Training with params: {'batch_size': 22, 'cell_type': 'gru', 'dense_1': 50, 'dropout_1': 0.05, 'dropout_2': 0.05, 'dropout_r': 0, 'embedding_size': 73, 'epochs': 2, 'opt_algo': 'adam', 'units': 45}\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86262/86265 [============================>.] - ETA: 0s - loss: 0.0658 - acc: 0.9784Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 248s 3ms/step - loss: 0.0658 - acc: 0.9784 - val_loss: 0.0493 - val_acc: 0.9826\n",
      "Epoch 2/2\n",
      "86262/86265 [============================>.] - ETA: 0s - loss: 0.0444 - acc: 0.9834Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 245s 3ms/step - loss: 0.0444 - acc: 0.9834 - val_loss: 0.0490 - val_acc: 0.9825\n",
      "\tScore 0.049022152191323345\n",
      "\n",
      "Training with params: {'batch_size': 23, 'cell_type': 'gru', 'dense_1': 54, 'dropout_1': 0.05, 'dropout_2': 0.07500000000000001, 'dropout_r': 0, 'embedding_size': 68, 'epochs': 2, 'opt_algo': 'adam', 'units': 46}\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86250/86265 [============================>.] - ETA: 0s - loss: 0.0684 - acc: 0.9780Epoch 00001: val_loss did not improve\n",
      "86265/86265 [==============================] - 235s 3ms/step - loss: 0.0684 - acc: 0.9780 - val_loss: 0.0501 - val_acc: 0.9823\n",
      "Epoch 2/2\n",
      "86250/86265 [============================>.] - ETA: 0s - loss: 0.0445 - acc: 0.9833Epoch 00002: val_loss did not improve\n",
      "86265/86265 [==============================] - 232s 3ms/step - loss: 0.0445 - acc: 0.9833 - val_loss: 0.0525 - val_acc: 0.9828\n",
      "\tScore 0.05013735340687872\n",
      "\n",
      "Best: {'batch_size': 6, 'cell_type': 1, 'dense_1': 16, 'dropout_1': 0.05, 'dropout_2': 0.025, 'embedding_size': 14, 'opt_algo': 0, 'units': 24}\n",
      "CPU times: user 20h 46min 42s, sys: 5h 26min 20s, total: 1d 2h 13min 2s\n",
      "Wall time: 3h 45min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "\n",
    "def score(p):\n",
    "    print(\"Training with params:\", p)\n",
    "    model = get_model_with_params(p)\n",
    "    h = model.fit(X_t, y, batch_size=p['batch_size'], epochs=p['epochs'], validation_split=0.1, callbacks=callbacks)\n",
    "    score = min(h.history['val_loss']) #h.history['val_loss'][-1]\n",
    "    print(\"\\tScore {0}\\n\".format(score))\n",
    "    return {'loss': score, 'status': STATUS_OK}\n",
    "\n",
    "def optimize():\n",
    "    trials = Trials()\n",
    "    space = {\n",
    "        'batch_size' : hp.choice('batch_size', np.arange(16, 33, dtype=int)),\n",
    "        'dropout_1': hp.quniform('dropout_1', 0.00, 0.15, 0.025),\n",
    "        'dropout_2': hp.quniform('dropout_2', 0.025, 0.2, 0.025),\n",
    "        'dropout_r': 0, # hp.quniform('dropout_r', 0.00, 0.15, 0.025),\n",
    "        'dense_1': hp.choice('dense_1', np.arange(44, 65, dtype=int)),\n",
    "        'cell_type': hp.choice('cell_type', ['lstm', 'gru']),\n",
    "        'embedding_size': hp.choice('embedding_size', np.arange(64, 129, dtype=int)), # [64, 96, 128]\n",
    "        'units': hp.choice('units', np.arange(40, 65, dtype=int)),\n",
    "        'opt_algo': hp.choice('opt_algo', ['adam']),  # 'rmsprop', 'nadam', \n",
    "        'epochs': 2,\n",
    "    }\n",
    "\n",
    "    best = fmin(score, space, algo=tpe.suggest, trials=trials, max_evals=24)\n",
    "\n",
    "    print(\"Best:\", best)\n",
    "    return best, trials\n",
    "\n",
    "b, t = optimize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1247: calling reduce_sum (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1213: calling reduce_max (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1349: calling reduce_mean (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 80)           1920000   \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 100)          39300     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                6464      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 390       \n",
      "=================================================================\n",
      "Total params: 1,966,154\n",
      "Trainable params: 1,966,154\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/2\n",
      "86240/86265 [============================>.] - ETA: 0s - loss: 0.0684 - acc: 0.9781Epoch 00001: val_loss improved from inf to 0.05107, saving model to weights_base.best.hdf5\n",
      "86265/86265 [==============================] - 187s 2ms/step - loss: 0.0684 - acc: 0.9781 - val_loss: 0.0511 - val_acc: 0.9819\n",
      "Epoch 2/2\n",
      "86240/86265 [============================>.] - ETA: 0s - loss: 0.0443 - acc: 0.9835Epoch 00002: val_loss improved from 0.05107 to 0.04911, saving model to weights_base.best.hdf5\n",
      "86265/86265 [==============================] - 183s 2ms/step - loss: 0.0443 - acc: 0.9835 - val_loss: 0.0491 - val_acc: 0.9826\n"
     ]
    }
   ],
   "source": [
    "p = {'cell_type': 'gru', 'opt_algo': 'adam', 'units': 50, 'batch_size': 32, 'embedding_size': 80,  \n",
    "     'dense_1': 64, 'dropout_1': 0.05, 'dropout_2': 0.05, 'dropout_r': 0.0, 'epochs': 2}\n",
    "manual_model = get_model_with_params(p)\n",
    "manual_model.summary()\n",
    "h = manual_model.fit(X_t, y, batch_size=p['batch_size'], epochs=p['epochs'], validation_split=0.1, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from crossvalidation import multilabel_cross_validation, multilabel_label_combinations\n",
    "from multilabel_classifier import MultilabelClassifier\n",
    "from transform_pipeline import TransformPipeline\n",
    "\n",
    "#from nltk.tokenize import wordpunct_tokenize\n",
    "#from nltk.stem.snowball import EnglishStemmer\n",
    "#from nltk.stem import WordNetLemmatizer\n",
    "#from functools import lru_cache\n",
    "\n",
    "#from textblob import TextBlob\n",
    "\n",
    "#from collections import OrderedDict\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "from visualizations import topn_features, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# class ModelTransformer(TransformerMixin):\n",
    "\n",
    "#     def __init__(self, model):\n",
    "#         self.model = model\n",
    "\n",
    "#     def fit(self, *args, **kwargs):\n",
    "#         self.model.fit(*args, **kwargs)\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, X, **transform_params):\n",
    "#         return DataFrame(self.model.predict(X))\n",
    "    \n",
    "\n",
    "class KerasAnswerExtractor(TransformerMixin):\n",
    "\n",
    "    def __init__(self, model, column_name):\n",
    "        self.model = model\n",
    "        self.column_name = column_name\n",
    "\n",
    "    def fit(self, *args, **kwargs):\n",
    "        #self.model.fit(*args, **kwargs)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, **transform_params):\n",
    "        return X\n",
    "    \n",
    "    def predict(self, X, *args, **kwargs):\n",
    "        prediction = (self.model.predict(X)[:, list_classes.index(self.column_name)])\n",
    "        return prediction.reshape(len(prediction), -1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted negative</th>\n",
       "      <th>predicted positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.986284</td>\n",
       "      <td>0.013716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.137722</td>\n",
       "      <td>0.862278</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          predicted negative  predicted positive\n",
       "negative            0.986284            0.013716\n",
       "positive            0.137722            0.862278"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_name = 'toxic'\n",
    "class_y = train[class_name].values\n",
    "confusion_matrix(KerasAnswerExtractor(manual_model, class_name),\n",
    "                 X_t,\n",
    "                 class_y.reshape(len(class_y), -1), \n",
    "                 do_fit=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_22 (InputLayer)        (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_22 (Embedding)     (None, 100, 78)           1872000   \n",
      "_________________________________________________________________\n",
      "bidirectional_22 (Bidirectio (None, 100, 128)          54912     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_22 (Glo (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_43 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_43 (Dense)             (None, 60)                7740      \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 60)                0         \n",
      "_________________________________________________________________\n",
      "dense_44 (Dense)             (None, 6)                 366       \n",
      "=================================================================\n",
      "Total params: 1,935,018\n",
      "Trainable params: 1,935,018\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "toxic log loss: 0.0697806057753072\n",
      "[[0.99196435 0.00803565]\n",
      " [0.19811627 0.80188373]]\n",
      "severe_toxic log loss: 0.02059794293569234\n",
      "[[9.99926227e-01 7.37727378e-05]\n",
      " [9.52331606e-01 4.76683938e-02]]\n",
      "obscene log loss: 0.04093004938620502\n",
      "[[0.99375152 0.00624848]\n",
      " [0.17302799 0.82697201]]\n",
      "threat log loss: 0.0117805910445291\n",
      "[[1. 0.]\n",
      " [1. 0.]]\n",
      "insult log loss: 0.05263961792770885\n",
      "[[0.99088773 0.00911227]\n",
      " [0.26211962 0.73788038]]\n",
      "identity_hate log loss: 0.020706462390103108\n",
      "[[9.99663289e-01 3.36710965e-04]\n",
      " [8.45208845e-01 1.54791155e-01]]\n"
     ]
    }
   ],
   "source": [
    "final_model = load_model(file_path, custom_objects={ })\n",
    "final_model.summary()\n",
    "y_train = final_model.predict(X_t)\n",
    "for class_name in list_classes:\n",
    "    one_column_answer = y_train[:, list_classes.index(class_name)]\n",
    "    pred = one_column_answer.reshape(len(one_column_answer), -1)\n",
    "    class_y = train[class_name].values\n",
    "    cm = confusion_matrix(class_y.reshape(len(class_y), -1), (pred > 0.5).astype(int))\n",
    "    print(class_name, \"log loss:\", log_loss(class_y.reshape(len(class_y), -1), pred))\n",
    "    print(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for class_name in list_classes:\n",
    "#     clf = KerasAnswerExtractor(manual_model, class_name)\n",
    "#     pred = clf.predict(X_t)\n",
    "#     class_y = train[class_name].values\n",
    "#     cm = confusion_matrix(class_y.reshape(len(class_y), -1), (pred > 0.5).astype(int))\n",
    "#     print(class_name, \"log loss:\", log_loss(class_y.reshape(len(class_y), -1), pred))\n",
    "#     print(cm.astype('float') / cm.sum(axis=1)[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toxic log loss: 0.07620896772834636\n",
    "[[0.98964371 0.01035629]\n",
    " [0.20082278 0.79917722]]\n",
    "severe_toxic log loss: 0.024454393614495473\n",
    "[[0.99581603 0.00418397]\n",
    " [0.53678756 0.46321244]]\n",
    "obscene log loss: 0.04788283939460444\n",
    "[[0.98856098 0.01143902]\n",
    " [0.13114112 0.86885888]]\n",
    "threat log loss: 0.0137781501289922\n",
    "[[9.99989534e-01 1.04661629e-05]\n",
    " [9.96721311e-01 3.27868852e-03]]\n",
    "insult log loss: 0.06003329518399972\n",
    "[[0.98591441 0.01408559]\n",
    " [0.20986359 0.79013641]]\n",
    "identity_hate log loss: 0.02840215769807751\n",
    "[[9.99905300e-01 9.46999590e-05]\n",
    " [9.87714988e-01 1.22850123e-02]]\n",
    "\n",
    "\n",
    "toxic log loss: 0.07290494285026734\n",
    "[[0.98549888 0.01450112]\n",
    " [0.13467576 0.86532424]]\n",
    "severe_toxic log loss: 0.023505014466099847\n",
    "[[0.99798706 0.00201294]\n",
    " [0.68082902 0.31917098]]\n",
    "obscene log loss: 0.04956906263386151\n",
    "[[0.98674263 0.01325737]\n",
    " [0.1233118  0.8766882 ]]\n",
    "threat log loss: 0.013953790726962609\n",
    "[[1. 0.]\n",
    " [1. 0.]]\n",
    "insult log loss: 0.06100324271304353\n",
    "[[0.98399315 0.01600685]\n",
    " [0.20587618 0.79412382]]\n",
    "identity_hate log loss: 0.028804451678192445\n",
    "[[1. 0.]\n",
    " [1. 0.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0],\n",
       "       [0],\n",
       "       [0],\n",
       "       ...,\n",
       "       [0],\n",
       "       [0],\n",
       "       [0]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(pred > 0.5).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9008559668824437"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(class_y.reshape(len(class_y), -1), pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_5 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_5 (Embedding)      (None, 100, 128)          3072000   \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, 100, 100)          53700     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 51)                5151      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 51)                0         \n",
      "_________________________________________________________________\n",
      "dropout_10 (Dropout)         (None, 51)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 6)                 312       \n",
      "=================================================================\n",
      "Total params: 3,131,163\n",
      "Trainable params: 3,131,163\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "final_model = load_model(file_path, custom_objects={ })\n",
    "final_model.summary()\n",
    "y_test = final_model.predict(X_te)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sample_submission = pd.read_csv(\"input/sample_submission.csv\")\n",
    "sample_submission[list_classes] = y_test\n",
    "sample_submission.to_csv(\"output/keras_tuned_0.0457.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plots\n",
    "# my_plots = ['loss', 'acc']\n",
    "# for plot in my_plots:\n",
    "#     plt.plot(history.history[plot])\n",
    "#     plt.plot(history.history['val_' + plot])\n",
    "#     plt.title('model ' + plot)\n",
    "#     plt.ylabel(plot)\n",
    "#     plt.xlabel('epoch')\n",
    "#     plt.legend(['train', 'test'], loc='upper left')\n",
    "#     plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
