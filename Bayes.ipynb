{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mrqtros/Software/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/home/mrqtros/Software/anaconda3/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.4 of module '_catboost' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from nltk.corpus import stopwords  # Import the stop word list\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import ExtraTreesClassifier, ExtraTreesRegressor\n",
    "from sklearn.ensemble import AdaBoostClassifier, AdaBoostRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.layers import Flatten, Dense, Activation, Dropout\n",
    "from keras.models import Model, Sequential\n",
    "from keras.regularizers import l2\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.callbacks import TensorBoard  \n",
    "from keras import backend as K\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import xgboost as xgb\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.facecolor'] = 'white'\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 9)\n",
    "sns.set(context='paper', style='darkgrid', rc={'figure.facecolor':'white'}, font_scale=1.2)\n",
    "\n",
    "# import logging       \n",
    "# logging.basicConfig(filename='./my_framework.txt', filemode='a', datefmt='%H:%M:%S', level=logging.DEBUG,\n",
    "#                     format='%(asctime)s %(levelname)s %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftrain = pd.read_csv(\"input/train.csv\")\n",
    "dftrain['comment_text'] = dftrain['comment_text'].apply(str) # some values parsed as float\n",
    "dftest = pd.read_csv(\"input/test.csv\")\n",
    "dftest['comment_text'] = dftest['comment_text'].apply(str) # some values parsed as float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_stops = set(stopwords.words(\"english\"))\n",
    "def preprocess_line(raw_line):\n",
    "    raw_line = re.sub('\\\\b\\\\d+\\\\b', ' ', raw_line)\n",
    "    raw_line = re.sub('[\\';:.,<>#*\"\\-=/?!№\\[\\]()«»_|\\\\\\\\…•+%]', ' ', raw_line)\n",
    "    words = raw_line.lower().split()\n",
    "    meaningful_words = [w for w in words if w not in english_stops]\n",
    "    return \" \".join(meaningful_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO IS IT WORTH DOING?\n",
    "dftrain['comment_text_preprocessed'] = dftrain['comment_text'].apply(preprocess_line)\n",
    "dftest['comment_text_preprocessed'] = dftest['comment_text'].apply(preprocess_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_text_preprocessed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nonsense kiss geek said true account terminated</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>please vandalize pages edit w merwin continue ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>points interest removed points interest sectio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>asking nationality racial offence wow aware bl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>reader going say ethereal vocal style dark lyr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  toxic  \\\n",
       "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
       "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
       "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
       "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4  79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                           comment_text_preprocessed  \n",
       "0    nonsense kiss geek said true account terminated  \n",
       "1  please vandalize pages edit w merwin continue ...  \n",
       "2  points interest removed points interest sectio...  \n",
       "3  asking nationality racial offence wow aware bl...  \n",
       "4  reader going say ethereal vocal style dark lyr...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain.head()\n",
    "# dftrain['comment_text_preprocessed'][0], dftrain['comment_text'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(max_features=60000)\n",
    "X = vectorizer.fit_transform(dftrain['comment_text_preprocessed'])\n",
    "y = dftrain['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction = clf.predict_proba(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18122484797361807"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y, prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rare word Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_counts = {}\n",
    "for sentence in dftrain['comment_text']:\n",
    "    sentence = sentence.lower()\n",
    "    for word in sentence.split():\n",
    "        word = word.strip('0123456789,.\" /:;-•()|—=&%!?~\\'+·{}#[]<>*@←→\\\\☎^_£✉►…♣©♦†☼¢♠♥™à✍«»₪±∇∆♫°≠ω`´≈✎★ᛏ☯✔')\n",
    "        if word in word_counts:\n",
    "            word_counts[word] += 1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "plain_list = sorted(word_counts.items(), key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('plain_my.txt', 'w') as plain_out:\n",
    "    for w,c in plain_list:\n",
    "        plain_out.write('%6s %s\\n' % (c,w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "118 ===d~~penis\n",
    "60 fc*k \n",
    "54 faggot!!!!jéské (https://encyclopediadramatica.rs/J%C3%A9sk%C3%A9_Couriano)\n",
    "50 font-size:85% (53 style=\"\"background style=\"\"background-color 43 style=\"\"border-spacing:8px;margin:0px 29 style=\"\"vertical-align)\n",
    "46 lunchables!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!1lol\n",
    "41 t@lk 13 †@1₭ 14 трёп\n",
    "38 ☺ 18 ｡◕‿◕｡\n",
    "37 asshole!!!this\n",
    "34 freedom!freedom!freedom!freedom!freedom!freedom!freedom!freedom!freedom!freedom!freedom!freedom!freedom!freedom!freedom!freedom!\n",
    "33 f@ggot\n",
    "32 zzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzzz\n",
    "30 f*** 30 f***ing 9 bullsh*t 18 fu*k 36 f*cking 60 fc*k 69 f*ck 1 f*ucked 1 su**s 1 s*#t 1 nig*ger 1 mothaf*cka \n",
    "27 fuck!{{unblock}}lifetime\n",
    "24 you!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
    "22 f**k 12 sh*t 15 f**king 22 *** 8 a**hole 11 fuck's \n",
    "19 fuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuckfuck\n",
    "18 #fdffe 12 #eef 12 #fee 12 #dde 12 #edd\n",
    "14 http://rexcurry.net/pledge2.html\n",
    "12 bullshido\n",
    "10 gayreek (https://www.urbandictionary.com/define.php?term=gayreek)\n",
    "10 kool\n",
    "10 suki (Даааааааааааааа :) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://justenglish.me/2014/07/18/lol-omg-and-ily-60-of-the-dominating-abbreviations/ \n",
    "wtf\n",
    "omfg\n",
    "ffs\n",
    "stfu\n",
    "wth\n",
    "ftw\n",
    "mf (motherfucker)\n",
    "gtfo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Отдельно все запиканные\n",
    "=      1 a***es\n",
    "=      1 a**hoe\n",
    "=      1 a*fgan\n",
    "=      1 a*s\n",
    "=      1 apparently*cough\n",
    "=      1 ars*hole\n",
    "=      1 as**ole\n",
    "=      1 as*fu*king\n",
    "=      1 as*hol*s\n",
    "=      1 as*hole\n",
    "=      1 ass*hole\n",
    "=      1 assh*le's\n",
    "=      1 assh*ole\n",
    "=      1 b******d\n",
    "=      1 b***er\n",
    "=      1 b**ching\n",
    "=      1 b**tard\n",
    "=      1 b*ch\n",
    "=      1 b*lls\n",
    "=      1 b*stard\n",
    "=      1 b*ugger\n",
    "=      1 ba*tard\n",
    "=      1 beyond*the\n",
    "=      1 bi*ch\n",
    "=      1 bl**dy\n",
    "=      1 bulls*it\n",
    "=      1 c*-algebras\n",
    "=      1 c*ck\n",
    "=      1 c*itadel\n",
    "=      1 c*ntfaced\n",
    "=      1 c*nts\n",
    "=      1 clusterf*ck\n",
    "=      1 cocksu*_cking\n",
    "=      1 cough*...(yes\n",
    "=      1 cough*encylopedia\n",
    "=      1 cough*like\n",
    "=      1 cr*p\n",
    "=      1 d***s\n",
    "=      1 d**b**s\n",
    "=      1 d*ck\n",
    "=      1 d*irty\n",
    "=      1 d*mb\n",
    "=      1 d*mn\n",
    "=      1 di*ks\n",
    "=      1 dqa1*0501:dqb\n",
    "=      1 dqa1*0505:dqb\n",
    "=      1 edio*magica\n",
    "=      1 edito*magica\n",
    "=      1 f*(&n\n",
    "=      1 f*(x\n",
    "=      1 f**%!ng\n",
    "=      1 f*****g\n",
    "=      1 f****er\n",
    "=      1 f****r\n",
    "=      1 f***ers\n",
    "=      1 f***in\n",
    "=      1 f***king\n",
    "=      1 f**ck\n",
    "=      1 f**k's\n",
    "=      1 f**k**g\n",
    "=      1 f**kin\n",
    "=      1 f**kker\n",
    "=      1 f*ck*ng\n",
    "=      1 f*ck.and\n",
    "=      1 f*ckhead\n",
    "=      1 f*ckin\n",
    "=      1 f*ggots\n",
    "=      1 f*u*c*k\n",
    "=      1 f*ucked\n",
    "=      1 f*ucking\n",
    "=      1 f_uc*ck\n",
    "=      1 facts*”\n",
    "=      1 fagg*t\n",
    "=      1 fu*er\n",
    "=      1 fu*khead\n",
    "=      1 fu*king\n",
    "=      1 fu*kwit\n",
    "=      1 fuckfu*ker\n",
    "=      1 g***y\n",
    "=      1 g*d\n",
    "=      1 g*y\n",
    "=      1 gam*boy\n",
    "=      1 h*ll\n",
    "=      1 h*r\n",
    "=      1 h*tler\n",
    "=      1 haven*t\n",
    "=      1 hon*ey\n",
    "=      1 id*ot\n",
    "=      1 id*t\n",
    "=      1 j*sus\n",
    "=      1 khasnya*thud\n",
    "=      1 l*a*b\n",
    "=      1 m*'s\n",
    "=      1 m*(1+i\n",
    "=      1 m*(1+r)*(1+p\n",
    "=      1 m*n\n",
    "=      1 ma**yks\n",
    "=      1 macy*s\n",
    "=      1 moderf***n\n",
    "=      1 mot*herfuc*ker\n",
    "=      1 moth*rfucker\n",
    "=      1 mothaf*cka\n",
    "=      1 mother****er\n",
    "=      1 motherf***in\n",
    "=      1 motherf**ker\n",
    "=      1 motherf**king\n",
    "=      1 motherf*cking\n",
    "=      1 motherfu*_ccker\n",
    "=      1 musashi*]'s\n",
    "=      1 n*****r\n",
    "=      1 n****r\n",
    "=      1 n**ger\n",
    "=      1 n*gga\n",
    "=      1 n*m\n",
    "=      1 n*zi\n",
    "=      1 nig*er\n",
    "=      1 nig*ger\n",
    "=      1 nudge*”\n",
    "=      1 p**r**b***y\n",
    "=      1 p**tang\n",
    "=      1 p*erverts\n",
    "=      1 p*opd*ck\n",
    "=      1 p*rverts\n",
    "=      1 p*ss\n",
    "=      1 page*-they\n",
    "=      1 per*ert\n",
    "=      1 pi*g\n",
    "=      1 pi*n\n",
    "=      1 pr***s\n",
    "=      1 pri*k\n",
    "=      1 q1*q2/(4pir\n",
    "=      1 r*(meters/ohm\n",
    "=      1 r*t*rd\n",
    "=      1 re***d\n",
    "=      1 ret*rds\n",
    "=      1 s*#t\n",
    "=      1 s*****”\n",
    "=      1 s***-standard\n",
    "=      1 s*cks\n",
    "=      1 s*d\n",
    "=      1 s*it\n",
    "=      1 septegram*talk*contributions\n",
    "=      1 sh**brained\n",
    "=      1 sh**t\n",
    "=      1 sh*thole\n",
    "=      1 sh*ting\n",
    "=      1 sh*tty\n",
    "=      1 starfu*k\n",
    "=      1 su**s\n",
    "=      1 swifty*talk\n",
    "=      1 t*rd\n",
    "=      1 t*ts\n",
    "=      1 t_amb*(gamma\n",
    "=      1 tr***s\n",
    "=      1 u*v\n",
    "=      1 un*sexyness\n",
    "=      1 v*rgins\n",
    "=      1 wh*r\n",
    "=      1 wh*re\n",
    "=      1 yaj*mart\n",
    "=      2 a***-retentive\n",
    "=      2 b*ll\n",
    "=      2 c*******ing\n",
    "=      2 c*****shhh\n",
    "=      2 c***s\n",
    "=      2 c**t\n",
    "=      2 co**sucker\n",
    "=      2 cr*ppy\n",
    "=      2 doesn*t\n",
    "=      2 f***er\n",
    "=      2 f**in\n",
    "=      2 f*k\n",
    "=      2 f*p\n",
    "=      2 ri*(meters/ohm\n",
    "=      2 s*n\n",
    "=      2 t*m\n",
    "=      2 t*n\n",
    "=      2 tw*t\n",
    "=      3 assh*le\n",
    "=      3 b***h\n",
    "=      3 bulls**t\n",
    "=      3 c*nt\n",
    "=      3 d**n\n",
    "=      3 di*k\n",
    "=      3 f**cking\n",
    "=      3 fuc*in\n",
    "=      3 motherf*cker\n",
    "=      3 t*u\n",
    "=      3 wal*mart\n",
    "=      4 b**ch\n",
    "=      4 f*cked\n",
    "=      4 f*cker\n",
    "=      4 f*ing\n",
    "=      4 fu*ker.sikh\n",
    "=      4 fuc*ers\n",
    "=      4 y*z\n",
    "=      5 b*tch\n",
    "=      5 fuc*ing\n",
    "=      6 f**ks\n",
    "=      6 fu*ker\n",
    "=      6 s**t\n",
    "=      7 m*a*s*h\n",
    "=      8 a**hole\n",
    "=      9 bullsh*t\n",
    "=     15 f**king\n",
    "=     17 sh*t\n",
    "=     18 fu*k\n",
    "=     22 f**k\n",
    "=     30 f***ing\n",
    "=     36 f*cking\n",
    "=     60 fc*k\n",
    "=     69 f*ck"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = dftrain['comment_text_preprocessed']\n",
    "y = y = dftrain['toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gscv_summary(grid, print_all=False, name='UKNWN'):\n",
    "    print(\"[%s] Best score/params: %s %s\" % (name, grid.best_score_, grid.best_params_))\n",
    "#     logging.debug(\"[%s] Best score/params: %s %s\" % (name, grid.best_score_, grid.best_params_))\n",
    "    if not print_all: \n",
    "        return\n",
    "    print(\"All params:\")\n",
    "    means = grid.cv_results_['mean_test_score']\n",
    "    stds = grid.cv_results_['std_test_score']\n",
    "    for mean, std, params in zip(means, stds, grid.cv_results_['params']):\n",
    "        print(\"%0.3f (+/-%0.03f) for %r\"\n",
    "              % (mean, std * 2, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer()),\n",
    "    ('mnb', MultinomialNB())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 12 candidates, totalling 60 fits\n",
      "[CV] tfidf__binary=True, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=True, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=True, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=True, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=True, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=True, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=True, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=True, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=True, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=True, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=True, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=True, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=True, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=True, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=True, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=True, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.0s\n",
      "[CV] tfidf__binary=True, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.1s\n",
      "[CV] tfidf__binary=True, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.2s\n",
      "[CV] tfidf__binary=True, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.4s\n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.4s\n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.6s\n",
      "[CV] tfidf__binary=True, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=True, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.4s\n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.5s\n",
      "[CV] tfidf__binary=True, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.2s\n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   6.0s\n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.3s\n",
      "[CV] tfidf__binary=True, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.4s\n",
      "[CV] tfidf__binary=True, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=True, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 tasks      | elapsed:   10.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] tfidf__binary=True, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=True, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=True, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.5s\n",
      "[CV] tfidf__binary=True, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   6.1s\n",
      "[CV] tfidf__binary=True, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.7s\n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.9s\n",
      "[CV] tfidf__binary=False, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=False, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.0s\n",
      "[CV] tfidf__binary=False, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.4s\n",
      "[CV] tfidf__binary=False, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.2s\n",
      "[CV] tfidf__binary=False, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.3s\n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.7s\n",
      "[CV] tfidf__binary=False, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.4s\n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.3s\n",
      "[CV] tfidf__binary=False, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=False, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.2s\n",
      "[CV] tfidf__binary=False, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=False, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   6.3s\n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.4s\n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.3s\n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.4s\n",
      "[CV] tfidf__binary=False, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   6.0s\n",
      "[CV] tfidf__binary=False, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=False, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=False, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=True, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.9s\n",
      "[CV] tfidf__binary=False, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=False, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.4s\n",
      "[CV] tfidf__binary=False, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.8s\n",
      "[CV] tfidf__binary=False, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.3s\n",
      "[CV] tfidf__binary=False, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.4s\n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.2s\n",
      "[CV] tfidf__binary=False, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=False, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.4s\n",
      "[CV] tfidf__binary=False, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.3s\n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.4s\n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.7s\n",
      "[CV] tfidf__binary=False, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=False, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] tfidf__binary=False, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.4s\n",
      "[CV] tfidf__binary=False, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   6.2s\n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6400, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   6.2s\n",
      "[CV] tfidf__binary=False, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV] tfidf__binary=False, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.8s\n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.5s\n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   6.2s\n",
      "[CV] tfidf__binary=False, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.5s\n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.7s\n",
      "[CV] tfidf__binary=False, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True \n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.6s\n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   5.0s\n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   4.5s\n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   3.5s\n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   4.7s\n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   4.0s\n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6500, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   4.7s\n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   4.5s\n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   4.3s\n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=True, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   4.7s\n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   4.8s\n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   4.6s\n",
      "[CV]  tfidf__binary=False, tfidf__max_features=6600, tfidf__norm=l2, tfidf__smooth_idf=False, tfidf__sublinear_tf=True, tfidf__use_idf=True, total=   4.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   37.5s remaining:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done  60 out of  60 | elapsed:   37.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MNB] Best score/params: -0.14088461628285465 {'tfidf__binary': False, 'tfidf__max_features': 6500, 'tfidf__norm': 'l2', 'tfidf__smooth_idf': False, 'tfidf__sublinear_tf': True, 'tfidf__use_idf': True}\n",
      "CPU times: user 6.18 s, sys: 1.39 s, total: 7.57 s\n",
      "Wall time: 41.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "param_grid = {\n",
    "    'tfidf__max_features': [6400, 6500, 6600],\n",
    "#     'tfidf__stop_words': ['english', None],\n",
    "    'tfidf__binary': [True, False],\n",
    "    'tfidf__norm': ['l2'],\n",
    "    'tfidf__smooth_idf': [True, False],\n",
    "    'tfidf__sublinear_tf': [True],\n",
    "    'tfidf__use_idf': [True],\n",
    "#     'mnb__alpha': [0.0, 1.0, 2.0],\n",
    "#     'mnb__fit_prior': [True, False],\n",
    "}\n",
    "grid = GridSearchCV(estimator=pipeline, param_grid=param_grid, cv=5, scoring='neg_log_loss', n_jobs=-1, verbose=2)\n",
    "grid.fit(X, y)\n",
    "gscv_summary(grid, name='MNB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Kernel NBSVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/jhoward/nb-svm-strong-linear-baseline-eda-0-052-lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.utils.validation import check_X_y, check_is_fitted\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy import sparse\n",
    "class NbSvmClassifier(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, C=1.0, dual=False, n_jobs=1):\n",
    "        self.C = C\n",
    "        self.dual = dual\n",
    "        self.n_jobs = n_jobs\n",
    "\n",
    "    def predict(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict(x.multiply(self._r))\n",
    "\n",
    "    def predict_proba(self, x):\n",
    "        # Verify that model has been fit\n",
    "        check_is_fitted(self, ['_r', '_clf'])\n",
    "        return self._clf.predict_proba(x.multiply(self._r))\n",
    "\n",
    "    def fit(self, x, y):\n",
    "        # Check that X and y have correct shape\n",
    "        y = y.values\n",
    "        x, y = check_X_y(x, y, accept_sparse=True)\n",
    "\n",
    "        def pr(x, y_i, y):\n",
    "            p = x[y==y_i].sum(0)\n",
    "            return (p+1) / ((y==y_i).sum()+1)\n",
    "\n",
    "        self._r = sparse.csr_matrix(np.log(pr(x,1,y) / pr(x,0,y)))\n",
    "        x_nb = x.multiply(self._r)\n",
    "        self._clf = LogisticRegression(C=self.C, dual=self.dual, n_jobs=self.n_jobs).fit(x_nb, y)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('./input/train.csv')\n",
    "test = pd.read_csv('./input/test.csv')\n",
    "subm = pd.read_csv('./input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9.585100e+04</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "      <td>95851.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>4.994359e+11</td>\n",
       "      <td>0.096368</td>\n",
       "      <td>0.010068</td>\n",
       "      <td>0.053301</td>\n",
       "      <td>0.003182</td>\n",
       "      <td>0.049713</td>\n",
       "      <td>0.008492</td>\n",
       "      <td>0.897862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.890136e+11</td>\n",
       "      <td>0.295097</td>\n",
       "      <td>0.099832</td>\n",
       "      <td>0.224635</td>\n",
       "      <td>0.056320</td>\n",
       "      <td>0.217352</td>\n",
       "      <td>0.091762</td>\n",
       "      <td>0.302831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>2.225664e+07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.473437e+11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.001297e+11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>7.501088e+11</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>9.999882e+11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id         toxic  severe_toxic       obscene        threat  \\\n",
       "count  9.585100e+04  95851.000000  95851.000000  95851.000000  95851.000000   \n",
       "mean   4.994359e+11      0.096368      0.010068      0.053301      0.003182   \n",
       "std    2.890136e+11      0.295097      0.099832      0.224635      0.056320   \n",
       "min    2.225664e+07      0.000000      0.000000      0.000000      0.000000   \n",
       "25%    2.473437e+11      0.000000      0.000000      0.000000      0.000000   \n",
       "50%    5.001297e+11      0.000000      0.000000      0.000000      0.000000   \n",
       "75%    7.501088e+11      0.000000      0.000000      0.000000      0.000000   \n",
       "max    9.999882e+11      1.000000      1.000000      1.000000      1.000000   \n",
       "\n",
       "             insult  identity_hate          none  \n",
       "count  95851.000000   95851.000000  95851.000000  \n",
       "mean       0.049713       0.008492      0.897862  \n",
       "std        0.217352       0.091762      0.302831  \n",
       "min        0.000000       0.000000      0.000000  \n",
       "25%        0.000000       0.000000      1.000000  \n",
       "50%        0.000000       0.000000      1.000000  \n",
       "75%        0.000000       0.000000      1.000000  \n",
       "max        1.000000       1.000000      1.000000  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "train['none'] = 1-train[label_cols].max(axis=1)\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "COMMENT = 'comment_text'\n",
    "train[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "test[COMMENT].fillna(\"unknown\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re, string\n",
    "re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "def tokenize(s): \n",
    "    return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n = train.shape[0]\n",
    "vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=True,\n",
    "               smooth_idf=True, sublinear_tf=True)\n",
    "trn_term_doc = vec.fit_transform(train[COMMENT])\n",
    "test_term_doc = vec.transform(test[COMMENT])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285100"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vec.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fit: toxic\n",
      "toxic log loss: 0.0683931911152053\n",
      "CV mean -0.10984694199293106 std 0.002453268100769032\n",
      "raw [-0.11233353 -0.11285825 -0.10970464 -0.10765906 -0.10667923]\n",
      "Fit: severe_toxic\n",
      "severe_toxic log loss: 0.008496710919669564\n",
      "CV mean -0.029764572797230233 std 0.0017811032304001543\n",
      "raw [-0.03100353 -0.03099577 -0.03155157 -0.02816316 -0.02710884]\n",
      "Fit: obscene\n",
      "obscene log loss: 0.031324106570019404\n",
      "CV mean -0.06168139148446224 std 0.0026725920672013596\n",
      "raw [-0.06027316 -0.05797601 -0.0643994  -0.06508642 -0.06067196]\n",
      "Fit: threat\n",
      "threat log loss: 0.0020182706494972194\n",
      "CV mean -0.011488439300173603 std 0.0008807495410935281\n",
      "raw [-0.01236054 -0.01243243 -0.01001941 -0.0113807  -0.01124912]\n",
      "Fit: insult\n",
      "insult log loss: 0.038527573970870324\n",
      "CV mean -0.07788704113017045 std 0.0015969837820944425\n",
      "raw [-0.07649849 -0.07868979 -0.07622583 -0.08057502 -0.07744607]\n",
      "Fit: identity_hate\n",
      "identity_hate log loss: 0.007024088231383722\n",
      "CV mean -0.028086126746360927 std 0.0005808056435436677\n",
      "raw [-0.02803159 -0.02730418 -0.02769012 -0.02898158 -0.02842316]\n"
     ]
    }
   ],
   "source": [
    "# preds = np.zeros((len(test), len(label_cols)))\n",
    "cvs = []\n",
    "for i, column in enumerate(label_cols):\n",
    "    print('Fit:', column)\n",
    "    model = NbSvmClassifier()\n",
    "    model.fit(trn_term_doc, train[column])\n",
    "    pred = model.predict_proba(trn_term_doc)\n",
    "    ll = log_loss(train[column], pred)\n",
    "    print(column, \"log loss:\", ll)\n",
    "    scores = cross_val_score(model, trn_term_doc, train[column], cv=5, scoring='neg_log_loss')\n",
    "    print(f\"CV mean {scores.mean()} std {scores.std()}\\nraw {scores}\")\n",
    "    cvs.append(scores.mean())\n",
    "#     preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.053125752241888084\n"
     ]
    }
   ],
   "source": [
    "print(sum(cvs) / len(cvs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
