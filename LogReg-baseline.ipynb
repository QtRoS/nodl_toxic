{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from crossvalidation import multilabel_cross_validation\n",
    "from multilabel_classifier import MultilabelClassifier\n",
    "from transform_pipeline import TransformPipeline\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from functools import lru_cache\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation\n",
    "\n",
    "I'll use features based on:\n",
    "\n",
    "- comment text \"as is\"\n",
    "- stemmed/lemmatized comment text\n",
    "- (maybe) sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain = pd.read_csv(\"input/train.csv\")\n",
    "dftrain['comment_text'] = dftrain['comment_text'].apply(str) # some values parsed as float\n",
    "dftest = pd.read_csv(\"input/test.csv\")\n",
    "dftest['comment_text'] = dftest['comment_text'].apply(str) # some values parsed as float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  toxic  \\\n",
       "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
       "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
       "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
       "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4  79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6044863</td>\n",
       "      <td>==Orphaned non-free media (Image:41cD1jboEvL. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6102620</td>\n",
       "      <td>::Kentuckiana is colloquial.  Even though the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14563293</td>\n",
       "      <td>Hello fellow Wikipedians,\\nI have just modifie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21086297</td>\n",
       "      <td>AKC Suspensions \\nThe Morning Call - Feb 24, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22982444</td>\n",
       "      <td>== [WIKI_LINK: Talk:Celts] ==</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text\n",
       "0   6044863  ==Orphaned non-free media (Image:41cD1jboEvL. ...\n",
       "1   6102620  ::Kentuckiana is colloquial.  Even though the ...\n",
       "2  14563293  Hello fellow Wikipedians,\\nI have just modifie...\n",
       "3  21086297  AKC Suspensions \\nThe Morning Call - Feb 24, 2...\n",
       "4  22982444                      == [WIKI_LINK: Talk:Celts] =="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = EnglishStemmer()\n",
    "\n",
    "@lru_cache(30000)\n",
    "def stem_word(word):\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "@lru_cache(30000)\n",
    "def lemmatize_word(word):\n",
    "    return lemmatizer.lemmatize(word)\n",
    "\n",
    "def simplify_text(text, simplifier):\n",
    "    tokens = wordpunct_tokenize(text.lower())\n",
    "    simplified_tokens = map(simplifier, tokens)\n",
    "    return \" \".join(simplified_tokens)\n",
    "\n",
    "def simplify_texts(texts, simplifier):\n",
    "    return [simplify_text(text, simplifier) for text in tqdm(texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 95851/95851 [00:17<00:00, 5506.56it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 226998/226998 [00:42<00:00, 5358.53it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 95851/95851 [00:14<00:00, 6462.49it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 226998/226998 [00:27<00:00, 8211.61it/s]\n"
     ]
    }
   ],
   "source": [
    "dftrain['comment_text_stemmed'] = simplify_texts(dftrain['comment_text'], stem_word)\n",
    "dftest['comment_text_stemmed'] = simplify_texts(dftest['comment_text'], stem_word)\n",
    "dftrain['comment_text_lemmatized'] = simplify_texts(dftrain['comment_text'], lemmatize_word)\n",
    "dftest['comment_text_lemmatized'] = simplify_texts(dftest['comment_text'], lemmatize_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(df):\n",
    "    blobs = [TextBlob(text) for text in tqdm(df['comment_text'])]\n",
    "    sentiments = [blob.sentiment for blob in tqdm(blobs)]\n",
    "    polatity = np.array([sentiment.polarity for sentiment in sentiments])\n",
    "    subjectivity = np.array([sentiment.subjectivity for sentiment in sentiments])\n",
    "    sentiment = polatity * subjectivity\n",
    "    return polatity, subjectivity, sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 95851/95851 [00:01<00:00, 48018.71it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 95851/95851 [02:07<00:00, 750.74it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 226998/226998 [00:06<00:00, 35007.00it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 226998/226998 [05:37<00:00, 672.65it/s]\n"
     ]
    }
   ],
   "source": [
    "dftrain['polarity'], dftrain['subjectivity'], dftrain['sentiment'] = sentiment(dftrain)\n",
    "dftest['polarity'], dftest['subjectivity'], dftest['sentiment'] = sentiment(dftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_text_stemmed</th>\n",
       "      <th>comment_text_lemmatized</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nonsens ? kiss off , geek . what i said is tru...</td>\n",
       "      <td>nonsense ? kiss off , geek . what i said is tr...</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.227500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" pleas do not vandal page , as you did with t...</td>\n",
       "      <td>\" please do not vandalize page , a you did wit...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" \"\" point of interest \"\" i remov the \"\" point...</td>\n",
       "      <td>\" \"\" point of interest \"\" i removed the \"\" poi...</td>\n",
       "      <td>-0.040625</td>\n",
       "      <td>0.771875</td>\n",
       "      <td>-0.031357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ask some his nation is a racial offenc . wow w...</td>\n",
       "      <td>asking some his nationality is a racial offenc...</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>the reader here is not go by my say so for eth...</td>\n",
       "      <td>the reader here is not going by my say so for ...</td>\n",
       "      <td>-0.075000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.018750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  toxic  \\\n",
       "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
       "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
       "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
       "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4  79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                comment_text_stemmed  \\\n",
       "0  nonsens ? kiss off , geek . what i said is tru...   \n",
       "1  \" pleas do not vandal page , as you did with t...   \n",
       "2  \" \"\" point of interest \"\" i remov the \"\" point...   \n",
       "3  ask some his nation is a racial offenc . wow w...   \n",
       "4  the reader here is not go by my say so for eth...   \n",
       "\n",
       "                             comment_text_lemmatized  polarity  subjectivity  \\\n",
       "0  nonsense ? kiss off , geek . what i said is tr...  0.350000      0.650000   \n",
       "1  \" please do not vandalize page , a you did wit...  0.000000      0.000000   \n",
       "2  \" \"\" point of interest \"\" i removed the \"\" poi... -0.040625      0.771875   \n",
       "3  asking some his nationality is a racial offenc...  0.175000      0.625000   \n",
       "4  the reader here is not going by my say so for ... -0.075000      0.250000   \n",
       "\n",
       "   sentiment  \n",
       "0   0.227500  \n",
       "1   0.000000  \n",
       "2  -0.031357  \n",
       "3   0.109375  \n",
       "4  -0.018750  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy model\n",
    "\n",
    "Let's build model that return 0.5 for each label probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.69314718,  0.69314718,  0.69314718])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultilabelClassifier([\n",
    "    DummyRegressor(strategy='constant', constant=0.5)\n",
    "    for _ in range(6)\n",
    "])\n",
    "multilabel_cross_validation(clf,\n",
    "                            np.zeros([len(dftrain), 1]),\n",
    "                            np.array(dftrain[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05563897,  0.05629918,  0.05564405])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', TfidfVectorizer()),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(6)\n",
    "    ]))\n",
    "])\n",
    "multilabel_cross_validation(clf,\n",
    "                            dftrain['comment_text'],\n",
    "                            np.array(dftrain[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05410874,  0.05448132,  0.05392603])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', TfidfVectorizer()),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(6)\n",
    "    ]))\n",
    "])\n",
    "multilabel_cross_validation(clf,\n",
    "                dftrain['comment_text_stemmed'],\n",
    "                np.array(dftrain[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05456093,  0.05511094,  0.05463724])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', TfidfVectorizer()),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(6)\n",
    "    ]))\n",
    "])\n",
    "multilabel_cross_validation(clf,\n",
    "                dftrain['comment_text_lemmatized'],\n",
    "                np.array(dftrain[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for bare logistic regression - stemmed text is a best choise.\n",
    "\n",
    "# Character features\n",
    "\n",
    "Let's add character n-gram based features (from source text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05096648,  0.05124373,  0.05062442])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', FeatureUnion([\n",
    "        ('words', TransformPipeline([\n",
    "            ('stemmed', FunctionTransformer(lambda X: X[:, 0], validate=False)),\n",
    "            ('vec', TfidfVectorizer()),\n",
    "        ])),\n",
    "        ('chars', TransformPipeline([\n",
    "            ('text', FunctionTransformer(lambda X: X[:, 1], validate=False)),\n",
    "            ('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(6)\n",
    "    ]))\n",
    "])\n",
    "multilabel_cross_validation(clf,\n",
    "                dftrain[['comment_text_stemmed', 'comment_text']],\n",
    "                np.array(dftrain[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's show top 30 features - from words list and from character n-grams list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topn_features(clf, n):\n",
    "    weights = np.abs(np.stack([\n",
    "        estimator.coef_[0]\n",
    "        for estimator in clf.steps[1][1].estimators\n",
    "    ]).max(axis=0))\n",
    "    word_tfidf = clf.steps[0][1].transformer_list[0][1].steps[1][1]\n",
    "    words = np.array(word_tfidf.get_feature_names())\n",
    "    chars_tfidf = clf.steps[0][1].transformer_list[1][1].steps[1][1]\n",
    "    chars = np.array(chars_tfidf.get_feature_names())\n",
    "    word_weights = weights[0:len(words)]\n",
    "    char_weights = weights[len(words): len(words) + len(chars)]\n",
    "    sorted_word_weights_indices = word_weights.argsort()[::-1]\n",
    "    sorted_char_weights_indices = char_weights.argsort()[::-1]\n",
    "    top_words = words[sorted_word_weights_indices[:n]]\n",
    "    top_words_weights = word_weights[sorted_word_weights_indices[:n]]\n",
    "    top_words_dict = OrderedDict([\n",
    "        (word, weight)\n",
    "        for word, weight in zip(top_words, top_words_weights)\n",
    "    ])\n",
    "    top_chars = chars[sorted_char_weights_indices[:n]]\n",
    "    top_chars_weights = char_weights[sorted_char_weights_indices[:n]]\n",
    "    top_chars_dict = OrderedDict([\n",
    "        (char, weight)\n",
    "        for char, weight in zip(top_chars, top_chars_weights)\n",
    "    ])\n",
    "    return top_words_dict, top_chars_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('fuck', 22.256612793114751),\n",
       "              ('idiot', 19.499189613074524),\n",
       "              ('shit', 16.627050488263766),\n",
       "              ('stupid', 16.320979068380609),\n",
       "              ('bullshit', 16.236003420307831),\n",
       "              ('bitch', 15.682711191483374),\n",
       "              ('ass', 14.570270833755579),\n",
       "              ('asshol', 13.1267843780647),\n",
       "              ('pussi', 13.091680790135561),\n",
       "              ('nigger', 12.802405334832791),\n",
       "              ('crap', 11.852676828939789),\n",
       "              ('peni', 11.476155729194247),\n",
       "              ('nigga', 11.189665961834825),\n",
       "              ('suck', 11.003739007093383),\n",
       "              ('kill', 10.942203500100117),\n",
       "              ('bastard', 10.939507236513503),\n",
       "              ('cunt', 10.910946776785497),\n",
       "              ('pathet', 10.696496106666764),\n",
       "              ('dick', 10.672477301479374),\n",
       "              ('die', 10.506547418470959),\n",
       "              ('moron', 10.295647343955036),\n",
       "              ('faggot', 10.241505137630787),\n",
       "              ('homosexu', 9.9298817683185145),\n",
       "              ('shoot', 9.8953405639902705),\n",
       "              ('gay', 9.5303342051885078),\n",
       "              ('hell', 9.0507210631210473),\n",
       "              ('whore', 8.7322506175088943),\n",
       "              ('nazi', 8.2980276861986422),\n",
       "              ('ck', 8.0713885282310436),\n",
       "              ('jerk', 8.0251566842725541)]),\n",
       " OrderedDict([('fuc', 20.567423839300904),\n",
       "              ('uck', 17.357164026559708),\n",
       "              ('sex', 13.913331254697738),\n",
       "              ('hit', 10.189584984170502),\n",
       "              ('dum', 9.5771429216052866),\n",
       "              ('CK', 9.4785159605986138),\n",
       "              ('bag', 8.5763262819486403),\n",
       "              ('ck', 8.0050954045474363),\n",
       "              ('*', 7.2488944995329661),\n",
       "              ('ian', 6.4389684578421402),\n",
       "              ('kk', 6.1365040408538496),\n",
       "              ('ole', 6.007176432873357),\n",
       "              ('coc', 5.9516618649392017),\n",
       "              ('gg', 5.9279703820334007),\n",
       "              ('U', 5.5948577046970529),\n",
       "              (' b', 5.5725357051062829),\n",
       "              ('!', 5.5673821482062751),\n",
       "              ('shi', 5.5102551790557648),\n",
       "              ('k', 5.3410451492312143),\n",
       "              ('FUC', 5.1996552685467501),\n",
       "              ('fuk', 5.1061895357113718),\n",
       "              ('a ', 4.9889504849942998),\n",
       "              ('ass', 4.9277365394859309),\n",
       "              ('ant', 4.8634622085083867),\n",
       "              ('F', 4.8322299251519203),\n",
       "              ('G', 4.7504290892109671),\n",
       "              ('ss ', 4.7454563202505469),\n",
       "              ('h', 4.6423734525396734),\n",
       "              (' je', 4.6019523346470752),\n",
       "              ('fa', 4.5336491609467799)]))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topn_features(clf, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per-class error/features visualization\n",
    "\n",
    "## Toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10511021,  0.10260038,  0.10240544])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', FeatureUnion([\n",
    "        ('words', TransformPipeline([\n",
    "            ('stemmed', FunctionTransformer(lambda X: X[:, 0], validate=False)),\n",
    "            ('vec', TfidfVectorizer()),\n",
    "        ])),\n",
    "        ('chars', TransformPipeline([\n",
    "            ('text', FunctionTransformer(lambda X: X[:, 1], validate=False)),\n",
    "            ('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(1)\n",
    "    ]))\n",
    "])\n",
    "multilabel_cross_validation(clf,\n",
    "                dftrain[['comment_text_stemmed', 'comment_text']],\n",
    "                np.array(dftrain[['toxic']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('fuck', 21.556863310636921),\n",
       "              ('idiot', 19.310399690208996),\n",
       "              ('shit', 17.108999100303659),\n",
       "              ('stupid', 16.373437740915879),\n",
       "              ('bullshit', 16.262366358222966),\n",
       "              ('ass', 13.507173437598825),\n",
       "              ('asshol', 13.160349136903445),\n",
       "              ('crap', 11.627398715155017),\n",
       "              ('peni', 11.515223645680223),\n",
       "              ('bitch', 11.01325263311629),\n",
       "              ('suck', 11.009684635219401),\n",
       "              ('bastard', 10.844944458429749),\n",
       "              ('dick', 10.793772454481308),\n",
       "              ('pathet', 10.593332732154837),\n",
       "              ('moron', 10.43499772724507),\n",
       "              ('faggot', 9.9707598124878185),\n",
       "              ('hell', 9.0134137320652066),\n",
       "              ('whore', 8.8995634547882965),\n",
       "              ('thank', 8.6532420679754409),\n",
       "              ('nigger', 8.5201402199601173),\n",
       "              ('cunt', 8.4793882287285207),\n",
       "              ('nazi', 8.3095274484037756),\n",
       "              ('pussi', 8.2465764898954621),\n",
       "              ('ck', 8.1937007870360397),\n",
       "              ('retard', 8.0124925243727958),\n",
       "              ('jerk', 7.9701423545796999),\n",
       "              ('gay', 7.7514126095180682),\n",
       "              ('dipshit', 7.5735419869454219),\n",
       "              ('loser', 7.5422677671412597),\n",
       "              ('fool', 7.4023176578615058)]),\n",
       " OrderedDict([('fuc', 20.997169044043833),\n",
       "              ('sex', 15.127525005429161),\n",
       "              ('uck', 11.629192533768348),\n",
       "              (')', 11.540806161614407),\n",
       "              ('dum', 9.2061104441408776),\n",
       "              ('bag', 8.1764760331549162),\n",
       "              ('CK', 7.9953164910669674),\n",
       "              ('ion', 7.2805396778973375),\n",
       "              ('ck', 6.5951320915799423),\n",
       "              (' 2', 6.2675403729886687),\n",
       "              (' b', 6.0087615478134362),\n",
       "              ('*', 5.9535963910588832),\n",
       "              (' \"', 5.7350755783777645),\n",
       "              ('ole', 5.6840208602600812),\n",
       "              ('!', 5.5347963772673738),\n",
       "              ('ant', 5.3737292405430308),\n",
       "              ('coc', 5.1048210950097666),\n",
       "              ('ss ', 4.7734168184638035),\n",
       "              ('? ', 4.6014052090921442),\n",
       "              (':', 4.3659477357582537),\n",
       "              ('U', 4.3560343123419294),\n",
       "              ('u', 4.3152091400354733),\n",
       "              ('kk', 4.2798202091852984),\n",
       "              ('e', 4.1704201215645229),\n",
       "              ('fag', 3.8285283892798603),\n",
       "              ('u a', 3.7917456827028002),\n",
       "              ('k', 3.789765884511382),\n",
       "              ('rse', 3.7684768219058347),\n",
       "              ('poo', 3.7600981194926399),\n",
       "              ('I ', 3.7376480597182562)]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topn_features(clf, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Severe toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02819114,  0.02658199,  0.02732254])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', FeatureUnion([\n",
    "        ('words', TransformPipeline([\n",
    "            ('stemmed', FunctionTransformer(lambda X: X[:, 0], validate=False)),\n",
    "            ('vec', TfidfVectorizer()),\n",
    "        ])),\n",
    "        ('chars', TransformPipeline([\n",
    "            ('text', FunctionTransformer(lambda X: X[:, 1], validate=False)),\n",
    "            ('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(1)\n",
    "    ]))\n",
    "])\n",
    "multilabel_cross_validation(clf,\n",
    "                dftrain[['comment_text_stemmed', 'comment_text']],\n",
    "                np.array(dftrain[['severe_toxic']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('die', 4.5241908895974863),\n",
       "              ('anus', 4.3392228638903827),\n",
       "              ('filthi', 4.3387998833207755),\n",
       "              ('for', 4.2208991381500809),\n",
       "              ('you', 3.7871714817410589),\n",
       "              ('nazi', 3.7512498891152219),\n",
       "              ('asshol', 3.6754582798040727),\n",
       "              ('rape', 3.6512539462794114),\n",
       "              ('cunt', 3.4334216260871395),\n",
       "              ('pathet', 3.3461932965370935),\n",
       "              ('nigger', 3.2294079384615459),\n",
       "              ('peni', 3.219114746132604),\n",
       "              ('dumb', 3.1812718147254953),\n",
       "              ('bitch', 3.1390526249996693),\n",
       "              ('jew', 3.0912162754395496),\n",
       "              ('shit', 3.0638882873775213),\n",
       "              ('pleas', 3.0562066519661233),\n",
       "              ('pig', 2.9058545970879819),\n",
       "              ('bastard', 2.8541498100552416),\n",
       "              ('moron', 2.8114795948168707),\n",
       "              ('peopl', 2.7147424376282676),\n",
       "              ('talk', 2.670487638078356),\n",
       "              ('homosexu', 2.6694312441763466),\n",
       "              ('mexican', 2.6620346757203133),\n",
       "              ('big', 2.6274969299262101),\n",
       "              ('fuckin', 2.604032907883794),\n",
       "              ('gonna', 2.5859536176867683),\n",
       "              ('dick', 2.5536193052332354),\n",
       "              ('hanibal911you', 2.4982535555156038),\n",
       "              ('fuck', 2.4462028564903289)]),\n",
       " OrderedDict([('CK', 9.0621024950613531),\n",
       "              ('ck', 7.6520032819547543),\n",
       "              ('uck', 6.2028520499195485),\n",
       "              ('\"', 5.720846611992247),\n",
       "              ('t', 5.156383492654645),\n",
       "              ('e', 5.1078249880418936),\n",
       "              ('n', 4.0508100782775101),\n",
       "              ('kin', 3.7551758230135039),\n",
       "              ('fuk', 3.4938996221536618),\n",
       "              ('mot', 3.2332517146091755),\n",
       "              ('*', 2.9789833245047301),\n",
       "              ('F', 2.6882873546380002),\n",
       "              (' Y', 2.3609994128691794),\n",
       "              ('U', 2.3356330865230124),\n",
       "              (' fu', 2.2756419690890839),\n",
       "              ('ker', 2.1845807077176991),\n",
       "              ('.', 2.0711233813275505),\n",
       "              (' f ', 2.0679709866475662),\n",
       "              (' i ', 2.0119155332397871),\n",
       "              ('shi', 1.7961278881031664),\n",
       "              ('g', 1.7697811650263087),\n",
       "              ('fag', 1.7427592781610426),\n",
       "              ('K', 1.5095026809757399),\n",
       "              ('on', 1.3009051832099239),\n",
       "              ('!!', 1.2799807596600603),\n",
       "              ('u', 1.2149683031663929),\n",
       "              ('sh', 0.95128141193772298),\n",
       "              ('hit', 0.84723943218392572),\n",
       "              ('H', 0.75398323029893188),\n",
       "              (' g', 0.73459702051875753)]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topn_features(clf, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obscene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05844435,  0.06063655,  0.06011712])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', FeatureUnion([\n",
    "        ('words', TransformPipeline([\n",
    "            ('stemmed', FunctionTransformer(lambda X: X[:, 0], validate=False)),\n",
    "            ('vec', TfidfVectorizer()),\n",
    "        ])),\n",
    "        ('chars', TransformPipeline([\n",
    "            ('text', FunctionTransformer(lambda X: X[:, 1], validate=False)),\n",
    "            ('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(1)\n",
    "    ]))\n",
    "])\n",
    "multilabel_cross_validation(clf,\n",
    "                dftrain[['comment_text_stemmed', 'comment_text']],\n",
    "                np.array(dftrain[['obscene']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('fuck', 18.265656261246004),\n",
       "              ('bitch', 16.360600632215004),\n",
       "              ('ass', 14.854779003131794),\n",
       "              ('bullshit', 14.654044134853201),\n",
       "              ('asshol', 13.017216149297594),\n",
       "              ('pussi', 12.921839077354855),\n",
       "              ('shit', 11.541233293174219),\n",
       "              ('cunt', 11.010894692406449),\n",
       "              ('bastard', 10.731539432597934),\n",
       "              ('stupid', 9.6411276001351158),\n",
       "              ('crap', 9.356520011588275),\n",
       "              ('idiot', 9.2372485344475184),\n",
       "              ('dick', 8.5039668448287582),\n",
       "              ('peni', 8.0166724165648269),\n",
       "              ('damn', 7.8817247511098056),\n",
       "              ('cock', 7.3046687909445769),\n",
       "              ('suck', 7.2465326006490978),\n",
       "              ('faggot', 7.1794684107467779),\n",
       "              ('dumbass', 7.1552196934730672),\n",
       "              ('dickhead', 7.1131029460335808),\n",
       "              ('wtf', 7.0822914791625573),\n",
       "              ('cocksuck', 6.9768330391335924),\n",
       "              ('ck', 6.5940644004893292),\n",
       "              ('cking', 6.5650537349393927),\n",
       "              ('goddamn', 6.0094380699092937),\n",
       "              ('thank', 5.6809957026425106),\n",
       "              ('sex', 5.5585351922241504),\n",
       "              ('anal', 5.4950144742801283),\n",
       "              ('bugger', 5.4442727594678848),\n",
       "              ('rape', 5.4246799761747573)]),\n",
       " OrderedDict([('fuc', 18.481430263596003),\n",
       "              ('uck', 17.192748129315152),\n",
       "              ('CK', 9.8995180870974444),\n",
       "              ('hit', 9.1731006493957796),\n",
       "              ('ck', 7.6485019835636887),\n",
       "              ('*', 7.1492299483040789),\n",
       "              ('FUC', 6.5698660198560406),\n",
       "              ('kk', 6.2248123427507984),\n",
       "              (')', 5.683662043936704),\n",
       "              ('shi', 5.354655208143865),\n",
       "              ('k', 5.0987976214485204),\n",
       "              ('fuk', 4.9937480061576105),\n",
       "              ('ole', 4.9773461742770575),\n",
       "              ('a ', 4.6416461579244368),\n",
       "              ('ass', 4.3875059519220398),\n",
       "              ('ick', 4.2944528585704553),\n",
       "              ('F', 4.2518873625128517),\n",
       "              ('cun', 4.138691397594255),\n",
       "              ('U', 4.1218343330890912),\n",
       "              (' u ', 3.948271639565311),\n",
       "              (' Y', 3.7953194887280062),\n",
       "              ('e', 3.7939194827816212),\n",
       "              ('g', 3.7552289778153023),\n",
       "              ('!', 3.648594095208153),\n",
       "              ('Fuc', 3.6159436167474253),\n",
       "              ('G', 3.0511585668691281),\n",
       "              (' je', 3.0150695110649592),\n",
       "              ('fag', 2.9021985816711533),\n",
       "              ('n', 2.6471333794553704),\n",
       "              ('u', 2.2919217570742889)]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topn_features(clf, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0122937 ,  0.01140329,  0.01157872])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', FeatureUnion([\n",
    "        ('words', TransformPipeline([\n",
    "            ('stemmed', FunctionTransformer(lambda X: X[:, 0], validate=False)),\n",
    "            ('vec', TfidfVectorizer()),\n",
    "        ])),\n",
    "        ('chars', TransformPipeline([\n",
    "            ('text', FunctionTransformer(lambda X: X[:, 1], validate=False)),\n",
    "            ('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(1)\n",
    "    ]))\n",
    "])\n",
    "multilabel_cross_validation(clf,\n",
    "                dftrain[['comment_text_stemmed', 'comment_text']],\n",
    "                np.array(dftrain[['threat']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('die', 10.648476461392372),\n",
       "              ('kill', 10.346868433147195),\n",
       "              ('shoot', 10.092111092819124),\n",
       "              ('will', 7.1737946794102445),\n",
       "              ('rape', 6.6868067914182738),\n",
       "              ('death', 6.6192326359451199),\n",
       "              ('cut', 5.7428963524895114),\n",
       "              ('burn', 5.2066808509506979),\n",
       "              ('you', 4.8308704557227102),\n",
       "              ('stab', 4.5497406099194473),\n",
       "              ('gonna', 4.5306958169627842),\n",
       "              ('your', 4.5148335351371927),\n",
       "              ('ll', 4.4859291891961535),\n",
       "              ('deserv', 4.4600400036417165),\n",
       "              ('ass', 4.4152586350877732),\n",
       "              ('ya', 4.2623563648853375),\n",
       "              ('punch', 4.0960584178586581),\n",
       "              ('dead', 3.9628381602954295),\n",
       "              ('kick', 3.5617121602747956),\n",
       "              ('the', 3.505404550580459),\n",
       "              ('thank', 3.2945355269076653),\n",
       "              ('hous', 3.0875479127600878),\n",
       "              ('it', 3.0447626890167898),\n",
       "              ('fire', 3.0290671412472614),\n",
       "              ('hunt', 3.006846683634802),\n",
       "              ('that', 2.7602485239935559),\n",
       "              ('go', 2.4568925886094046),\n",
       "              ('down', 2.4284626918981833),\n",
       "              ('fuck', 2.4172916252206047),\n",
       "              ('and', 2.361812394555542)]),\n",
       " OrderedDict([('e', 5.3220696761618189),\n",
       "              ('I', 2.6396107213145026),\n",
       "              ('u', 2.2429639372193781),\n",
       "              ('s', 2.2372041717503786),\n",
       "              ('m g', 2.2261892685671567),\n",
       "              ('!!', 2.1185592785716425),\n",
       "              ('\"', 1.7386058316658908),\n",
       "              ('fuc', 1.3581591982525867),\n",
       "              (' ', 1.2779062961976759),\n",
       "              ('t', 1.2227853017307921),\n",
       "              ('a', 0.21710752705874589),\n",
       "              ('DO,', 0.0),\n",
       "              (\"DL'\", 0.0),\n",
       "              ('DO?', 0.0),\n",
       "              ('DK.', 0.0),\n",
       "              ('DKA', 0.0),\n",
       "              ('DKP', 0.0),\n",
       "              ('DKi', 0.0),\n",
       "              ('DO/', 0.0),\n",
       "              ('DKp', 0.0),\n",
       "              ('DKx', 0.0),\n",
       "              ('DL', 0.0),\n",
       "              ('DL ', 0.0),\n",
       "              ('DL\"', 0.0),\n",
       "              ('DL(', 0.0),\n",
       "              ('DK)', 0.0),\n",
       "              ('DL)', 0.0),\n",
       "              ('DL*', 0.0),\n",
       "              ('DL,', 0.0),\n",
       "              ('DL-', 0.0)]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topn_features(clf, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07649189,  0.07662821,  0.07866182])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', FeatureUnion([\n",
    "        ('words', TransformPipeline([\n",
    "            ('stemmed', FunctionTransformer(lambda X: X[:, 0], validate=False)),\n",
    "            ('vec', TfidfVectorizer()),\n",
    "        ])),\n",
    "        ('chars', TransformPipeline([\n",
    "            ('text', FunctionTransformer(lambda X: X[:, 1], validate=False)),\n",
    "            ('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(1)\n",
    "    ]))\n",
    "])\n",
    "multilabel_cross_validation(clf,\n",
    "                dftrain[['comment_text_stemmed', 'comment_text']],\n",
    "                np.array(dftrain[['insult']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('idiot', 14.616490335233244),\n",
       "              ('stupid', 10.457621118352366),\n",
       "              ('bitch', 10.072553193582161),\n",
       "              ('asshol', 9.3076422786496469),\n",
       "              ('bastard', 9.2867052743397807),\n",
       "              ('moron', 9.1222214723918338),\n",
       "              ('loser', 7.9753045257333133),\n",
       "              ('faggot', 7.8295440840863764),\n",
       "              ('cunt', 7.5671112350704579),\n",
       "              ('ass', 6.7679912424449986),\n",
       "              ('fool', 6.7654298821606558),\n",
       "              ('dickhead', 6.6830191102982415),\n",
       "              ('pathet', 6.6703511825327464),\n",
       "              ('retard', 6.6669358744124816),\n",
       "              ('pig', 6.6309203480658585),\n",
       "              ('goddamn', 5.4861742467558434),\n",
       "              ('jerk', 5.2029577153186706),\n",
       "              ('nigger', 5.1395652810676067),\n",
       "              ('whore', 5.1287129525926769),\n",
       "              ('fat', 5.0401393490227546),\n",
       "              ('thank', 5.0328154603322188),\n",
       "              ('scum', 4.9166954302241264),\n",
       "              ('gay', 4.8986794095660748),\n",
       "              ('filthi', 4.8700338293422183),\n",
       "              ('nigga', 4.8200772871603155),\n",
       "              ('talk', 4.7037805551297245),\n",
       "              ('coward', 4.6621546069373236),\n",
       "              ('dumb', 4.5733220401156034),\n",
       "              ('jackass', 4.5728918349717826),\n",
       "              ('ugli', 4.4023118051397478)]),\n",
       " OrderedDict([('uck', 11.560266609509839),\n",
       "              (')', 9.5769981826127246),\n",
       "              ('dum', 8.0064795845581571),\n",
       "              ('ion', 6.8556031130739417),\n",
       "              ('hit', 6.5483473831748471),\n",
       "              ('CK', 5.2906096431151708),\n",
       "              ('U', 5.2790422065959577),\n",
       "              ('a ', 5.2672333774711522),\n",
       "              (':', 5.2284507629782695),\n",
       "              ('bag', 4.8566149007862958),\n",
       "              ('fa', 4.5297780015893787),\n",
       "              ('You', 4.2979862056778497),\n",
       "              ('G', 4.2077728509426384),\n",
       "              (' je', 4.1388217424367806),\n",
       "              ('u ', 4.1355721444086946),\n",
       "              ('ker', 4.1194505041595297),\n",
       "              ('F', 4.1133718840489593),\n",
       "              ('kk', 4.0765341790109613),\n",
       "              ('t', 4.072269762900107),\n",
       "              ('I', 3.9344062673530629),\n",
       "              ('.1', 3.9311640890813973),\n",
       "              ('*', 3.867596129891715),\n",
       "              (' a ', 3.8451990953063611),\n",
       "              ('Go ', 3.8268146086968038),\n",
       "              ('ick', 3.6907515051887314),\n",
       "              ('k', 3.6903902604407062),\n",
       "              ('nc', 3.6184336078045716),\n",
       "              ('u', 3.5502234605987812),\n",
       "              (' u ', 3.5255206988904453),\n",
       "              (', y', 3.4736221318559384)]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topn_features(clf, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02691533,  0.02579004,  0.02610342])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', FeatureUnion([\n",
    "        ('words', TransformPipeline([\n",
    "            ('stemmed', FunctionTransformer(lambda X: X[:, 0], validate=False)),\n",
    "            ('vec', TfidfVectorizer()),\n",
    "        ])),\n",
    "        ('chars', TransformPipeline([\n",
    "            ('text', FunctionTransformer(lambda X: X[:, 1], validate=False)),\n",
    "            ('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(1)\n",
    "    ]))\n",
    "])\n",
    "multilabel_cross_validation(clf,\n",
    "                dftrain[['comment_text_stemmed', 'comment_text']],\n",
    "                np.array(dftrain[['identity_hate']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('nigger', 12.759037521275481),\n",
       "              ('nigga', 11.147609260536905),\n",
       "              ('homosexu', 9.6700961660905929),\n",
       "              ('gay', 9.4781637261634142),\n",
       "              ('jew', 7.4979543549309771),\n",
       "              ('homo', 6.8577903362231885),\n",
       "              ('black', 6.5458585084710936),\n",
       "              ('muslim', 6.3850104875205567),\n",
       "              ('nazi', 6.2518252347333476),\n",
       "              ('talk', 6.2302387818109324),\n",
       "              ('faggot', 6.0820215947901248),\n",
       "              ('negro', 5.376064331910646),\n",
       "              ('american', 4.910696050893554),\n",
       "              ('fagot', 4.4042940246812901),\n",
       "              ('racist', 4.2587095248069184),\n",
       "              ('turk', 4.0208334741833331),\n",
       "              ('mexican', 3.9881974726680509),\n",
       "              ('asian', 3.8862894599380109),\n",
       "              ('paki', 3.8678541453647681),\n",
       "              ('semit', 3.7136666716299742),\n",
       "              ('fucker', 3.6797347055951355),\n",
       "              ('thank', 3.6281273742440261),\n",
       "              ('like', 3.5420425433354672),\n",
       "              ('stupid', 3.4305694321334546),\n",
       "              ('not', 3.4252047758702933),\n",
       "              ('boy', 3.363647403657696),\n",
       "              ('it', 3.299758096819347),\n",
       "              ('there', 3.2764605790780337),\n",
       "              ('hate', 3.2747236951938596),\n",
       "              ('fag', 3.0050195314619348)]),\n",
       " OrderedDict([('ian', 7.1604981858388861),\n",
       "              ('gg', 6.1358910161251199),\n",
       "              ('\"', 6.0151056506535312),\n",
       "              ('ion', 5.3985150101297545),\n",
       "              ('e', 5.3691340409811481),\n",
       "              ('h', 4.5407344334535313),\n",
       "              (' ', 3.5973506146358698),\n",
       "              ('O', 3.5085741232048422),\n",
       "              ('I ', 3.3342657894430303),\n",
       "              ('u', 3.2673983211066853),\n",
       "              ('0', 3.1283928618917689),\n",
       "              ('on', 3.0506450464485528),\n",
       "              ('H', 2.978369232903455),\n",
       "              ('t', 2.8660475717555203),\n",
       "              ('mal', 2.4946916786611122),\n",
       "              ('IS', 2.1992902077107264),\n",
       "              ('..', 2.040452924520801),\n",
       "              (' g', 1.873331290481109),\n",
       "              ('sh', 1.6801129024089392),\n",
       "              ('!', 1.4471740221787663),\n",
       "              (' je', 1.3363610149754723),\n",
       "              ('s a', 1.2457380016236972),\n",
       "              ('uck', 1.2380228298041029),\n",
       "              ('A', 0.92695837244325618),\n",
       "              ('!!', 0.74173620105664517),\n",
       "              ('.1', 0.72659223108274462),\n",
       "              ('E', 0.64850761406041968),\n",
       "              ('o', 0.53968378164629738),\n",
       "              ('fuc', 0.32211955791724634),\n",
       "              ('a', 0.26872750695720993)]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topn_features(clf, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import corpus\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'b', 'c', 'd'}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(\"abdc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = set(corpus.stopwords.words('english')) | set(\"[]`()[]{},<>/\\?\")\n",
    "\n",
    "def clean_text(text):\n",
    "    tokens = wordpunct_tokenize(text)\n",
    "    filtered = filter(lambda token: token.lower() not in stopwords, tokens)\n",
    "    return \" \".join(filtered)\n",
    "\n",
    "def clean_texts(texts):\n",
    "    return [clean_text(text) for text in tqdm(texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
