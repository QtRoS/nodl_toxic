{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from crossvalidation import multilabel_cross_validation, multilabel_label_combinations\n",
    "from multilabel_classifier import MultilabelClassifier\n",
    "from transform_pipeline import TransformPipeline\n",
    "\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.stem.snowball import EnglishStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from functools import lru_cache\n",
    "\n",
    "from textblob import TextBlob\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from sklearn.dummy import DummyRegressor\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer, MaxAbsScaler\n",
    "\n",
    "from visualizations import topn_features, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset preparation\n",
    "\n",
    "I'll use features based on:\n",
    "\n",
    "- comment text \"as is\"\n",
    "- stemmed/lemmatized comment text\n",
    "- (maybe) sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftrain = pd.read_csv(\"input/train.csv\")\n",
    "dftrain['comment_text'] = dftrain['comment_text'].apply(str) # some values parsed as float\n",
    "dftest = pd.read_csv(\"input/test.csv\")\n",
    "dftest['comment_text'] = dftest['comment_text'].apply(str) # some values parsed as float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  toxic  \\\n",
       "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
       "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
       "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
       "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4  79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6044863</td>\n",
       "      <td>==Orphaned non-free media (Image:41cD1jboEvL. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6102620</td>\n",
       "      <td>::Kentuckiana is colloquial.  Even though the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14563293</td>\n",
       "      <td>Hello fellow Wikipedians,\\nI have just modifie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21086297</td>\n",
       "      <td>AKC Suspensions \\nThe Morning Call - Feb 24, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22982444</td>\n",
       "      <td>== [WIKI_LINK: Talk:Celts] ==</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text\n",
       "0   6044863  ==Orphaned non-free media (Image:41cD1jboEvL. ...\n",
       "1   6102620  ::Kentuckiana is colloquial.  Even though the ...\n",
       "2  14563293  Hello fellow Wikipedians,\\nI have just modifie...\n",
       "3  21086297  AKC Suspensions \\nThe Morning Call - Feb 24, 2...\n",
       "4  22982444                      == [WIKI_LINK: Talk:Celts] =="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = EnglishStemmer()\n",
    "\n",
    "@lru_cache(30000)\n",
    "def stem_word(word):\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "@lru_cache(30000)\n",
    "def lemmatize_word(word):\n",
    "    return lemmatizer.lemmatize(word)\n",
    "\n",
    "def simplify_text(text, simplifier):\n",
    "    tokens = wordpunct_tokenize(text.lower())\n",
    "    simplified_tokens = map(simplifier, tokens)\n",
    "    return \" \".join(simplified_tokens)\n",
    "\n",
    "def simplify_texts(texts, simplifier):\n",
    "    return [simplify_text(text, simplifier) for text in tqdm(texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 95851/95851 [00:24<00:00, 3969.80it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 226998/226998 [00:48<00:00, 4725.14it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████| 95851/95851 [00:15<00:00, 6037.28it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████| 226998/226998 [00:35<00:00, 6364.01it/s]\n"
     ]
    }
   ],
   "source": [
    "dftrain['comment_text_stemmed'] = simplify_texts(dftrain['comment_text'], stem_word)\n",
    "dftest['comment_text_stemmed'] = simplify_texts(dftest['comment_text'], stem_word)\n",
    "dftrain['comment_text_lemmatized'] = simplify_texts(dftrain['comment_text'], lemmatize_word)\n",
    "dftest['comment_text_lemmatized'] = simplify_texts(dftest['comment_text'], lemmatize_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentiment(df):\n",
    "    blobs = [TextBlob(text) for text in tqdm(df['comment_text'])]\n",
    "    sentiments = [blob.sentiment for blob in tqdm(blobs)]\n",
    "    polatity = np.array([sentiment.polarity for sentiment in sentiments])\n",
    "    subjectivity = np.array([sentiment.subjectivity for sentiment in sentiments])\n",
    "    sentiment = polatity * subjectivity\n",
    "    return polatity, subjectivity, sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 95851/95851 [00:02<00:00, 39456.15it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 95851/95851 [02:41<00:00, 595.33it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████| 226998/226998 [00:07<00:00, 29511.06it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████| 226998/226998 [06:55<00:00, 546.59it/s]\n"
     ]
    }
   ],
   "source": [
    "dftrain['polarity'], dftrain['subjectivity'], dftrain['sentiment'] = sentiment(dftrain)\n",
    "dftest['polarity'], dftest['subjectivity'], dftest['sentiment'] = sentiment(dftest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>comment_text_stemmed</th>\n",
       "      <th>comment_text_lemmatized</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nonsens ? kiss off , geek . what i said is tru...</td>\n",
       "      <td>nonsense ? kiss off , geek . what i said is tr...</td>\n",
       "      <td>0.350000</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.227500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" pleas do not vandal page , as you did with t...</td>\n",
       "      <td>\" please do not vandalize page , a you did wit...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" \"\" point of interest \"\" i remov the \"\" point...</td>\n",
       "      <td>\" \"\" point of interest \"\" i removed the \"\" poi...</td>\n",
       "      <td>-0.040625</td>\n",
       "      <td>0.771875</td>\n",
       "      <td>-0.031357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ask some his nation is a racial offenc . wow w...</td>\n",
       "      <td>asking some his nationality is a racial offenc...</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>0.109375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>the reader here is not go by my say so for eth...</td>\n",
       "      <td>the reader here is not going by my say so for ...</td>\n",
       "      <td>-0.075000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>-0.018750</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  toxic  \\\n",
       "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
       "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
       "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
       "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4  79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                comment_text_stemmed  \\\n",
       "0  nonsens ? kiss off , geek . what i said is tru...   \n",
       "1  \" pleas do not vandal page , as you did with t...   \n",
       "2  \" \"\" point of interest \"\" i remov the \"\" point...   \n",
       "3  ask some his nation is a racial offenc . wow w...   \n",
       "4  the reader here is not go by my say so for eth...   \n",
       "\n",
       "                             comment_text_lemmatized  polarity  subjectivity  \\\n",
       "0  nonsense ? kiss off , geek . what i said is tr...  0.350000      0.650000   \n",
       "1  \" please do not vandalize page , a you did wit...  0.000000      0.000000   \n",
       "2  \" \"\" point of interest \"\" i removed the \"\" poi... -0.040625      0.771875   \n",
       "3  asking some his nationality is a racial offenc...  0.175000      0.625000   \n",
       "4  the reader here is not going by my say so for ... -0.075000      0.250000   \n",
       "\n",
       "   sentiment  \n",
       "0   0.227500  \n",
       "1   0.000000  \n",
       "2  -0.031357  \n",
       "3   0.109375  \n",
       "4  -0.018750  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dftrain = pd.read_csv(\"input/train_adv.csv\")\n",
    "# dftrain['comment_text'] = dftrain['comment_text'].apply(str) # some values parsed as float\n",
    "dftest = pd.read_csv(\"input/test_adv.csv\")\n",
    "# dftest['comment_text'] = dftest['comment_text'].apply(str) # some values parsed as float"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dummy model\n",
    "\n",
    "Let's build model that return 0.5 for each label probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.69314718, 0.69314718, 0.69314718])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = MultilabelClassifier([\n",
    "    DummyRegressor(strategy='constant', constant=0.5)\n",
    "    for _ in range(6)\n",
    "])\n",
    "multilabel_cross_validation(clf,\n",
    "                            np.zeros([len(dftrain), 1]),\n",
    "                            np.array(dftrain[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0556387 , 0.05629894, 0.05564404])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', TfidfVectorizer()),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(6)\n",
    "    ]))\n",
    "])\n",
    "multilabel_cross_validation(clf,\n",
    "                            dftrain['comment_text'],\n",
    "                            np.array(dftrain[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05290194, 0.05321799, 0.05259117])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', TfidfVectorizer(max_features=10000, binary=True, norm='l2', smooth_idf=True, sublinear_tf=True, use_idf=True)),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(6)\n",
    "    ]))\n",
    "])\n",
    "multilabel_cross_validation(clf,\n",
    "                dftrain['comment_text_stemmed'],\n",
    "                np.array(dftrain[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05456107, 0.05511099, 0.05463727])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', TfidfVectorizer()),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(6)\n",
    "    ]))\n",
    "])\n",
    "multilabel_cross_validation(clf,\n",
    "                dftrain['comment_text_lemmatized'],\n",
    "                np.array(dftrain[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for bare logistic regression - stemmed text is a best choise.\n",
    "\n",
    "# Character features\n",
    "\n",
    "Let's add character n-gram based features (from source text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04972983, 0.04990565, 0.04951608])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', FeatureUnion([\n",
    "        ('words', TransformPipeline([\n",
    "            ('stemmed', FunctionTransformer(lambda X: X[:, 0], validate=False)),\n",
    "            ('vec', TfidfVectorizer(max_features=6500, binary=True, norm='l2', smooth_idf=True, sublinear_tf=True, use_idf=True)),\n",
    "        ])),\n",
    "        ('chars', TransformPipeline([\n",
    "            ('text', FunctionTransformer(lambda X: X[:, 1], validate=False)),\n",
    "            ('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(6)\n",
    "    ]))\n",
    "])\n",
    "multilabel_cross_validation(clf,\n",
    "                dftrain[['comment_text_stemmed', 'comment_text']],\n",
    "                np.array(dftrain[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "6500  array([0.04972983, 0.04990565, 0.04951608])\n",
    "7500  array([0.04974486, 0.0499596 , 0.04946345])\n",
    "8000  array([0.04983992, 0.049903  , 0.04944069])\n",
    "10000 array([0.04984456, 0.04995329, 0.04949518])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's show top 30 features - from words list and from character n-grams list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('fuck', 22.258629928450581),\n",
       "              ('idiot', 19.496728472845685),\n",
       "              ('shit', 16.627022092542468),\n",
       "              ('stupid', 16.321173442144822),\n",
       "              ('bullshit', 16.234971783888582),\n",
       "              ('bitch', 15.685465620721198),\n",
       "              ('ass', 14.566450399722072),\n",
       "              ('asshol', 13.127424796729347),\n",
       "              ('pussi', 13.094236593684162),\n",
       "              ('nigger', 12.802131977647788),\n",
       "              ('crap', 11.850163237299537),\n",
       "              ('peni', 11.476548473431224),\n",
       "              ('nigga', 11.191932448267101),\n",
       "              ('suck', 11.005929809981092),\n",
       "              ('kill', 10.942967855170675),\n",
       "              ('bastard', 10.939489330136974),\n",
       "              ('cunt', 10.911695491594049),\n",
       "              ('pathet', 10.696962979232737),\n",
       "              ('dick', 10.672028146667035),\n",
       "              ('die', 10.507648532169508),\n",
       "              ('moron', 10.295281680816011),\n",
       "              ('faggot', 10.242676674637879),\n",
       "              ('homosexu', 9.9315303609515855),\n",
       "              ('shoot', 9.8980432871107773),\n",
       "              ('gay', 9.530545342654678),\n",
       "              ('hell', 9.0501358700410304),\n",
       "              ('whore', 8.7292696093662538),\n",
       "              ('nazi', 8.3015865058452718),\n",
       "              ('ck', 8.0708981969341167),\n",
       "              ('jerk', 8.0259396951056932)]),\n",
       " OrderedDict([('fuc', 20.567255303902002),\n",
       "              ('uck', 17.376688460860965),\n",
       "              ('sex', 13.918081288301389),\n",
       "              ('hit', 10.205754927834816),\n",
       "              ('dum', 9.5826201431131111),\n",
       "              ('CK', 9.4871537421232617),\n",
       "              ('bag', 8.5755994754237932),\n",
       "              ('ck', 8.0186950052464745),\n",
       "              ('*', 7.2486652510711727),\n",
       "              ('ian', 6.4076422344069748),\n",
       "              ('kk', 6.1303386040680783),\n",
       "              ('ole', 6.0053755502889059),\n",
       "              ('coc', 5.9508026024594898),\n",
       "              ('gg', 5.9259127348689402),\n",
       "              ('U', 5.5911068586227808),\n",
       "              ('!', 5.5729722816206584),\n",
       "              (' b', 5.5643354245310288),\n",
       "              ('shi', 5.5181799583787976),\n",
       "              ('k', 5.3705237567661808),\n",
       "              ('FUC', 5.1949801804846478),\n",
       "              ('fuk', 5.1140050138937889),\n",
       "              ('a ', 5.0077183810632961),\n",
       "              ('ass', 4.9481753957269863),\n",
       "              ('ant', 4.8634476035542882),\n",
       "              ('F', 4.8364132982881207),\n",
       "              ('G', 4.7516683903208419),\n",
       "              ('ss ', 4.749808703251178),\n",
       "              ('h', 4.6194007429545287),\n",
       "              (' je', 4.6040052584559348),\n",
       "              ('ick', 4.5559659192718982)]))"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topn_features(clf, 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Per-class error/features visualization\n",
    "\n",
    "There I'll build for each class:\n",
    "- cross-validation scores per class\n",
    "- list of 30 top word/character features\n",
    "- confusion matrix with:\n",
    "    - false positive rate (part of negative samples predicted as positive)\n",
    "    - false negative rate (part of positive samples predicted as negative)\n",
    "    - true positive rate (part of positive samples predicted as positive)\n",
    "    - true negative rate (part of negative samples predicted as negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.105112  ,  0.10259742,  0.1023982 ])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', FeatureUnion([\n",
    "        ('words', TransformPipeline([\n",
    "            ('stemmed', FunctionTransformer(lambda X: X[:, 0], validate=False)),\n",
    "            ('vec', TfidfVectorizer()),\n",
    "        ])),\n",
    "        ('chars', TransformPipeline([\n",
    "            ('text', FunctionTransformer(lambda X: X[:, 1], validate=False)),\n",
    "            ('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(1)\n",
    "    ]))\n",
    "])\n",
    "multilabel_cross_validation(clf,\n",
    "                dftrain[['comment_text_stemmed', 'comment_text']],\n",
    "                np.array(dftrain[['toxic']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('fuck', 21.556119614103412),\n",
       "              ('idiot', 19.317049558444587),\n",
       "              ('shit', 17.110099475784853),\n",
       "              ('stupid', 16.36257824598469),\n",
       "              ('bullshit', 16.26183782367546),\n",
       "              ('ass', 13.507419437664371),\n",
       "              ('asshol', 13.16003029944566),\n",
       "              ('crap', 11.628581292841442),\n",
       "              ('peni', 11.516648658328211),\n",
       "              ('bitch', 11.013434505924108),\n",
       "              ('suck', 11.009842443430925),\n",
       "              ('bastard', 10.843526134232516),\n",
       "              ('dick', 10.787729163039733),\n",
       "              ('pathet', 10.589895397067135),\n",
       "              ('moron', 10.436272972665206),\n",
       "              ('faggot', 9.9766246865283144),\n",
       "              ('hell', 9.0134897970140937),\n",
       "              ('whore', 8.9041253750908975),\n",
       "              ('thank', 8.6488614249627904),\n",
       "              ('nigger', 8.5229018974044202),\n",
       "              ('cunt', 8.4803818059381708),\n",
       "              ('nazi', 8.3083609068579403),\n",
       "              ('pussi', 8.2487500159690175),\n",
       "              ('ck', 8.1912591893195881),\n",
       "              ('retard', 8.0171789272726883),\n",
       "              ('jerk', 7.9697376018350141),\n",
       "              ('gay', 7.7528157787758394),\n",
       "              ('dipshit', 7.5750006333085809),\n",
       "              ('loser', 7.5427138007552257),\n",
       "              ('fool', 7.4033285323547444)]),\n",
       " OrderedDict([('fuc', 20.994757270942724),\n",
       "              ('sex', 15.129104492981325),\n",
       "              ('uck', 11.628605363804324),\n",
       "              (')', 11.531544872558728),\n",
       "              ('dum', 9.2051257884049953),\n",
       "              ('bag', 8.1787065010701099),\n",
       "              ('CK', 7.9820013879646092),\n",
       "              ('ion', 7.2694733704193668),\n",
       "              ('ck', 6.5965073054362913),\n",
       "              (' 2', 6.2578459981273911),\n",
       "              (' b', 6.0024626740107268),\n",
       "              ('*', 5.9507915549440655),\n",
       "              (' \"', 5.7193476015114602),\n",
       "              ('ole', 5.6838175316312798),\n",
       "              ('!', 5.5341073585077307),\n",
       "              ('ant', 5.3762703685125084),\n",
       "              ('coc', 5.1108843687793648),\n",
       "              ('ss ', 4.7813879510221806),\n",
       "              ('? ', 4.6095651948195471),\n",
       "              ('U', 4.3614990547811452),\n",
       "              (':', 4.3608598885203254),\n",
       "              ('u', 4.3164029487620326),\n",
       "              ('kk', 4.2821561133148744),\n",
       "              ('e', 4.1575249815811945),\n",
       "              ('fag', 3.8190174289560392),\n",
       "              ('u a', 3.7927672209257759),\n",
       "              ('k', 3.7911979973503946),\n",
       "              ('rse', 3.7664776550686474),\n",
       "              ('poo', 3.7513052112537952),\n",
       "              ('I ', 3.7226797321902096)]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topn_features(clf, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted negative</th>\n",
       "      <th>predicted positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.990348</td>\n",
       "      <td>0.009652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.314855</td>\n",
       "      <td>0.685145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          predicted negative  predicted positive\n",
       "negative            0.990348            0.009652\n",
       "positive            0.314855            0.685145"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', FeatureUnion([\n",
    "        ('words', TransformPipeline([\n",
    "            ('stemmed', FunctionTransformer(lambda X: X[:, 0], validate=False)),\n",
    "            ('vec', TfidfVectorizer()),\n",
    "        ])),\n",
    "        ('chars', TransformPipeline([\n",
    "            ('text', FunctionTransformer(lambda X: X[:, 1], validate=False)),\n",
    "            ('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(1)\n",
    "    ]))\n",
    "])\n",
    "confusion_matrix(clf,\n",
    "                dftrain[['comment_text_stemmed', 'comment_text']],\n",
    "                np.array(dftrain[['toxic']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Severe toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.0281903 ,  0.02658158,  0.0273223 ])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', FeatureUnion([\n",
    "        ('words', TransformPipeline([\n",
    "            ('stemmed', FunctionTransformer(lambda X: X[:, 0], validate=False)),\n",
    "            ('vec', TfidfVectorizer()),\n",
    "        ])),\n",
    "        ('chars', TransformPipeline([\n",
    "            ('text', FunctionTransformer(lambda X: X[:, 1], validate=False)),\n",
    "            ('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(1)\n",
    "    ]))\n",
    "])\n",
    "multilabel_cross_validation(clf,\n",
    "                dftrain[['comment_text_stemmed', 'comment_text']],\n",
    "                np.array(dftrain[['severe_toxic']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('die', 4.5263704831269633),\n",
       "              ('filthi', 4.3384082283558136),\n",
       "              ('anus', 4.3372787615404009),\n",
       "              ('for', 4.2245073938649496),\n",
       "              ('you', 3.7642516916357232),\n",
       "              ('nazi', 3.7507138357978307),\n",
       "              ('asshol', 3.673707292716796),\n",
       "              ('rape', 3.6500550640802643),\n",
       "              ('cunt', 3.4341921683995738),\n",
       "              ('pathet', 3.3443010832459823),\n",
       "              ('nigger', 3.2309395092759465),\n",
       "              ('peni', 3.2199889358307567),\n",
       "              ('dumb', 3.1817894396460162),\n",
       "              ('bitch', 3.1378055264838149),\n",
       "              ('jew', 3.0932868150425117),\n",
       "              ('shit', 3.0630431375692786),\n",
       "              ('pleas', 3.055809159036424),\n",
       "              ('pig', 2.9056214795834578),\n",
       "              ('bastard', 2.8533346999478373),\n",
       "              ('moron', 2.8125867279913326),\n",
       "              ('peopl', 2.7133439357101903),\n",
       "              ('homosexu', 2.6730763968730566),\n",
       "              ('talk', 2.6720183680210279),\n",
       "              ('mexican', 2.6613294360823043),\n",
       "              ('big', 2.6293949480915044),\n",
       "              ('fuckin', 2.6018790485503862),\n",
       "              ('gonna', 2.5859217426468297),\n",
       "              ('dick', 2.5566202772460098),\n",
       "              ('hanibal911you', 2.500823930041745),\n",
       "              ('fuck', 2.4475799607759194)]),\n",
       " OrderedDict([('CK', 9.0394561675021414),\n",
       "              ('ck', 7.6363651354393154),\n",
       "              ('uck', 6.2215971610311049),\n",
       "              ('\"', 5.7159748769113241),\n",
       "              ('t', 5.1443830819612577),\n",
       "              ('e', 5.1027922147610507),\n",
       "              ('n', 4.0452501115545214),\n",
       "              ('kin', 3.753311882034994),\n",
       "              ('fuk', 3.4975809285703221),\n",
       "              ('mot', 3.2343054288088209),\n",
       "              ('*', 2.9832343238652754),\n",
       "              ('F', 2.6806162139637895),\n",
       "              (' Y', 2.4070952365937099),\n",
       "              ('U', 2.3580741891220129),\n",
       "              (' fu', 2.2748946820453404),\n",
       "              ('ker', 2.1784513311245273),\n",
       "              ('.', 2.0882830750871748),\n",
       "              (' f ', 2.07180543165933),\n",
       "              (' i ', 2.0063703866304019),\n",
       "              ('g', 1.7544361650666149),\n",
       "              ('shi', 1.7501867021473478),\n",
       "              ('fag', 1.743025500783651),\n",
       "              ('K', 1.5318886859977638),\n",
       "              ('on', 1.3011497362316884),\n",
       "              ('!!', 1.2711156780742752),\n",
       "              ('u', 1.1938166580004246),\n",
       "              ('sh', 0.97403374746954607),\n",
       "              ('hit', 0.88025325822609291),\n",
       "              ('H', 0.75139864745014073),\n",
       "              (' g', 0.74084320419742078)]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topn_features(clf, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted negative</th>\n",
       "      <th>predicted positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.996965</td>\n",
       "      <td>0.003035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.701245</td>\n",
       "      <td>0.298755</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          predicted negative  predicted positive\n",
       "negative            0.996965            0.003035\n",
       "positive            0.701245            0.298755"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', FeatureUnion([\n",
    "        ('words', TransformPipeline([\n",
    "            ('stemmed', FunctionTransformer(lambda X: X[:, 0], validate=False)),\n",
    "            ('vec', TfidfVectorizer()),\n",
    "        ])),\n",
    "        ('chars', TransformPipeline([\n",
    "            ('text', FunctionTransformer(lambda X: X[:, 1], validate=False)),\n",
    "            ('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(1)\n",
    "    ]))\n",
    "])\n",
    "confusion_matrix(clf,\n",
    "                dftrain[['comment_text_stemmed', 'comment_text']],\n",
    "                np.array(dftrain[['severe_toxic']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obscene"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.05844263,  0.06063497,  0.06012041])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', FeatureUnion([\n",
    "        ('words', TransformPipeline([\n",
    "            ('stemmed', FunctionTransformer(lambda X: X[:, 0], validate=False)),\n",
    "            ('vec', TfidfVectorizer()),\n",
    "        ])),\n",
    "        ('chars', TransformPipeline([\n",
    "            ('text', FunctionTransformer(lambda X: X[:, 1], validate=False)),\n",
    "            ('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(1)\n",
    "    ]))\n",
    "])\n",
    "multilabel_cross_validation(clf,\n",
    "                dftrain[['comment_text_stemmed', 'comment_text']],\n",
    "                np.array(dftrain[['obscene']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('fuck', 18.266791247746649),\n",
       "              ('bitch', 16.362478869621985),\n",
       "              ('ass', 14.853128494532696),\n",
       "              ('bullshit', 14.653320957807894),\n",
       "              ('asshol', 13.018990465908336),\n",
       "              ('pussi', 12.920922736131878),\n",
       "              ('shit', 11.540668736325051),\n",
       "              ('cunt', 11.00799444571949),\n",
       "              ('bastard', 10.735868196959578),\n",
       "              ('stupid', 9.6381743695724147),\n",
       "              ('crap', 9.3555266637787309),\n",
       "              ('idiot', 9.2377273259359036),\n",
       "              ('dick', 8.5019932226675792),\n",
       "              ('peni', 8.0180774082955129),\n",
       "              ('damn', 7.8829963802860625),\n",
       "              ('cock', 7.3013123759169609),\n",
       "              ('suck', 7.2489875723766888),\n",
       "              ('faggot', 7.1858384462856506),\n",
       "              ('dumbass', 7.1558718104891899),\n",
       "              ('dickhead', 7.1168858959576218),\n",
       "              ('wtf', 7.0828228964832061),\n",
       "              ('cocksuck', 6.9772820597801841),\n",
       "              ('ck', 6.5931036053357541),\n",
       "              ('cking', 6.566582742081235),\n",
       "              ('goddamn', 6.0105154225083464),\n",
       "              ('thank', 5.6790555212474558),\n",
       "              ('sex', 5.5547684863574682),\n",
       "              ('anal', 5.4943415487801452),\n",
       "              ('bugger', 5.4416109969177304),\n",
       "              ('rape', 5.4267079887930594)]),\n",
       " OrderedDict([('fuc', 18.490661678841356),\n",
       "              ('uck', 17.16477510368146),\n",
       "              ('CK', 9.8973832646250521),\n",
       "              ('hit', 9.166589487394182),\n",
       "              ('ck', 7.6700718546943856),\n",
       "              ('*', 7.1492141664940378),\n",
       "              ('FUC', 6.5722388656999842),\n",
       "              ('kk', 6.2236752037964953),\n",
       "              (')', 5.6829909385565118),\n",
       "              ('shi', 5.3681731194915443),\n",
       "              ('k', 5.1339803283158476),\n",
       "              ('fuk', 4.9994243145118462),\n",
       "              ('ole', 4.9715841345111889),\n",
       "              ('a ', 4.632305075838123),\n",
       "              ('ass', 4.3853991731454594),\n",
       "              ('ick', 4.2833871690062342),\n",
       "              ('F', 4.2408514701309246),\n",
       "              ('cun', 4.1440830108213191),\n",
       "              ('U', 4.1321524542173087),\n",
       "              (' u ', 3.9243917514050426),\n",
       "              (' Y', 3.8037984108084761),\n",
       "              ('e', 3.7972826453779396),\n",
       "              ('g', 3.744806563991173),\n",
       "              ('!', 3.6499758526353676),\n",
       "              ('Fuc', 3.6228635277736743),\n",
       "              ('G', 3.0455835621849241),\n",
       "              (' je', 3.0075339041846032),\n",
       "              ('fag', 2.9044769418959762),\n",
       "              ('n', 2.6634088055265206),\n",
       "              ('u', 2.3347123098133831)]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topn_features(clf, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted negative</th>\n",
       "      <th>predicted positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.995151</td>\n",
       "      <td>0.004849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.285826</td>\n",
       "      <td>0.714174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          predicted negative  predicted positive\n",
       "negative            0.995151            0.004849\n",
       "positive            0.285826            0.714174"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', FeatureUnion([\n",
    "        ('words', TransformPipeline([\n",
    "            ('stemmed', FunctionTransformer(lambda X: X[:, 0], validate=False)),\n",
    "            ('vec', TfidfVectorizer()),\n",
    "        ])),\n",
    "        ('chars', TransformPipeline([\n",
    "            ('text', FunctionTransformer(lambda X: X[:, 1], validate=False)),\n",
    "            ('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(1)\n",
    "    ]))\n",
    "])\n",
    "confusion_matrix(clf,\n",
    "                dftrain[['comment_text_stemmed', 'comment_text']],\n",
    "                np.array(dftrain[['obscene']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Threat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01229396,  0.01140302,  0.01157844])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', FeatureUnion([\n",
    "        ('words', TransformPipeline([\n",
    "            ('stemmed', FunctionTransformer(lambda X: X[:, 0], validate=False)),\n",
    "            ('vec', TfidfVectorizer()),\n",
    "        ])),\n",
    "        ('chars', TransformPipeline([\n",
    "            ('text', FunctionTransformer(lambda X: X[:, 1], validate=False)),\n",
    "            ('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(1)\n",
    "    ]))\n",
    "])\n",
    "multilabel_cross_validation(clf,\n",
    "                dftrain[['comment_text_stemmed', 'comment_text']],\n",
    "                np.array(dftrain[['threat']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('die', 10.647688909907837),\n",
       "              ('kill', 10.345985014225247),\n",
       "              ('shoot', 10.093137263103564),\n",
       "              ('will', 7.1736492500765685),\n",
       "              ('rape', 6.6852856630364226),\n",
       "              ('death', 6.6221636896929965),\n",
       "              ('cut', 5.7417548014601687),\n",
       "              ('burn', 5.2044836931849563),\n",
       "              ('you', 4.8260395358764923),\n",
       "              ('stab', 4.5506834263939799),\n",
       "              ('gonna', 4.5303218631765532),\n",
       "              ('your', 4.5142581595666247),\n",
       "              ('ll', 4.4851084387028619),\n",
       "              ('deserv', 4.4621357510804671),\n",
       "              ('ass', 4.4103939918060062),\n",
       "              ('ya', 4.2604268752057264),\n",
       "              ('punch', 4.0920921424254502),\n",
       "              ('dead', 3.9608575979529363),\n",
       "              ('kick', 3.5578818157611276),\n",
       "              ('the', 3.5088202325086724),\n",
       "              ('thank', 3.2971763605901847),\n",
       "              ('hous', 3.0832267409827758),\n",
       "              ('it', 3.0539009905526617),\n",
       "              ('fire', 3.0257061790579147),\n",
       "              ('hunt', 3.0110007822570952),\n",
       "              ('that', 2.7685652399352358),\n",
       "              ('go', 2.4473164669601153),\n",
       "              ('down', 2.4279652080242293),\n",
       "              ('fuck', 2.4180789331428154),\n",
       "              ('and', 2.3525375877317951)]),\n",
       " OrderedDict([('e', 5.3533664267403287),\n",
       "              ('I', 2.5954315164135759),\n",
       "              ('s', 2.2331193087872512),\n",
       "              ('m g', 2.2217045748436299),\n",
       "              ('u', 2.19818886763044),\n",
       "              ('!!', 2.1225772880985319),\n",
       "              ('\"', 1.7380214930259041),\n",
       "              ('fuc', 1.3600029982897233),\n",
       "              (' ', 1.3382222839664042),\n",
       "              ('t', 1.2454079786682462),\n",
       "              ('a', 0.2477012318532695),\n",
       "              ('DO,', 0.0),\n",
       "              (\"DL'\", 0.0),\n",
       "              ('DO?', 0.0),\n",
       "              ('DK.', 0.0),\n",
       "              ('DKA', 0.0),\n",
       "              ('DKP', 0.0),\n",
       "              ('DKi', 0.0),\n",
       "              ('DO/', 0.0),\n",
       "              ('DKp', 0.0),\n",
       "              ('DKx', 0.0),\n",
       "              ('DL', 0.0),\n",
       "              ('DL ', 0.0),\n",
       "              ('DL\"', 0.0),\n",
       "              ('DL(', 0.0),\n",
       "              ('DK)', 0.0),\n",
       "              ('DL)', 0.0),\n",
       "              ('DL*', 0.0),\n",
       "              ('DL,', 0.0),\n",
       "              ('DL-', 0.0)]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topn_features(clf, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted negative</th>\n",
       "      <th>predicted positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.999539</td>\n",
       "      <td>0.000461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.828947</td>\n",
       "      <td>0.171053</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          predicted negative  predicted positive\n",
       "negative            0.999539            0.000461\n",
       "positive            0.828947            0.171053"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', FeatureUnion([\n",
    "        ('words', TransformPipeline([\n",
    "            ('stemmed', FunctionTransformer(lambda X: X[:, 0], validate=False)),\n",
    "            ('vec', TfidfVectorizer()),\n",
    "        ])),\n",
    "        ('chars', TransformPipeline([\n",
    "            ('text', FunctionTransformer(lambda X: X[:, 1], validate=False)),\n",
    "            ('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(1)\n",
    "    ]))\n",
    "])\n",
    "confusion_matrix(clf,\n",
    "                dftrain[['comment_text_stemmed', 'comment_text']],\n",
    "                np.array(dftrain[['threat']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.07649148,  0.0766289 ,  0.07866396])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', FeatureUnion([\n",
    "        ('words', TransformPipeline([\n",
    "            ('stemmed', FunctionTransformer(lambda X: X[:, 0], validate=False)),\n",
    "            ('vec', TfidfVectorizer()),\n",
    "        ])),\n",
    "        ('chars', TransformPipeline([\n",
    "            ('text', FunctionTransformer(lambda X: X[:, 1], validate=False)),\n",
    "            ('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(1)\n",
    "    ]))\n",
    "])\n",
    "multilabel_cross_validation(clf,\n",
    "                dftrain[['comment_text_stemmed', 'comment_text']],\n",
    "                np.array(dftrain[['insult']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('idiot', 14.61493401320846),\n",
       "              ('stupid', 10.460707863000124),\n",
       "              ('bitch', 10.068198552822121),\n",
       "              ('asshol', 9.3061777198414628),\n",
       "              ('bastard', 9.2856195589938437),\n",
       "              ('moron', 9.121413446122336),\n",
       "              ('loser', 7.9746274054634316),\n",
       "              ('faggot', 7.8300219514181473),\n",
       "              ('cunt', 7.5646356738279161),\n",
       "              ('ass', 6.7678215949833813),\n",
       "              ('fool', 6.7641099580020558),\n",
       "              ('dickhead', 6.6816746161238454),\n",
       "              ('pathet', 6.6719829595830022),\n",
       "              ('retard', 6.6661075767170868),\n",
       "              ('pig', 6.6304465355617435),\n",
       "              ('goddamn', 5.4838582697541458),\n",
       "              ('jerk', 5.2033711209353175),\n",
       "              ('nigger', 5.1427837628813027),\n",
       "              ('whore', 5.1291570014010208),\n",
       "              ('fat', 5.0383341804619866),\n",
       "              ('thank', 5.030816872102152),\n",
       "              ('scum', 4.9127856365103799),\n",
       "              ('gay', 4.8990980625820892),\n",
       "              ('filthi', 4.8687232635281745),\n",
       "              ('nigga', 4.8238530635724146),\n",
       "              ('talk', 4.7042615654082862),\n",
       "              ('coward', 4.6629554128377952),\n",
       "              ('dumb', 4.5758746480673862),\n",
       "              ('jackass', 4.5722422984184261),\n",
       "              ('ugli', 4.4014843220681659)]),\n",
       " OrderedDict([('uck', 11.563833293055229),\n",
       "              (')', 9.5861270127933889),\n",
       "              ('dum', 8.0008469764515606),\n",
       "              ('ion', 6.8616406723990337),\n",
       "              ('hit', 6.5501539851742603),\n",
       "              ('CK', 5.2851215747137061),\n",
       "              ('U', 5.2813734747029528),\n",
       "              ('a ', 5.2397958397771918),\n",
       "              (':', 5.2379914941472769),\n",
       "              ('bag', 4.8625698856457342),\n",
       "              ('fa', 4.5292035451203088),\n",
       "              ('You', 4.2922598112527686),\n",
       "              ('G', 4.2031430447481402),\n",
       "              (' je', 4.1382469884666193),\n",
       "              ('F', 4.1187683238138852),\n",
       "              ('u ', 4.1143505938269351),\n",
       "              ('ker', 4.1135009205189768),\n",
       "              ('kk', 4.0782330334353114),\n",
       "              ('t', 4.0693312114827345),\n",
       "              ('I', 3.9435043942720251),\n",
       "              ('.1', 3.9327631001218388),\n",
       "              ('*', 3.8663540131122933),\n",
       "              (' a ', 3.8662959143816567),\n",
       "              ('Go ', 3.8261555364606585),\n",
       "              ('ick', 3.6949938044959381),\n",
       "              ('k', 3.6829208747439983),\n",
       "              ('nc', 3.6325368962745959),\n",
       "              ('u', 3.5540280998371205),\n",
       "              (' u ', 3.5311975067115422),\n",
       "              (', y', 3.4759019956104247)]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topn_features(clf, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted negative</th>\n",
       "      <th>predicted positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.991964</td>\n",
       "      <td>0.008036</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.440806</td>\n",
       "      <td>0.559194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          predicted negative  predicted positive\n",
       "negative            0.991964            0.008036\n",
       "positive            0.440806            0.559194"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', FeatureUnion([\n",
    "        ('words', TransformPipeline([\n",
    "            ('stemmed', FunctionTransformer(lambda X: X[:, 0], validate=False)),\n",
    "            ('vec', TfidfVectorizer()),\n",
    "        ])),\n",
    "        ('chars', TransformPipeline([\n",
    "            ('text', FunctionTransformer(lambda X: X[:, 1], validate=False)),\n",
    "            ('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(1)\n",
    "    ]))\n",
    "])\n",
    "confusion_matrix(clf,\n",
    "                dftrain[['comment_text_stemmed', 'comment_text']],\n",
    "                np.array(dftrain[['insult']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identity hate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02691722,  0.02579108,  0.02610433])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', FeatureUnion([\n",
    "        ('words', TransformPipeline([\n",
    "            ('stemmed', FunctionTransformer(lambda X: X[:, 0], validate=False)),\n",
    "            ('vec', TfidfVectorizer()),\n",
    "        ])),\n",
    "        ('chars', TransformPipeline([\n",
    "            ('text', FunctionTransformer(lambda X: X[:, 1], validate=False)),\n",
    "            ('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(1)\n",
    "    ]))\n",
    "])\n",
    "multilabel_cross_validation(clf,\n",
    "                dftrain[['comment_text_stemmed', 'comment_text']],\n",
    "                np.array(dftrain[['identity_hate']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OrderedDict([('nigger', 12.764406797245742),\n",
       "              ('nigga', 11.153887367146481),\n",
       "              ('homosexu', 9.6661401511341953),\n",
       "              ('gay', 9.4736620791336961),\n",
       "              ('jew', 7.4960558066314116),\n",
       "              ('homo', 6.8583479533617346),\n",
       "              ('black', 6.5465079342987682),\n",
       "              ('muslim', 6.3840189731608463),\n",
       "              ('nazi', 6.2521839762790714),\n",
       "              ('talk', 6.2307795578939107),\n",
       "              ('faggot', 6.0887844521414705),\n",
       "              ('negro', 5.3776978496225913),\n",
       "              ('american', 4.9083247662213232),\n",
       "              ('fagot', 4.4063654082752919),\n",
       "              ('racist', 4.2579558026969382),\n",
       "              ('turk', 4.0178618178692549),\n",
       "              ('mexican', 3.9855816742388699),\n",
       "              ('asian', 3.8882240504775378),\n",
       "              ('paki', 3.8665816350835498),\n",
       "              ('semit', 3.7121166563254069),\n",
       "              ('fucker', 3.6795101881780741),\n",
       "              ('thank', 3.6275443289448104),\n",
       "              ('like', 3.5460603195372191),\n",
       "              ('stupid', 3.4301329616529923),\n",
       "              ('not', 3.422936611047743),\n",
       "              ('boy', 3.3616257725218381),\n",
       "              ('it', 3.2939627442455222),\n",
       "              ('there', 3.2793993939314467),\n",
       "              ('hate', 3.2737248913689698),\n",
       "              ('fag', 3.0061235872854248)]),\n",
       " OrderedDict([('ian', 7.1634426516740763),\n",
       "              ('gg', 6.1145827930385934),\n",
       "              ('\"', 6.0315232078983865),\n",
       "              ('ion', 5.4190615739562302),\n",
       "              ('e', 5.3558802555675511),\n",
       "              ('h', 4.5586862748802357),\n",
       "              (' ', 3.574871451536433),\n",
       "              ('O', 3.5174955129091932),\n",
       "              ('I ', 3.3138670572344693),\n",
       "              ('u', 3.2958370464006284),\n",
       "              ('0', 3.1233009400271565),\n",
       "              ('on', 3.0332680486773249),\n",
       "              ('H', 2.9855933343715195),\n",
       "              ('t', 2.8539192411295038),\n",
       "              ('mal', 2.5038123292699459),\n",
       "              ('IS', 2.1990329797281483),\n",
       "              ('..', 2.0332955025921522),\n",
       "              (' g', 1.8982237195409923),\n",
       "              ('sh', 1.6814692211442095),\n",
       "              ('!', 1.4433890076657805),\n",
       "              (' je', 1.3222053921198407),\n",
       "              ('s a', 1.2508801958055142),\n",
       "              ('uck', 1.2318709001919703),\n",
       "              ('A', 0.92515822048278606),\n",
       "              ('!!', 0.75420238314392518),\n",
       "              ('.1', 0.72912320115419349),\n",
       "              ('E', 0.65224637245623585),\n",
       "              ('o', 0.54680727514452609),\n",
       "              ('fuc', 0.32223352817669126),\n",
       "              ('a', 0.2798483170485247)]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topn_features(clf, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted negative</th>\n",
       "      <th>predicted positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>negative</th>\n",
       "      <td>0.998864</td>\n",
       "      <td>0.001136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>positive</th>\n",
       "      <td>0.754902</td>\n",
       "      <td>0.245098</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          predicted negative  predicted positive\n",
       "negative            0.998864            0.001136\n",
       "positive            0.754902            0.245098"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = Pipeline([\n",
    "    ('vec', FeatureUnion([\n",
    "        ('words', TransformPipeline([\n",
    "            ('stemmed', FunctionTransformer(lambda X: X[:, 0], validate=False)),\n",
    "            ('vec', TfidfVectorizer()),\n",
    "        ])),\n",
    "        ('chars', TransformPipeline([\n",
    "            ('text', FunctionTransformer(lambda X: X[:, 1], validate=False)),\n",
    "            ('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)),\n",
    "        ])),\n",
    "    ])),\n",
    "    ('clf', MultilabelClassifier([\n",
    "        LogisticRegression(penalty='l1')\n",
    "        for _ in range(1)\n",
    "    ]))\n",
    "])\n",
    "confusion_matrix(clf,\n",
    "                dftrain[['comment_text_stemmed', 'comment_text']],\n",
    "                np.array(dftrain[['identity_hate']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
