{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy  as np\n",
    "from itertools import chain\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from crossvalidation import multilabel_label_combinations\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Embedding, InputLayer, Bidirectional, LSTM, GlobalMaxPool1D, Dropout, Dense\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain = pd.read_csv('input/train.csv')\n",
    "dftest = pd.read_csv('input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain['comment_text'] = dftrain['comment_text'].apply(str)\n",
    "dftest['comment_text'] = dftest['comment_text'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  toxic  \\\n",
       "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
       "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
       "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
       "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4  79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6044863</td>\n",
       "      <td>==Orphaned non-free media (Image:41cD1jboEvL. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6102620</td>\n",
       "      <td>::Kentuckiana is colloquial.  Even though the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14563293</td>\n",
       "      <td>Hello fellow Wikipedians,\\nI have just modifie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21086297</td>\n",
       "      <td>AKC Suspensions \\nThe Morning Call - Feb 24, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22982444</td>\n",
       "      <td>== [WIKI_LINK: Talk:Celts] ==</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text\n",
       "0   6044863  ==Orphaned non-free media (Image:41cD1jboEvL. ...\n",
       "1   6102620  ::Kentuckiana is colloquial.  Even though the ...\n",
       "2  14563293  Hello fellow Wikipedians,\\nI have just modifie...\n",
       "3  21086297  AKC Suspensions \\nThe Morning Call - Feb 24, 2...\n",
       "4  22982444                      == [WIKI_LINK: Talk:Celts] =="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fasttext-train.txt', 'w', encoding='utf-8') as target:\n",
    "    for text in list(dftrain['comment_text']) + list(dftest['comment_text']):\n",
    "        target.write(\"__label__0__\\t{0}\\n\".format(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Read 1M words\n",
      "Read 2M words\n",
      "Read 3M words\n",
      "Read 4M words\n",
      "Read 5M words\n",
      "Read 6M words\n",
      "Read 7M words\n",
      "Read 8M words\n",
      "Read 9M words\n",
      "Read 10M words\n",
      "Read 11M words\n",
      "Read 12M words\n",
      "Read 13M words\n",
      "Read 14M words\n",
      "Read 15M words\n",
      "Read 16M words\n",
      "Read 17M words\n",
      "Read 18M words\n",
      "Read 19M words\n",
      "Read 20M words\n",
      "Read 21M words\n",
      "Read 22M words\n",
      "Read 23M words\n",
      "Read 24M words\n",
      "Read 24M words\n",
      "Number of words:  119980\n",
      "Number of labels: 1\n",
      "\n",
      "Progress: 0.0%  words/sec/thread: 21  lr: 0.050000  loss: 4.143619  eta: -596523h-14m \n",
      "Progress: 0.0%  words/sec/thread: 43  lr: 0.050000  loss: 4.152472  eta: 132h27m \n",
      "Progress: 0.0%  words/sec/thread: 69  lr: 0.050000  loss: 4.156240  eta: 65h19m \n",
      "Progress: 0.0%  words/sec/thread: 92  lr: 0.050000  loss: 4.157403  eta: 41h0m \n",
      "Progress: 0.0%  words/sec/thread: 118  lr: 0.050000  loss: 4.158525  eta: 30h42m \n",
      "Progress: 0.0%  words/sec/thread: 145  lr: 0.050000  loss: 4.158800  eta: 24h0m \n",
      "Progress: 0.0%  words/sec/thread: 178  lr: 0.050000  loss: 4.159240  eta: 20h3m \n",
      "Progress: 0.0%  words/sec/thread: 215  lr: 0.050000  loss: 4.159158  eta: 15h54m \n",
      "Progress: 0.0%  words/sec/thread: 244  lr: 0.050000  loss: 4.159098  eta: 13h9m \n",
      "Progress: 0.0%  words/sec/thread: 272  lr: 0.050000  loss: 4.159150  eta: 11h36m \n",
      "Progress: 0.0%  words/sec/thread: 298  lr: 0.049999  loss: 4.159002  eta: 10h25m \n",
      "Progress: 0.0%  words/sec/thread: 326  lr: 0.049999  loss: 4.158776  eta: 9h30m \n",
      "Progress: 0.0%  words/sec/thread: 353  lr: 0.049999  loss: 4.158630  eta: 8h40m \n",
      "Progress: 0.0%  words/sec/thread: 526  lr: 0.049999  loss: 4.158446  eta: 5h40m \n",
      "Progress: 0.0%  words/sec/thread: 553  lr: 0.049999  loss: 4.158350  eta: 5h23m \n",
      "Progress: 0.0%  words/sec/thread: 579  lr: 0.049999  loss: 4.158234  eta: 5h7m \n",
      "Progress: 0.0%  words/sec/thread: 626  lr: 0.049999  loss: 4.158099  eta: 4h41m \n",
      "Progress: 0.0%  words/sec/thread: 649  lr: 0.049999  loss: 4.157962  eta: 4h31m \n",
      "Progress: 0.0%  words/sec/thread: 692  lr: 0.049999  loss: 4.157828  eta: 4h12m \n",
      "Progress: 0.0%  words/sec/thread: 711  lr: 0.049999  loss: 4.157679  eta: 4h5m \n",
      "Progress: 0.0%  words/sec/thread: 730  lr: 0.049999  loss: 4.157588  eta: 3h58m \n",
      "Progress: 0.0%  words/sec/thread: 754  lr: 0.049998  loss: 4.157440  eta: 3h52m \n",
      "Progress: 0.0%  words/sec/thread: 971  lr: 0.049998  loss: 4.157242  eta: 3h50m \n",
      "Progress: 0.0%  words/sec/thread: 1015  lr: 0.049998  loss: 4.157045  eta: 2h51m \n",
      "Progress: 0.0%  words/sec/thread: 1036  lr: 0.049998  loss: 4.156923  eta: 2h47m \n",
      "Progress: 0.0%  words/sec/thread: 1174  lr: 0.049998  loss: 4.156576  eta: 2h44m \n",
      "Progress: 0.0%  words/sec/thread: 1240  lr: 0.049997  loss: 4.156512  eta: 2h19m \n",
      "Progress: 0.0%  words/sec/thread: 1294  lr: 0.049997  loss: 4.156262  eta: 2h17m \n",
      "Progress: 0.0%  words/sec/thread: 1647  lr: 0.049997  loss: 4.156100  eta: 2h13m \n",
      "Progress: 0.0%  words/sec/thread: 1688  lr: 0.049996  loss: 4.155982  eta: 1h41m \n",
      "Progress: 0.0%  words/sec/thread: 1730  lr: 0.049996  loss: 4.155864  eta: 1h40m \n",
      "Progress: 0.0%  words/sec/thread: 1751  lr: 0.049996  loss: 4.155669  eta: 1h38m \n",
      "Progress: 0.0%  words/sec/thread: 1772  lr: 0.049996  loss: 4.155571  eta: 1h37m \n",
      "Progress: 0.0%  words/sec/thread: 1790  lr: 0.049996  loss: 4.155478  eta: 1h35m \n",
      "Progress: 0.0%  words/sec/thread: 1814  lr: 0.049996  loss: 4.155273  eta: 1h34m \n",
      "Progress: 0.0%  words/sec/thread: 1841  lr: 0.049996  loss: 4.155063  eta: 1h33m \n",
      "Progress: 0.0%  words/sec/thread: 1895  lr: 0.049996  loss: 4.154864  eta: 1h30m \n",
      "Progress: 0.0%  words/sec/thread: 2361  lr: 0.049996  loss: 4.154517  eta: 1h31m \n",
      "Progress: 0.0%  words/sec/thread: 2407  lr: 0.049995  loss: 4.154105  eta: 1h11m \n",
      "Progress: 0.0%  words/sec/thread: 2530  lr: 0.049995  loss: 4.153526  eta: 1h9m \n",
      "Progress: 0.0%  words/sec/thread: 2587  lr: 0.049994  loss: 4.153198  eta: 1h6m \n",
      "Progress: 0.0%  words/sec/thread: 2686  lr: 0.049994  loss: 4.152653  eta: 1h5m \n",
      "Progress: 0.0%  words/sec/thread: 2747  lr: 0.049994  loss: 4.152290  eta: 1h3m \n",
      "Progress: 0.0%  words/sec/thread: 2764  lr: 0.049994  loss: 4.152082  eta: 1h1m \n",
      "Progress: 0.0%  words/sec/thread: 2810  lr: 0.049994  loss: 4.151650  eta: 1h0m \n",
      "Progress: \n"
     ]
    }
   ],
   "source": [
    "!fasttext skipgram -input fasttext-train.txt -output fasttext-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wordset = set(chain(*map(wordpunct_tokenize, dftrain['comment_text'])))\n",
    "test_wordset = set(chain(*map(wordpunct_tokenize, dftest['comment_text'])))\n",
    "wordset = train_wordset | test_wordset\n",
    "with open('fasttext-words.txt', 'w', encoding='utf-8') as target:\n",
    "    for word in wordset:\n",
    "        target.write(\"{0}\\n\".format(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fasttext print-word-vectors fasttext-model.bin < fasttext-words.txt > fasttext-word-vectors.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = np.zeros([len(wordset), 100])\n",
    "word2index = {}\n",
    "with open('fasttext-word-vectors.txt', 'r', encoding='utf-8') as src:\n",
    "    for line in map(lambda row: row.strip().split(' '), src):\n",
    "        word = line[0]\n",
    "        vector = np.fromiter(map(float, line[1:]), dtype=np.float)\n",
    "        idx = len(word2index)\n",
    "        word2index[word] = idx\n",
    "        embedding[idx, :] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_NB_WORDS = 30000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(list(dftrain['comment_text']) + list(dftest['comment_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 100\n",
    "\n",
    "sequences = tokenizer.texts_to_sequences(dftrain['comment_text'])\n",
    "X = pad_sequences(sequences, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix = np.zeros([len(wordset) + 1, 100])\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if word not in word2index:\n",
    "        continue\n",
    "    embedding_matrix[index, :] = embedding[word2index[word], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(dftrain[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']])\n",
    "y_combinations = multilabel_label_combinations(y, 2)\n",
    "y_converted = np.zeros([len(y)])\n",
    "for i, row in enumerate(y_combinations):\n",
    "    idx = np.all(y == row, axis=1)\n",
    "    y_converted[idx] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(len(dftrain), dtype=np.int)\n",
    "train_idx, test_idx, _, _ = train_test_split(idx, y_converted, stratify=y_converted, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X = X[train_idx]\n",
    "train_y = y[train_idx]\n",
    "val_X = X[test_idx]\n",
    "val_y = y[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding = Embedding(len(wordset) + 1, \n",
    "                      100,\n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=MAXLEN,\n",
    "                      trainable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 100)          42534000  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 100)          60400     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 42,599,756\n",
      "Trainable params: 65,756\n",
      "Non-trainable params: 42,534,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential([\n",
    "    InputLayer(input_shape=(MAXLEN,), dtype='int32'),\n",
    "    embedding,\n",
    "    Bidirectional(LSTM(50, return_sequences=True)),\n",
    "    GlobalMaxPool1D(),\n",
    "    Dropout(0.1),\n",
    "    Dense(50, activation='relu'),\n",
    "    Dropout(0.1),\n",
    "    Dense(6, activation='sigmoid')\n",
    "])\n",
    "model.compile('nadam', 'binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 86265 samples, validate on 9586 samples\n",
      "Epoch 1/5\n",
      "86265/86265 [==============================] - 1942s 23ms/step - loss: 0.0585 - val_loss: 0.0501\n",
      "Epoch 2/5\n",
      "86265/86265 [==============================] - 2016s 23ms/step - loss: 0.0500 - val_loss: 0.0464\n",
      "Epoch 3/5\n",
      "86265/86265 [==============================] - 1812s 21ms/step - loss: 0.0473 - val_loss: 0.0463\n",
      "Epoch 4/5\n",
      "  512/86265 [..............................] - ETA: 23:04 - loss: 0.0439"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-3829a621e7e1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m               \u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m           ],\n\u001b[1;32m----> 9\u001b[1;33m           verbose=True)\n\u001b[0m",
      "\u001b[1;32mC:\\Anaconda3\\envs\\kaggle-toxic\\lib\\site-packages\\keras\\models.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[0;32m    891\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 893\u001b[1;33m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m    894\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    895\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\kaggle-toxic\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m   1629\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1630\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1631\u001b[1;33m                               validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m   1632\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1633\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\kaggle-toxic\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[1;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[0;32m   1211\u001b[0m                     \u001b[0mbatch_logs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'size'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1212\u001b[0m                     \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1213\u001b[1;33m                     \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1214\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\kaggle-toxic\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2330\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m   2331\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2332\u001b[1;33m                               **self.session_kwargs)\n\u001b[0m\u001b[0;32m   2333\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2334\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\kaggle-toxic\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    776\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 778\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    779\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\kaggle-toxic\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m--> 982\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    983\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\kaggle-toxic\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1030\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m-> 1032\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m   1033\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\kaggle-toxic\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1037\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1039\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1040\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda3\\envs\\kaggle-toxic\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1021\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m   1022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1023\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(train_X, train_y,\n",
    "          validation_data=(val_X, val_y),\n",
    "          epochs=5,\n",
    "          batch_size=32,\n",
    "          callbacks=[\n",
    "              ModelCheckpoint('model.h5', save_best_only=True),\n",
    "              EarlyStopping(patience=2),\n",
    "          ],\n",
    "          verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = tokenizer.texts_to_sequences(dftest['comment_text'])\n",
    "X = pad_sequences(sequences, MAXLEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226998/226998 [==============================] - 1047s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(X, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('input/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']] = prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6044863</td>\n",
       "      <td>0.015270</td>\n",
       "      <td>8.847006e-05</td>\n",
       "      <td>0.001641</td>\n",
       "      <td>2.240585e-04</td>\n",
       "      <td>0.001740</td>\n",
       "      <td>0.000470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6102620</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>2.160608e-07</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>3.814144e-07</td>\n",
       "      <td>0.000017</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14563293</td>\n",
       "      <td>0.000254</td>\n",
       "      <td>4.207182e-07</td>\n",
       "      <td>0.000038</td>\n",
       "      <td>8.167399e-07</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.000006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21086297</td>\n",
       "      <td>0.004583</td>\n",
       "      <td>3.357466e-05</td>\n",
       "      <td>0.000566</td>\n",
       "      <td>7.805817e-05</td>\n",
       "      <td>0.000582</td>\n",
       "      <td>0.000248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22982444</td>\n",
       "      <td>0.006814</td>\n",
       "      <td>3.882337e-05</td>\n",
       "      <td>0.000813</td>\n",
       "      <td>9.414572e-05</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.000253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id     toxic  severe_toxic   obscene        threat    insult  \\\n",
       "0   6044863  0.015270  8.847006e-05  0.001641  2.240585e-04  0.001740   \n",
       "1   6102620  0.000208  2.160608e-07  0.000030  3.814144e-07  0.000017   \n",
       "2  14563293  0.000254  4.207182e-07  0.000038  8.167399e-07  0.000018   \n",
       "3  21086297  0.004583  3.357466e-05  0.000566  7.805817e-05  0.000582   \n",
       "4  22982444  0.006814  3.882337e-05  0.000813  9.414572e-05  0.000817   \n",
       "\n",
       "   identity_hate  \n",
       "0       0.000470  \n",
       "1       0.000008  \n",
       "2       0.000006  \n",
       "3       0.000248  \n",
       "4       0.000253  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('output.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
