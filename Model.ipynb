{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Row 1 of 2419021\n",
      "Row 10001 of 2419021\n",
      "Row 20001 of 2419021\n",
      "Row 30001 of 2419021\n",
      "Row 40001 of 2419021\n",
      "Row 50001 of 2419021\n",
      "Row 60001 of 2419021\n",
      "Row 70001 of 2419021\n",
      "Row 80001 of 2419021\n",
      "Row 90001 of 2419021\n",
      "Row 100001 of 2419021\n",
      "Row 110001 of 2419021\n",
      "Row 120001 of 2419021\n",
      "Row 130001 of 2419021\n",
      "Row 140001 of 2419021\n",
      "Row 150001 of 2419021\n",
      "Row 160001 of 2419021\n",
      "Row 170001 of 2419021\n",
      "Row 180001 of 2419021\n",
      "Row 190001 of 2419021\n",
      "Row 200001 of 2419021\n",
      "Row 210001 of 2419021\n",
      "Row 220001 of 2419021\n",
      "Row 230001 of 2419021\n",
      "Row 240001 of 2419021\n",
      "Row 250001 of 2419021\n",
      "Row 260001 of 2419021\n",
      "Row 270001 of 2419021\n",
      "Row 280001 of 2419021\n",
      "Row 290001 of 2419021\n",
      "Row 300001 of 2419021\n",
      "Row 310001 of 2419021\n",
      "Row 320001 of 2419021\n",
      "Row 330001 of 2419021\n",
      "Row 340001 of 2419021\n",
      "Row 350001 of 2419021\n",
      "Row 360001 of 2419021\n",
      "Row 370001 of 2419021\n",
      "Row 380001 of 2419021\n",
      "Row 390001 of 2419021\n",
      "Row 400001 of 2419021\n",
      "Row 410001 of 2419021\n",
      "Row 420001 of 2419021\n",
      "Row 430001 of 2419021\n",
      "Row 440001 of 2419021\n",
      "Row 450001 of 2419021\n",
      "Row 460001 of 2419021\n",
      "Row 470001 of 2419021\n",
      "Row 480001 of 2419021\n",
      "Row 490001 of 2419021\n",
      "Row 500001 of 2419021\n",
      "Row 510001 of 2419021\n",
      "Row 520001 of 2419021\n",
      "Row 530001 of 2419021\n",
      "Row 540001 of 2419021\n",
      "Row 550001 of 2419021\n",
      "Row 560001 of 2419021\n",
      "Row 570001 of 2419021\n",
      "Row 580001 of 2419021\n",
      "Row 590001 of 2419021\n",
      "Row 600001 of 2419021\n",
      "Row 610001 of 2419021\n",
      "Row 620001 of 2419021\n",
      "Row 630001 of 2419021\n",
      "Row 640001 of 2419021\n",
      "Row 650001 of 2419021\n",
      "Row 660001 of 2419021\n",
      "Row 670001 of 2419021\n",
      "Row 680001 of 2419021\n",
      "Row 690001 of 2419021\n",
      "Row 700001 of 2419021\n",
      "Row 710001 of 2419021\n",
      "Row 720001 of 2419021\n",
      "Row 730001 of 2419021\n",
      "Row 740001 of 2419021\n",
      "Row 750001 of 2419021\n",
      "Row 760001 of 2419021\n",
      "Row 770001 of 2419021\n",
      "Row 780001 of 2419021\n",
      "Row 790001 of 2419021\n",
      "Row 800001 of 2419021\n",
      "Row 810001 of 2419021\n",
      "Row 820001 of 2419021\n",
      "Row 830001 of 2419021\n",
      "Row 840001 of 2419021\n",
      "Row 850001 of 2419021\n",
      "Row 860001 of 2419021\n",
      "Row 870001 of 2419021\n",
      "Row 880001 of 2419021\n",
      "Row 890001 of 2419021\n",
      "Row 900001 of 2419021\n",
      "Row 910001 of 2419021\n",
      "Row 920001 of 2419021\n",
      "Row 930001 of 2419021\n",
      "Row 940001 of 2419021\n",
      "Row 950001 of 2419021\n",
      "Row 960001 of 2419021\n",
      "Row 970001 of 2419021\n",
      "Row 980001 of 2419021\n",
      "Row 990001 of 2419021\n",
      "Row 1000001 of 2419021\n",
      "Row 1010001 of 2419021\n",
      "Row 1020001 of 2419021\n",
      "Row 1030001 of 2419021\n",
      "Row 1040001 of 2419021\n",
      "Row 1050001 of 2419021\n",
      "Row 1060001 of 2419021\n",
      "Row 1070001 of 2419021\n",
      "Row 1080001 of 2419021\n",
      "Row 1090001 of 2419021\n",
      "Row 1100001 of 2419021\n",
      "Row 1110001 of 2419021\n",
      "Row 1120001 of 2419021\n",
      "Row 1130001 of 2419021\n",
      "Row 1140001 of 2419021\n",
      "Row 1150001 of 2419021\n",
      "Row 1160001 of 2419021\n",
      "Row 1170001 of 2419021\n",
      "Row 1180001 of 2419021\n",
      "Row 1190001 of 2419021\n",
      "Row 1200001 of 2419021\n",
      "Row 1210001 of 2419021\n",
      "Row 1220001 of 2419021\n",
      "Row 1230001 of 2419021\n",
      "Row 1240001 of 2419021\n",
      "Row 1250001 of 2419021\n",
      "Row 1260001 of 2419021\n",
      "Row 1270001 of 2419021\n",
      "Row 1280001 of 2419021\n",
      "Row 1290001 of 2419021\n",
      "Row 1300001 of 2419021\n",
      "Row 1310001 of 2419021\n",
      "Row 1320001 of 2419021\n",
      "Row 1330001 of 2419021\n",
      "Row 1340001 of 2419021\n",
      "Row 1350001 of 2419021\n",
      "Row 1360001 of 2419021\n",
      "Row 1370001 of 2419021\n",
      "Row 1380001 of 2419021\n",
      "Row 1390001 of 2419021\n",
      "Row 1400001 of 2419021\n",
      "Row 1410001 of 2419021\n",
      "Row 1420001 of 2419021\n",
      "Row 1430001 of 2419021\n",
      "Row 1440001 of 2419021\n",
      "Row 1450001 of 2419021\n",
      "Row 1460001 of 2419021\n",
      "Row 1470001 of 2419021\n",
      "Row 1480001 of 2419021\n",
      "Row 1490001 of 2419021\n",
      "Row 1500001 of 2419021\n",
      "Row 1510001 of 2419021\n",
      "Row 1520001 of 2419021\n",
      "Row 1530001 of 2419021\n",
      "Row 1540001 of 2419021\n",
      "Row 1550001 of 2419021\n",
      "Row 1560001 of 2419021\n",
      "Row 1570001 of 2419021\n",
      "Row 1580001 of 2419021\n",
      "Row 1590001 of 2419021\n",
      "Row 1600001 of 2419021\n",
      "Row 1610001 of 2419021\n",
      "Row 1620001 of 2419021\n",
      "Row 1630001 of 2419021\n",
      "Row 1640001 of 2419021\n",
      "Row 1650001 of 2419021\n",
      "Row 1660001 of 2419021\n",
      "Row 1670001 of 2419021\n",
      "Row 1680001 of 2419021\n",
      "Row 1690001 of 2419021\n",
      "Row 1700001 of 2419021\n",
      "Row 1710001 of 2419021\n",
      "Row 1720001 of 2419021\n",
      "Row 1730001 of 2419021\n",
      "Row 1740001 of 2419021\n",
      "Row 1750001 of 2419021\n",
      "Row 1760001 of 2419021\n",
      "Row 1770001 of 2419021\n",
      "Row 1780001 of 2419021\n",
      "Row 1790001 of 2419021\n",
      "Row 1800001 of 2419021\n",
      "Row 1810001 of 2419021\n",
      "Row 1820001 of 2419021\n",
      "Row 1830001 of 2419021\n",
      "Row 1840001 of 2419021\n",
      "Row 1850001 of 2419021\n",
      "Row 1860001 of 2419021\n",
      "Row 1870001 of 2419021\n",
      "Row 1880001 of 2419021\n",
      "Row 1890001 of 2419021\n",
      "Row 1900001 of 2419021\n",
      "Row 1910001 of 2419021\n",
      "Row 1920001 of 2419021\n",
      "Row 1930001 of 2419021\n",
      "Row 1940001 of 2419021\n",
      "Row 1950001 of 2419021\n",
      "Row 1960001 of 2419021\n",
      "Row 1970001 of 2419021\n",
      "Row 1980001 of 2419021\n",
      "Row 1990001 of 2419021\n",
      "Row 2000001 of 2419021\n",
      "Row 2010001 of 2419021\n",
      "Row 2020001 of 2419021\n",
      "Row 2030001 of 2419021\n",
      "Row 2040001 of 2419021\n",
      "Row 2050001 of 2419021\n",
      "Row 2060001 of 2419021\n",
      "Row 2070001 of 2419021\n",
      "Row 2080001 of 2419021\n",
      "Row 2090001 of 2419021\n",
      "Row 2100001 of 2419021\n",
      "Row 2110001 of 2419021\n",
      "Row 2120001 of 2419021\n",
      "Row 2130001 of 2419021\n",
      "Row 2140001 of 2419021\n",
      "Row 2150001 of 2419021\n",
      "Row 2160001 of 2419021\n",
      "Row 2170001 of 2419021\n",
      "Row 2180001 of 2419021\n",
      "Row 2190001 of 2419021\n",
      "Row 2200001 of 2419021\n",
      "Row 2210001 of 2419021\n",
      "Row 2220001 of 2419021\n",
      "Row 2230001 of 2419021\n",
      "Row 2240001 of 2419021\n",
      "Row 2250001 of 2419021\n",
      "Row 2260001 of 2419021\n",
      "Row 2270001 of 2419021\n",
      "Row 2280001 of 2419021\n",
      "Row 2290001 of 2419021\n",
      "Row 2300001 of 2419021\n",
      "Row 2310001 of 2419021\n",
      "Row 2320001 of 2419021\n",
      "Row 2330001 of 2419021\n",
      "Row 2340001 of 2419021\n",
      "Row 2350001 of 2419021\n",
      "Row 2360001 of 2419021\n",
      "Row 2370001 of 2419021\n",
      "Row 2380001 of 2419021\n",
      "Row 2390001 of 2419021\n",
      "Row 2400001 of 2419021\n",
      "Row 2410001 of 2419021\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def read_vectors():\n",
    "    with open('vectors.vec', 'r', encoding='utf-8') as src:\n",
    "        row_count, vector_size = map(int, src.readline().split(' '))\n",
    "        vectors = np.zeros([row_count, vector_size], dtype=np.float32)\n",
    "        word2index = {}\n",
    "        for i in range(row_count):\n",
    "            if (i % 10000) == 0:\n",
    "                print(\"Row {0} of {1}\".format(i + 1, row_count))\n",
    "            row = src.readline()\n",
    "            parts = row.split(' ')\n",
    "            if len(parts) != (vector_size + 1):\n",
    "                continue\n",
    "            word = parts[0]\n",
    "            vector = np.fromiter(map(float, parts[1:]), dtype=np.float32)\n",
    "            word2index[word] = len(word2index)\n",
    "            vectors[word2index[word]] = vector\n",
    "    return word2index, vectors\n",
    "\n",
    "global_word2index, global_vectors = read_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "dftrain = pd.read_csv('input/train.csv')\n",
    "dftest = pd.read_csv('input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def tokenize(text, lowercase=True):\n",
    "    if lowercase:\n",
    "        text = text.lower()\n",
    "    delimeter = \"([\\\\/.,`~@#4%^&*()-+\\[\\]{}<>'\\\"]*[ \\s\\n\\t\\r]+|[!?.])\"\n",
    "    tokens = re.split(delimeter, text + \" \")\n",
    "    stripped_tokens = map(str.strip, tokens)\n",
    "    noempty_tokens = filter(bool, stripped_tokens)\n",
    "    return list(noempty_tokens)\n",
    "  \n",
    "text_train = dftrain['comment_text'].apply(tokenize)\n",
    "text_test = dftest['comment_text'].apply(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "\n",
    "def get_words():\n",
    "    words_train = set(chain(*text_train))\n",
    "    words_test = set(chain(*text_test))\n",
    "    vocabulary_words = set(global_word2index.keys())\n",
    "    return sorted((words_train | words_test) & vocabulary_words)\n",
    "\n",
    "words = get_words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vectors():\n",
    "    word2index = {\n",
    "        word: i\n",
    "        for i, word in enumerate(words)\n",
    "    }\n",
    "    vectors = np.zeros([\n",
    "        len(word2index),\n",
    "        global_vectors.shape[1]\n",
    "    ])\n",
    "    for i, word in enumerate(words):\n",
    "        vectors[i] = global_vectors[global_word2index[word]]\n",
    "    return word2index, vectors\n",
    "\n",
    "word2index, vectors = get_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "221"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gc\n",
    "\n",
    "del global_word2index\n",
    "del global_vectors\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 500\n",
    "\n",
    "def tokens_to_sequence(tokens):\n",
    "    result = np.ones([MAXLEN], dtype=np.int32) * len(word2index)\n",
    "    subsequence = [\n",
    "        word2index[word]\n",
    "        for word in tokens\n",
    "        if word in word2index\n",
    "    ]\n",
    "    size = min(MAXLEN, len(subsequence))\n",
    "    result[:size] = subsequence[:size]\n",
    "    return result\n",
    "\n",
    "sequence_train = np.array([tokens_to_sequence(text) for text in text_train])\n",
    "sequence_test = np.array([tokens_to_sequence(text) for text in text_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del text_train\n",
    "del text_test\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_matrix():\n",
    "    embedding_matrix = np.zeros([vectors.shape[0] + 1, vectors.shape[1]])\n",
    "    embedding_matrix[:vectors.shape[0], :] = vectors\n",
    "    return embedding_matrix\n",
    "\n",
    "embedding_matrix = get_embedding_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "targets_train = np.array(dftrain[targets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "188"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del dftrain\n",
    "del dftest\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import InputLayer, Embedding, Bidirectional, CuDNNGRU, Dropout, Dense, Input, Multiply, Lambda\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.models import Sequential, Model\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tflearn.objectives import roc_auc_score as roc_auc_score_tf\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_loss(y_true, y_pred):\n",
    "    return roc_auc_score_tf(y_pred, y_true)\n",
    "\n",
    "\n",
    "def loss(y_true, y_pred):\n",
    "    weight_0 = 1/6\n",
    "    weight_1 = 1/6\n",
    "    weight_2 = 1/6\n",
    "    weight_3 = 1/6\n",
    "    weight_4 = 1/6\n",
    "    weight_5 = 1/6\n",
    "    return  weight_0 * label_loss(y_true[:, 0], y_pred[:, 0]) + \\\n",
    "            weight_1 * label_loss(y_true[:, 1], y_pred[:, 1]) + \\\n",
    "            weight_2 * label_loss(y_true[:, 2], y_pred[:, 2]) + \\\n",
    "            weight_3 * label_loss(y_true[:, 3], y_pred[:, 3]) + \\\n",
    "            weight_4 * label_loss(y_true[:, 4], y_pred[:, 4]) + \\\n",
    "            weight_5 * label_loss(y_true[:, 5], y_pred[:, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.008\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def loss_test():\n",
    "    with tf.Session() as sess:\n",
    "        y_pred = tf.constant([[0.8, 0.7, 0.8, 0.7, 0.8, 0.7,],\n",
    "                              [0.7, 0.6, 0.7, 0.6, 0.7, 0.6,],\n",
    "                              [0.1, 0.0, 0.1, 0.0, 0.1, 0.0,],\n",
    "                              [0.2, 0.1, 0.2, 0.1, 0.2, 0.1,]])\n",
    "        y_true = tf.constant([[1.0, 0.0, 1.0, 0.0, 1.0, 0.0,],\n",
    "                              [1.0, 1.0, 1.0, 1.0, 1.0, 1.0,],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0,],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0,]])\n",
    "        print(sess.run(loss(y_pred, y_true)))\n",
    "        y_pred = tf.constant([[1.0, 0.0, 1.0, 0.0, 1.0, 0.0,],\n",
    "                              [1.0, 1.0, 1.0, 1.0, 1.0, 1.0,],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0,],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0,]])\n",
    "        y_true = tf.constant([[1.0, 0.0, 1.0, 0.0, 1.0, 0.0,],\n",
    "                              [1.0, 1.0, 1.0, 1.0, 1.0, 1.0,],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0,],\n",
    "                              [0.0, 0.0, 0.0, 0.0, 0.0, 0.0,]])\n",
    "        print(sess.run(loss(y_pred, y_true)))\n",
    "        \n",
    "loss_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding():\n",
    "    return Embedding(embedding_matrix.shape[0],\n",
    "                     embedding_matrix.shape[1],\n",
    "                     weights=[embedding_matrix],\n",
    "                     trainable=False,\n",
    "                     input_length=MAXLEN)\n",
    "\n",
    "def get_model():\n",
    "    K.clear_session()\n",
    "    model = Sequential([\n",
    "        InputLayer(input_shape=(MAXLEN,), dtype='int32'),\n",
    "        get_embedding(),\n",
    "        Bidirectional(CuDNNGRU(64, return_sequences=True)),\n",
    "        Dropout(0.3),\n",
    "        Bidirectional(CuDNNGRU(64, return_sequences=False)),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(6, activation='sigmoid')\n",
    "    ])\n",
    "    model.compile(optimizer=RMSprop(clipvalue=2, clipnorm=2),\n",
    "                  loss=loss,\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def get_model_trainable_weights(model):\n",
    "    weights = []\n",
    "    for layer in model.layers:\n",
    "        if layer.trainable:\n",
    "            weights.append(layer.get_weights())\n",
    "    return weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from keras.callbacks import Callback\n",
    "\n",
    "\n",
    "def metric(y_true, y_pred):\n",
    "    values = []\n",
    "    for i in range(y_true.shape[1]):\n",
    "        values.append(roc_auc_score(y_true[:, i], y_pred[:, i]))\n",
    "    return np.array(values).mean()\n",
    "\n",
    "\n",
    "class checkpoint(Callback):\n",
    "    def __init__(self, training_data, validation_data, batch_size, fname):\n",
    "        self.x = training_data[0]\n",
    "        self.y = training_data[1]\n",
    "        self.x_val = validation_data[0]\n",
    "        self.y_val = validation_data[1]\n",
    "        self.batch_size = batch_size\n",
    "        self.fname = fname\n",
    "        self.best_score = None\n",
    "        self.best_epoch = None\n",
    "\n",
    "\n",
    "    def on_train_begin(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_train_end(self, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        y_pred = self.model.predict(self.x, batch_size=self.batch_size)\n",
    "        roc = metric(self.y, y_pred)\n",
    "        y_pred_val = self.model.predict(self.x_val, batch_size=self.batch_size)\n",
    "        roc_val = metric(self.y_val, y_pred_val)\n",
    "        print('\\rroc-auc: %s - roc-auc_val: %s' % (str(round(roc,4)),str(round(roc_val,4))),end=100*' '+'\\n')\n",
    "        \n",
    "        if (self.best_score is None) or (roc_val > self.best_score):\n",
    "            self.best_score = roc_val\n",
    "            self.best_epoch = epoch\n",
    "            self.model.save(self.fname)\n",
    "\n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        return\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLD_COUNT = 10\n",
    "FOLD_SIZE = len(sequence_train) // FOLD_COUNT\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "\n",
    "def fold_mask(fold):\n",
    "    start_index = FOLD_SIZE * fold\n",
    "    if fold == FOLD_COUNT - 1:\n",
    "        end_index = len(sequence_train) - 1\n",
    "    else:\n",
    "        end_index = FOLD_SIZE * (fold + 1)\n",
    "    fold_mask = np.array([True] * len(sequence_train))\n",
    "    fold_mask[start_index : end_index] = False\n",
    "    return fold_mask\n",
    "\n",
    "\n",
    "def fold_train(fold):\n",
    "    print(\"Training fold {0}\".format(fold))\n",
    "    train_mask = fold_mask(fold)\n",
    "    val_mask = np.logical_not(train_mask)\n",
    "    model = get_model()\n",
    "    model.fit(sequence_train[train_mask], targets_train[train_mask], \n",
    "              validation_data=(sequence_train[val_mask], targets_train[val_mask]),\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=60,\n",
    "              verbose=True,\n",
    "              callbacks=[\n",
    "                  checkpoint(\n",
    "                      (sequence_train[train_mask], targets_train[train_mask]),\n",
    "                      (sequence_train[val_mask], targets_train[val_mask]),\n",
    "                      BATCH_SIZE,\n",
    "                      'model.tmp'\n",
    "                  ),\n",
    "                  ModelCheckpoint('model.tmp', save_best_only=True),\n",
    "                  EarlyStopping(monitor='val_loss', patience=5)\n",
    "              ])\n",
    "    model.load_weights('model.tmp')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 0\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/60\n",
      "roc-auc: 0.9838 - roc-auc_val: 0.9827                                                                                                    \n",
      "143614/143614 [==============================] - 277s 2ms/step - loss: 2.0053 - acc: 0.0204 - val_loss: 1.0417 - val_acc: 0.0013\n",
      "Epoch 2/60\n",
      "roc-auc: 0.9875 - roc-auc_val: 0.9852                                                                                                    \n",
      "143614/143614 [==============================] - 276s 2ms/step - loss: 1.0260 - acc: 0.0016 - val_loss: 0.9130 - val_acc: 8.7736e-04\n",
      "Epoch 3/60\n",
      "roc-auc: 0.9904 - roc-auc_val: 0.988                                                                                                    \n",
      "143614/143614 [==============================] - 272s 2ms/step - loss: 0.8574 - acc: 0.0091 - val_loss: 0.9466 - val_acc: 0.0021\n",
      "Epoch 4/60\n",
      "roc-auc: 0.9917 - roc-auc_val: 0.9885                                                                                                    \n",
      "143614/143614 [==============================] - 278s 2ms/step - loss: 0.7594 - acc: 0.0152 - val_loss: 0.8580 - val_acc: 0.0014\n",
      "Epoch 5/60\n",
      "roc-auc: 0.9927 - roc-auc_val: 0.9887                                                                                                    \n",
      "143614/143614 [==============================] - 279s 2ms/step - loss: 0.6888 - acc: 0.0352 - val_loss: 0.8491 - val_acc: 0.0452\n",
      "Epoch 6/60\n",
      "roc-auc: 0.9934 - roc-auc_val: 0.9895                                                                                                    \n",
      "143614/143614 [==============================] - 281s 2ms/step - loss: 0.6229 - acc: 0.0825 - val_loss: 0.8534 - val_acc: 0.1733\n",
      "Epoch 7/60\n",
      "roc-auc: 0.9939 - roc-auc_val: 0.9883                                                                                                    \n",
      "143614/143614 [==============================] - 279s 2ms/step - loss: 0.5675 - acc: 0.2037 - val_loss: 1.1504 - val_acc: 0.2782\n",
      "Epoch 8/60\n",
      "roc-auc: 0.9944 - roc-auc_val: 0.9884                                                                                                    \n",
      "143614/143614 [==============================] - 275s 2ms/step - loss: 0.5235 - acc: 0.3880 - val_loss: 0.9833 - val_acc: 0.3815\n",
      "Epoch 9/60\n",
      "roc-auc: 0.9946 - roc-auc_val: 0.9887                                                                                                    \n",
      "143614/143614 [==============================] - 273s 2ms/step - loss: 0.4690 - acc: 0.5031 - val_loss: 1.0237 - val_acc: 0.4795\n",
      "Epoch 10/60\n",
      "roc-auc: 0.9949 - roc-auc_val: 0.9878                                                                                                    \n",
      "143614/143614 [==============================] - 274s 2ms/step - loss: 0.4398 - acc: 0.6610 - val_loss: 1.3582 - val_acc: 0.7002\n",
      "Training fold 1\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/60\n",
      "roc-auc: 0.983 - roc-auc_val: 0.9806                                                                                                    \n",
      "143614/143614 [==============================] - 282s 2ms/step - loss: 2.0607 - acc: 0.0876 - val_loss: 1.2944 - val_acc: 3.7601e-04\n",
      "Epoch 2/60\n",
      "roc-auc: 0.9879 - roc-auc_val: 0.9857                                                                                                    \n",
      "143614/143614 [==============================] - 274s 2ms/step - loss: 0.9989 - acc: 0.0016 - val_loss: 1.1191 - val_acc: 0.0016\n",
      "Epoch 3/60\n",
      "roc-auc: 0.9904 - roc-auc_val: 0.9885                                                                                                    \n",
      "143614/143614 [==============================] - 266s 2ms/step - loss: 0.8460 - acc: 0.0025 - val_loss: 0.8986 - val_acc: 7.5202e-04\n",
      "Epoch 4/60\n",
      "roc-auc: 0.9918 - roc-auc_val: 0.9892                                                                                                    \n",
      "143614/143614 [==============================] - 264s 2ms/step - loss: 0.7473 - acc: 0.0225 - val_loss: 0.9000 - val_acc: 0.0033\n",
      "Epoch 5/60\n",
      "roc-auc: 0.9924 - roc-auc_val: 0.9887                                                                                                    \n",
      "143614/143614 [==============================] - 260s 2ms/step - loss: 0.6831 - acc: 0.0289 - val_loss: 1.1189 - val_acc: 0.0089\n",
      "Epoch 6/60\n",
      "roc-auc: 0.9933 - roc-auc_val: 0.9888                                                                                                    \n",
      "143614/143614 [==============================] - 263s 2ms/step - loss: 0.6347 - acc: 0.0451 - val_loss: 0.8975 - val_acc: 0.0060\n",
      "Epoch 7/60\n",
      "roc-auc: 0.9934 - roc-auc_val: 0.9893                                                                                                    \n",
      "143614/143614 [==============================] - 263s 2ms/step - loss: 0.5794 - acc: 0.1228 - val_loss: 0.9931 - val_acc: 0.0704\n",
      "Epoch 8/60\n",
      "roc-auc: 0.9942 - roc-auc_val: 0.9885                                                                                                    \n",
      "143614/143614 [==============================] - 261s 2ms/step - loss: 0.5348 - acc: 0.3178 - val_loss: 1.2945 - val_acc: 0.5912\n",
      "Epoch 9/60\n",
      "roc-auc: 0.9946 - roc-auc_val: 0.9886                                                                                                    \n",
      "143614/143614 [==============================] - 260s 2ms/step - loss: 0.4791 - acc: 0.5778 - val_loss: 1.3393 - val_acc: 0.6433\n",
      "Epoch 10/60\n",
      "roc-auc: 0.9948 - roc-auc_val: 0.9885                                                                                                    \n",
      "143614/143614 [==============================] - 260s 2ms/step - loss: 0.4352 - acc: 0.6319 - val_loss: 1.2031 - val_acc: 0.6798\n",
      "Epoch 11/60\n",
      "roc-auc: 0.9953 - roc-auc_val: 0.988                                                                                                    \n",
      "143614/143614 [==============================] - 260s 2ms/step - loss: 0.4046 - acc: 0.7018 - val_loss: 1.5467 - val_acc: 0.6346\n",
      "Training fold 2\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/60\n",
      "roc-auc: 0.981 - roc-auc_val: 0.9812                                                                                                    \n",
      "143614/143614 [==============================] - 266s 2ms/step - loss: 2.1002 - acc: 0.0059 - val_loss: 1.0720 - val_acc: 8.7736e-04\n",
      "Epoch 2/60\n",
      "roc-auc: 0.9873 - roc-auc_val: 0.9874                                                                                                    \n",
      "143614/143614 [==============================] - 266s 2ms/step - loss: 1.0030 - acc: 0.0013 - val_loss: 0.9459 - val_acc: 0.0011\n",
      "Epoch 3/60\n",
      "roc-auc: 0.9897 - roc-auc_val: 0.9896                                                                                                    \n",
      "143614/143614 [==============================] - 267s 2ms/step - loss: 0.8455 - acc: 0.0063 - val_loss: 0.8583 - val_acc: 0.0016\n",
      "Epoch 4/60\n",
      "roc-auc: 0.9906 - roc-auc_val: 0.9888                                                                                                    \n",
      "143614/143614 [==============================] - 262s 2ms/step - loss: 0.7522 - acc: 0.0585 - val_loss: 0.8498 - val_acc: 0.1766\n",
      "Epoch 5/60\n",
      "roc-auc: 0.9925 - roc-auc_val: 0.9902                                                                                                    \n",
      "143614/143614 [==============================] - 262s 2ms/step - loss: 0.6940 - acc: 0.3886 - val_loss: 0.8774 - val_acc: 0.4830\n",
      "Epoch 6/60\n",
      "roc-auc: 0.9923 - roc-auc_val: 0.9905                                                                                                    \n",
      "143614/143614 [==============================] - 262s 2ms/step - loss: 0.6327 - acc: 0.5628 - val_loss: 1.0137 - val_acc: 0.7287\n",
      "Epoch 7/60\n",
      "roc-auc: 0.9938 - roc-auc_val: 0.9905                                                                                                    \n",
      "143614/143614 [==============================] - 259s 2ms/step - loss: 0.5779 - acc: 0.6367 - val_loss: 0.9360 - val_acc: 0.6103\n",
      "Epoch 8/60\n",
      "roc-auc: 0.9941 - roc-auc_val: 0.9907                                                                                                    \n",
      "143614/143614 [==============================] - 262s 2ms/step - loss: 0.5260 - acc: 0.6981 - val_loss: 1.0647 - val_acc: 0.7354\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/60\n",
      "roc-auc: 0.9943 - roc-auc_val: 0.9899                                                                                                    \n",
      "143614/143614 [==============================] - 259s 2ms/step - loss: 0.4732 - acc: 0.7576 - val_loss: 1.2206 - val_acc: 0.7884\n",
      "Training fold 3\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/60\n",
      "roc-auc: 0.9827 - roc-auc_val: 0.9781                                                                                                    \n",
      "143614/143614 [==============================] - 268s 2ms/step - loss: 1.9161 - acc: 0.0022 - val_loss: 1.1284 - val_acc: 0.0029\n",
      "Epoch 2/60\n",
      "roc-auc: 0.9876 - roc-auc_val: 0.9828                                                                                                    \n",
      "143614/143614 [==============================] - 263s 2ms/step - loss: 0.9773 - acc: 0.0021 - val_loss: 1.1351 - val_acc: 0.0034\n",
      "Epoch 3/60\n",
      "roc-auc: 0.9898 - roc-auc_val: 0.985                                                                                                    \n",
      "143614/143614 [==============================] - 266s 2ms/step - loss: 0.8246 - acc: 0.0026 - val_loss: 0.9903 - val_acc: 0.0031\n",
      "Epoch 4/60\n",
      "roc-auc: 0.9917 - roc-auc_val: 0.9868                                                                                                    \n",
      "143614/143614 [==============================] - 266s 2ms/step - loss: 0.7525 - acc: 0.0025 - val_loss: 0.9282 - val_acc: 0.0029\n",
      "Epoch 5/60\n",
      "roc-auc: 0.9925 - roc-auc_val: 0.9864                                                                                                    \n",
      "143614/143614 [==============================] - 260s 2ms/step - loss: 0.6665 - acc: 0.0046 - val_loss: 0.9614 - val_acc: 0.0087\n",
      "Epoch 6/60\n",
      "roc-auc: 0.9933 - roc-auc_val: 0.986                                                                                                    \n",
      "143614/143614 [==============================] - 259s 2ms/step - loss: 0.6176 - acc: 0.0562 - val_loss: 1.0768 - val_acc: 0.0696\n",
      "Epoch 7/60\n",
      "roc-auc: 0.9933 - roc-auc_val: 0.9864                                                                                                    \n",
      "143614/143614 [==============================] - 260s 2ms/step - loss: 0.5603 - acc: 0.1990 - val_loss: 1.0868 - val_acc: 0.2737\n",
      "Epoch 8/60\n",
      "roc-auc: 0.9943 - roc-auc_val: 0.9863                                                                                                    \n",
      "143614/143614 [==============================] - 260s 2ms/step - loss: 0.5234 - acc: 0.3697 - val_loss: 1.1015 - val_acc: 0.2881\n",
      "Epoch 9/60\n",
      "roc-auc: 0.9946 - roc-auc_val: 0.9866                                                                                                    \n",
      "143614/143614 [==============================] - 259s 2ms/step - loss: 0.4879 - acc: 0.4824 - val_loss: 1.1759 - val_acc: 0.4421\n",
      "Training fold 4\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/60\n",
      "roc-auc: 0.9806 - roc-auc_val: 0.9808                                                                                                    \n",
      "143614/143614 [==============================] - 268s 2ms/step - loss: 1.9060 - acc: 0.0054 - val_loss: 1.1351 - val_acc: 1.8801e-04\n",
      "Epoch 2/60\n",
      "roc-auc: 0.9876 - roc-auc_val: 0.9862                                                                                                    \n",
      "143614/143614 [==============================] - 267s 2ms/step - loss: 0.9956 - acc: 0.0013 - val_loss: 1.0498 - val_acc: 0.0019\n",
      "Epoch 3/60\n",
      "roc-auc: 0.9895 - roc-auc_val: 0.9876                                                                                                    \n",
      "143614/143614 [==============================] - 266s 2ms/step - loss: 0.8275 - acc: 0.0086 - val_loss: 0.9979 - val_acc: 0.0044\n",
      "Epoch 4/60\n",
      "roc-auc: 0.9911 - roc-auc_val: 0.9884                                                                                                    \n",
      "143614/143614 [==============================] - 266s 2ms/step - loss: 0.7523 - acc: 0.0183 - val_loss: 0.9937 - val_acc: 0.1980\n",
      "Epoch 5/60\n",
      "roc-auc: 0.9923 - roc-auc_val: 0.9881                                                                                                    \n",
      "143614/143614 [==============================] - 262s 2ms/step - loss: 0.6761 - acc: 0.1432 - val_loss: 0.9372 - val_acc: 0.1492\n",
      "Epoch 6/60\n",
      "roc-auc: 0.9929 - roc-auc_val: 0.9884                                                                                                    \n",
      "143614/143614 [==============================] - 263s 2ms/step - loss: 0.6188 - acc: 0.4524 - val_loss: 1.1035 - val_acc: 0.6524\n",
      "Epoch 7/60\n",
      "roc-auc: 0.9929 - roc-auc_val: 0.988                                                                                                    \n",
      "143614/143614 [==============================] - 260s 2ms/step - loss: 0.5700 - acc: 0.6971 - val_loss: 1.1756 - val_acc: 0.8159\n",
      "Epoch 8/60\n",
      "roc-auc: 0.9937 - roc-auc_val: 0.988                                                                                                    \n",
      "143614/143614 [==============================] - 259s 2ms/step - loss: 0.5206 - acc: 0.7560 - val_loss: 1.3713 - val_acc: 0.8253\n",
      "Epoch 9/60\n",
      "roc-auc: 0.9942 - roc-auc_val: 0.9886                                                                                                    \n",
      "143614/143614 [==============================] - 262s 2ms/step - loss: 0.4811 - acc: 0.8204 - val_loss: 1.5428 - val_acc: 0.8836\n",
      "Epoch 10/60\n",
      "roc-auc: 0.9948 - roc-auc_val: 0.9885                                                                                                    \n",
      "143614/143614 [==============================] - 260s 2ms/step - loss: 0.4465 - acc: 0.8504 - val_loss: 1.5795 - val_acc: 0.8661\n",
      "Training fold 5\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/60\n",
      "roc-auc: 0.9827 - roc-auc_val: 0.9822                                                                                                    \n",
      "143614/143614 [==============================] - 268s 2ms/step - loss: 1.9651 - acc: 0.0022 - val_loss: 0.9453 - val_acc: 0.0021\n",
      "Epoch 2/60\n",
      "roc-auc: 0.9871 - roc-auc_val: 0.9862                                                                                                    \n",
      "143614/143614 [==============================] - 267s 2ms/step - loss: 1.0363 - acc: 0.0021 - val_loss: 0.7155 - val_acc: 0.0023\n",
      "Epoch 3/60\n",
      "roc-auc: 0.9897 - roc-auc_val: 0.9885                                                                                                    \n",
      "143614/143614 [==============================] - 262s 2ms/step - loss: 0.8710 - acc: 0.0023 - val_loss: 0.7202 - val_acc: 0.0026\n",
      "Epoch 4/60\n",
      "roc-auc: 0.9915 - roc-auc_val: 0.9899                                                                                                    \n",
      "143614/143614 [==============================] - 266s 2ms/step - loss: 0.7659 - acc: 0.0041 - val_loss: 0.6608 - val_acc: 0.0034\n",
      "Epoch 5/60\n",
      "roc-auc: 0.9923 - roc-auc_val: 0.9904                                                                                                    \n",
      "143614/143614 [==============================] - 263s 2ms/step - loss: 0.6983 - acc: 0.0076 - val_loss: 0.7007 - val_acc: 0.0042\n",
      "Epoch 6/60\n",
      "roc-auc: 0.9927 - roc-auc_val: 0.9899                                                                                                    \n",
      "143614/143614 [==============================] - 260s 2ms/step - loss: 0.6445 - acc: 0.0426 - val_loss: 0.6978 - val_acc: 0.0044\n",
      "Epoch 7/60\n",
      "roc-auc: 0.9931 - roc-auc_val: 0.9893                                                                                                    \n",
      "143614/143614 [==============================] - 259s 2ms/step - loss: 0.6001 - acc: 0.1033 - val_loss: 0.7549 - val_acc: 0.0656\n",
      "Epoch 8/60\n",
      "roc-auc: 0.9941 - roc-auc_val: 0.9902                                                                                                    \n",
      "143614/143614 [==============================] - 259s 2ms/step - loss: 0.5487 - acc: 0.1804 - val_loss: 0.8801 - val_acc: 0.3165\n",
      "Epoch 9/60\n",
      "roc-auc: 0.9944 - roc-auc_val: 0.9901                                                                                                    \n",
      "143614/143614 [==============================] - 260s 2ms/step - loss: 0.5003 - acc: 0.3787 - val_loss: 1.0631 - val_acc: 0.5591\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training fold 6\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/60\n",
      "roc-auc: 0.9838 - roc-auc_val: 0.9824                                                                                                    \n",
      "143614/143614 [==============================] - 267s 2ms/step - loss: 2.0744 - acc: 0.0060 - val_loss: 0.9450 - val_acc: 0.0020\n",
      "Epoch 2/60\n",
      "roc-auc: 0.9881 - roc-auc_val: 0.9862                                                                                                    \n",
      "143614/143614 [==============================] - 267s 2ms/step - loss: 0.9920 - acc: 0.0192 - val_loss: 0.8725 - val_acc: 0.0045\n",
      "Epoch 3/60\n",
      "roc-auc: 0.9905 - roc-auc_val: 0.9881                                                                                                    \n",
      "143614/143614 [==============================] - 266s 2ms/step - loss: 0.8391 - acc: 0.0060 - val_loss: 0.7374 - val_acc: 0.0030\n",
      "Epoch 4/60\n",
      "roc-auc: 0.9913 - roc-auc_val: 0.9884                                                                                                    \n",
      "143614/143614 [==============================] - 262s 2ms/step - loss: 0.7821 - acc: 0.0093 - val_loss: 0.7441 - val_acc: 0.0073\n",
      "Epoch 5/60\n",
      "roc-auc: 0.992 - roc-auc_val: 0.988                                                                                                    \n",
      "143614/143614 [==============================] - 260s 2ms/step - loss: 0.6959 - acc: 0.0560 - val_loss: 0.8094 - val_acc: 0.0063\n",
      "Epoch 6/60\n",
      "roc-auc: 0.993 - roc-auc_val: 0.9894                                                                                                    \n",
      "143614/143614 [==============================] - 266s 2ms/step - loss: 0.6360 - acc: 0.0901 - val_loss: 0.7169 - val_acc: 0.0186\n",
      "Epoch 7/60\n",
      "roc-auc: 0.9939 - roc-auc_val: 0.9894                                                                                                    \n",
      "143614/143614 [==============================] - 263s 2ms/step - loss: 0.5769 - acc: 0.2083 - val_loss: 0.8238 - val_acc: 0.0375\n",
      "Epoch 8/60\n",
      "roc-auc: 0.9942 - roc-auc_val: 0.9891                                                                                                    \n",
      "143614/143614 [==============================] - 260s 2ms/step - loss: 0.5416 - acc: 0.2562 - val_loss: 0.8981 - val_acc: 0.3524\n",
      "Epoch 9/60\n",
      "roc-auc: 0.9948 - roc-auc_val: 0.9892                                                                                                    \n",
      "143614/143614 [==============================] - 260s 2ms/step - loss: 0.4826 - acc: 0.4340 - val_loss: 1.0024 - val_acc: 0.4528\n",
      "Epoch 10/60\n",
      "roc-auc: 0.9951 - roc-auc_val: 0.9891                                                                                                    \n",
      "143614/143614 [==============================] - 259s 2ms/step - loss: 0.4423 - acc: 0.5502 - val_loss: 0.9948 - val_acc: 0.3768\n",
      "Epoch 11/60\n",
      "roc-auc: 0.9953 - roc-auc_val: 0.9883                                                                                                    \n",
      "143614/143614 [==============================] - 259s 2ms/step - loss: 0.4045 - acc: 0.6063 - val_loss: 1.3368 - val_acc: 0.6826\n",
      "Training fold 7\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/60\n",
      "roc-auc: 0.9819 - roc-auc_val: 0.9819                                                                                                    \n",
      "143614/143614 [==============================] - 269s 2ms/step - loss: 2.0750 - acc: 0.0025 - val_loss: 1.0180 - val_acc: 1.8801e-04\n",
      "Epoch 2/60\n",
      "roc-auc: 0.9876 - roc-auc_val: 0.9864                                                                                                    \n",
      "143614/143614 [==============================] - 267s 2ms/step - loss: 1.0155 - acc: 0.0014 - val_loss: 0.7942 - val_acc: 0.0023\n",
      "Epoch 3/60\n",
      "roc-auc: 0.9899 - roc-auc_val: 0.9877                                                                                                    \n",
      "143614/143614 [==============================] - 269s 2ms/step - loss: 0.8746 - acc: 0.0019 - val_loss: 0.7895 - val_acc: 0.0011\n",
      "Epoch 4/60\n",
      "roc-auc: 0.9904 - roc-auc_val: 0.9879                                                                                                    \n",
      "143614/143614 [==============================] - 261s 2ms/step - loss: 0.7706 - acc: 0.0287 - val_loss: 0.8431 - val_acc: 0.0177\n",
      "Epoch 5/60\n",
      "roc-auc: 0.9922 - roc-auc_val: 0.989                                                                                                    \n",
      "143614/143614 [==============================] - 262s 2ms/step - loss: 0.7089 - acc: 0.2186 - val_loss: 0.8001 - val_acc: 0.1413\n",
      "Epoch 6/60\n",
      "roc-auc: 0.9928 - roc-auc_val: 0.9894                                                                                                    \n",
      "143614/143614 [==============================] - 265s 2ms/step - loss: 0.6396 - acc: 0.1789 - val_loss: 0.7769 - val_acc: 0.1500\n",
      "Epoch 7/60\n",
      "roc-auc: 0.9934 - roc-auc_val: 0.9892                                                                                                    \n",
      "143614/143614 [==============================] - 259s 2ms/step - loss: 0.5785 - acc: 0.2238 - val_loss: 0.8900 - val_acc: 0.4982\n",
      "Epoch 8/60\n",
      "roc-auc: 0.9939 - roc-auc_val: 0.989                                                                                                    \n",
      "143614/143614 [==============================] - 259s 2ms/step - loss: 0.5497 - acc: 0.4313 - val_loss: 1.0139 - val_acc: 0.4243\n",
      "Epoch 9/60\n",
      "roc-auc: 0.9947 - roc-auc_val: 0.989                                                                                                    \n",
      "143614/143614 [==============================] - 260s 2ms/step - loss: 0.4881 - acc: 0.4133 - val_loss: 0.9896 - val_acc: 0.2737\n",
      "Epoch 10/60\n",
      "roc-auc: 0.9947 - roc-auc_val: 0.989                                                                                                    \n",
      "143614/143614 [==============================] - 260s 2ms/step - loss: 0.4411 - acc: 0.4176 - val_loss: 1.1214 - val_acc: 0.4599\n",
      "Epoch 11/60\n",
      "roc-auc: 0.9952 - roc-auc_val: 0.9883                                                                                                    \n",
      "143614/143614 [==============================] - 259s 2ms/step - loss: 0.4118 - acc: 0.4455 - val_loss: 1.2622 - val_acc: 0.2165\n",
      "Training fold 8\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/60\n",
      "roc-auc: 0.9813 - roc-auc_val: 0.981                                                                                                    \n",
      "143614/143614 [==============================] - 267s 2ms/step - loss: 1.9836 - acc: 9.9572e-04 - val_loss: 1.0432 - val_acc: 6.8935e-04\n",
      "Epoch 2/60\n",
      "roc-auc: 0.9878 - roc-auc_val: 0.9863                                                                                                    \n",
      "143614/143614 [==============================] - 266s 2ms/step - loss: 0.9620 - acc: 0.0021 - val_loss: 0.9183 - val_acc: 0.0028\n",
      "Epoch 3/60\n",
      "roc-auc: 0.991 - roc-auc_val: 0.9872                                                                                                    \n",
      "143614/143614 [==============================] - 262s 2ms/step - loss: 0.8229 - acc: 0.0032 - val_loss: 0.9292 - val_acc: 0.0034\n",
      "Epoch 4/60\n",
      "roc-auc: 0.9919 - roc-auc_val: 0.9875                                                                                                    \n",
      "143614/143614 [==============================] - 266s 2ms/step - loss: 0.7406 - acc: 0.0059 - val_loss: 0.8289 - val_acc: 0.0032\n",
      "Epoch 5/60\n",
      "roc-auc: 0.9926 - roc-auc_val: 0.9876                                                                                                    \n",
      "143614/143614 [==============================] - 262s 2ms/step - loss: 0.6712 - acc: 0.0051 - val_loss: 0.9016 - val_acc: 0.0033\n",
      "Epoch 6/60\n",
      "roc-auc: 0.9935 - roc-auc_val: 0.9882                                                                                                    \n",
      "143614/143614 [==============================] - 264s 2ms/step - loss: 0.6257 - acc: 0.0272 - val_loss: 1.0116 - val_acc: 0.0123\n",
      "Epoch 7/60\n",
      "roc-auc: 0.994 - roc-auc_val: 0.9868                                                                                                    \n",
      "143614/143614 [==============================] - 260s 2ms/step - loss: 0.5693 - acc: 0.0615 - val_loss: 1.3368 - val_acc: 0.0291\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/60\n",
      "roc-auc: 0.9943 - roc-auc_val: 0.9865                                                                                                    \n",
      "143614/143614 [==============================] - 260s 2ms/step - loss: 0.5163 - acc: 0.1235 - val_loss: 1.5388 - val_acc: 0.0703\n",
      "Epoch 9/60\n",
      "roc-auc: 0.9945 - roc-auc_val: 0.9874                                                                                                    \n",
      "143614/143614 [==============================] - 260s 2ms/step - loss: 0.4782 - acc: 0.4122 - val_loss: 1.5983 - val_acc: 0.3970\n",
      "Training fold 9\n",
      "Train on 143614 samples, validate on 15957 samples\n",
      "Epoch 1/60\n",
      "roc-auc: 0.9825 - roc-auc_val: 0.9773                                                                                                    \n",
      "143614/143614 [==============================] - 268s 2ms/step - loss: 1.9060 - acc: 0.0380 - val_loss: 1.3044 - val_acc: 0.0845\n",
      "Epoch 2/60\n",
      "roc-auc: 0.9871 - roc-auc_val: 0.9826                                                                                                    \n",
      "143614/143614 [==============================] - 267s 2ms/step - loss: 0.9804 - acc: 0.0292 - val_loss: 0.9614 - val_acc: 0.0038\n",
      "Epoch 3/60\n",
      "roc-auc: 0.99 - roc-auc_val: 0.986                                                                                                    \n",
      "143614/143614 [==============================] - 267s 2ms/step - loss: 0.8371 - acc: 0.0526 - val_loss: 0.9217 - val_acc: 0.0786\n",
      "Epoch 4/60\n",
      "roc-auc: 0.9918 - roc-auc_val: 0.9878                                                                                                    \n",
      "143614/143614 [==============================] - 267s 2ms/step - loss: 0.7541 - acc: 0.1341 - val_loss: 0.8361 - val_acc: 0.0742\n",
      "Epoch 5/60\n",
      "roc-auc: 0.9922 - roc-auc_val: 0.9868                                                                                                    \n",
      "143614/143614 [==============================] - 259s 2ms/step - loss: 0.6726 - acc: 0.2401 - val_loss: 0.9708 - val_acc: 0.1162\n",
      "Epoch 6/60\n",
      "roc-auc: 0.9927 - roc-auc_val: 0.9873                                                                                                    \n",
      "143614/143614 [==============================] - 259s 2ms/step - loss: 0.6264 - acc: 0.3659 - val_loss: 1.0237 - val_acc: 0.2557\n",
      "Epoch 7/60\n",
      "roc-auc: 0.9936 - roc-auc_val: 0.9883                                                                                                    \n",
      "143614/143614 [==============================] - 261s 2ms/step - loss: 0.5755 - acc: 0.4697 - val_loss: 0.9148 - val_acc: 0.4639\n",
      "Epoch 8/60\n",
      "roc-auc: 0.9942 - roc-auc_val: 0.9893                                                                                                    \n",
      "143614/143614 [==============================] - 262s 2ms/step - loss: 0.5285 - acc: 0.5236 - val_loss: 0.9928 - val_acc: 0.5786\n",
      "Epoch 9/60\n",
      "roc-auc: 0.9946 - roc-auc_val: 0.9876                                                                                                    \n",
      "143614/143614 [==============================] - 260s 2ms/step - loss: 0.4868 - acc: 0.6763 - val_loss: 1.2280 - val_acc: 0.7066\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "for i in range(FOLD_COUNT):\n",
    "    model = fold_train(i)\n",
    "    trainable_weights = get_model_trainable_weights(model)\n",
    "    with open('fold-{0}.pkl'.format(i), 'wb') as target:\n",
    "        pickle.dump(trainable_weights, target)\n",
    "    del model\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_trained_models_weights():\n",
    "    models = []\n",
    "    for i in range(FOLD_COUNT):\n",
    "        fname = 'fold-{0}.pkl'.format(i)\n",
    "        with open(fname, 'rb') as src:\n",
    "            models.append(pickle.load(src))\n",
    "    return models\n",
    "\n",
    "model_weights = read_trained_models_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def build_model_val_predictions():\n",
    "#    for i, weights in enumerate(model_weights):\n",
    "#        val_mask = np.logical_not(fold_mask(i))\n",
    "#        model = get_model()\n",
    "#        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_11 (InputLayer)           (None, 500)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 500, 300)     173121600   input_11[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "sequential_1 (Sequential)       (None, 6)            219366      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 6)            219366      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_3 (Sequential)       (None, 6)            219366      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_4 (Sequential)       (None, 6)            219366      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_5 (Sequential)       (None, 6)            219366      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_6 (Sequential)       (None, 6)            219366      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_7 (Sequential)       (None, 6)            219366      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_8 (Sequential)       (None, 6)            219366      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_9 (Sequential)       (None, 6)            219366      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "sequential_10 (Sequential)      (None, 6)            219366      embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 6)            0           sequential_1[1][0]               \n",
      "                                                                 sequential_2[1][0]               \n",
      "                                                                 sequential_3[1][0]               \n",
      "                                                                 sequential_4[1][0]               \n",
      "                                                                 sequential_5[1][0]               \n",
      "                                                                 sequential_6[1][0]               \n",
      "                                                                 sequential_7[1][0]               \n",
      "                                                                 sequential_8[1][0]               \n",
      "                                                                 sequential_9[1][0]               \n",
      "                                                                 sequential_10[1][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 6)            0           multiply_1[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 175,315,260\n",
      "Trainable params: 2,193,660\n",
      "Non-trainable params: 173,121,600\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def build_model():\n",
    "    K.clear_session()\n",
    "    fold_models = []\n",
    "    \n",
    "    for i in range(FOLD_COUNT):\n",
    "        fold_model = Sequential([\n",
    "            InputLayer(input_shape=(MAXLEN, embedding_matrix.shape[1])),\n",
    "            Bidirectional(CuDNNGRU(64, return_sequences=True)),\n",
    "            Dropout(0.3),\n",
    "            Bidirectional(CuDNNGRU(64, return_sequences=False)),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dense(6, activation='sigmoid')\n",
    "        ])\n",
    "        weights = model_weights[i]\n",
    "        trainable_layers = filter(lambda layer: layer.trainable,\n",
    "                                  fold_model.layers)\n",
    "        for i, layer in enumerate(trainable_layers):\n",
    "            layer.set_weights(weights[i])\n",
    "        fold_models.append(fold_model)\n",
    "    \n",
    "    input_layer = Input(shape=(MAXLEN,), dtype='int32')\n",
    "    embedding = get_embedding() (input_layer)\n",
    "    fold_outputs = [\n",
    "        model(embedding)\n",
    "        for model in fold_models\n",
    "    ]\n",
    "    multiply = Multiply() (fold_outputs)\n",
    "    output = Lambda (lambda X: X ** (1 / FOLD_COUNT)) (multiply)\n",
    "    model = Model(input_layer, output)\n",
    "    model.compile(optimizer=RMSprop(clipvalue=1, clipnorm=1),\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    return model \n",
    "\n",
    "\n",
    "model = build_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 604s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "prediction = model.predict(sequence_test, verbose=True, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "153164"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission = pd.read_csv('input/sample_submission.csv')\n",
    "len(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(153164, 6)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic\n",
      "0.016473548\n",
      "0.94471747\n",
      "0.32081753\n",
      "\n",
      "severe_toxic\n",
      "0.0052384604\n",
      "0.9832689\n",
      "0.31150168\n",
      "\n",
      "obscene\n",
      "0.014428718\n",
      "0.9641874\n",
      "0.35117787\n",
      "\n",
      "threat\n",
      "0.00039980368\n",
      "0.9549562\n",
      "0.22804485\n",
      "\n",
      "insult\n",
      "0.0026214288\n",
      "0.9325024\n",
      "0.32914492\n",
      "\n",
      "identity_hate\n",
      "0.0013246255\n",
      "0.96608764\n",
      "0.28602156\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i, target in enumerate(targets):\n",
    "    print(target)\n",
    "    print(prediction[:, i].min())\n",
    "    print(prediction[:, i].max())\n",
    "    print(prediction[:, i].mean())\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[targets] = prediction\n",
    "submission.to_csv('output.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
