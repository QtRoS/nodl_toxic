{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "from itertools import chain\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "from crossvalidation import multilabel_label_combinations\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss, confusion_matrix\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Input, InputLayer, Embedding, Conv1D, LSTM, Bidirectional, GlobalMaxPool1D, Add, Dropout, Dense\n",
    "from keras.models import Sequential, Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain = pd.read_csv(\"input/train.csv\")\n",
    "dftrain['comment_text'] = dftrain['comment_text'].apply(str) # some values parsed as float\n",
    "dftest = pd.read_csv(\"input/test.csv\")\n",
    "dftest['comment_text'] = dftest['comment_text'].apply(str) # some values parsed as float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  toxic  \\\n",
       "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
       "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
       "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
       "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4  79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0             0        0       0       0              0  \n",
       "1             0        0       0       0              0  \n",
       "2             0        0       0       0              0  \n",
       "3             0        0       0       0              0  \n",
       "4             0        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6044863</td>\n",
       "      <td>==Orphaned non-free media (Image:41cD1jboEvL. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6102620</td>\n",
       "      <td>::Kentuckiana is colloquial.  Even though the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14563293</td>\n",
       "      <td>Hello fellow Wikipedians,\\nI have just modifie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>21086297</td>\n",
       "      <td>AKC Suspensions \\nThe Morning Call - Feb 24, 2...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22982444</td>\n",
       "      <td>== [WIKI_LINK: Talk:Celts] ==</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text\n",
       "0   6044863  ==Orphaned non-free media (Image:41cD1jboEvL. ...\n",
       "1   6102620  ::Kentuckiana is colloquial.  Even though the ...\n",
       "2  14563293  Hello fellow Wikipedians,\\nI have just modifie...\n",
       "3  21086297  AKC Suspensions \\nThe Morning Call - Feb 24, 2...\n",
       "4  22982444                      == [WIKI_LINK: Talk:Celts] =="
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftest.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    delimeter = \"([?\\\\/.,`~!@#4%^&*()-+\\[\\]{}<>'\\\"]*[ \\s\\n\\t\\r]+)\"\n",
    "    tokens = re.split(delimeter, text + \" \")\n",
    "    stripped_tokens = map(str.strip, tokens)\n",
    "    noempty_tokens = filter(bool, stripped_tokens)\n",
    "    return list(noempty_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    return \" \".join(tokenize(text.replace(\"'ll\", \" will\").replace(\"n't\", \" not\")))\n",
    "\n",
    "\n",
    "def preprocess_texts(texts):\n",
    "    return [preprocess_text(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain['preprocessed_text'] = preprocess_texts(dftrain['comment_text'])\n",
    "dftest['preprocessed_text'] = preprocess_texts(dftest['comment_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Nonsense ? kiss off , geek . what I said is tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" Please do not vandalize pages , as you did w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" \"\"Points of interest \"\" I removed the \"\"poin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  toxic  \\\n",
       "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
       "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
       "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
       "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4  79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  Nonsense ? kiss off , geek . what I said is tr...  \n",
       "1  \" Please do not vandalize pages , as you did w...  \n",
       "2  \" \"\"Points of interest \"\" I removed the \"\"poin...  \n",
       "3  Asking some his nationality is a Racial offenc...  \n",
       "4  The reader here is not going by my say so for ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_idx():\n",
    "    def get_label_combination_indices():\n",
    "        labels = np.array(dftrain[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']])\n",
    "        label_combinations = multilabel_label_combinations(labels, 2)\n",
    "        label_combination_indices = np.zeros([len(dftrain)])\n",
    "        for i, row in enumerate(label_combinations):\n",
    "            idx = np.all(labels == row, axis=1)\n",
    "            label_combination_indices[idx] = i\n",
    "        return label_combination_indices\n",
    "\n",
    "    label_combination_indices = get_label_combination_indices()\n",
    "    train_idx, val_idx, _, _ = train_test_split(np.arange(len(dftrain), dtype=np.int), \n",
    "                                                label_combination_indices, \n",
    "                                                stratify=label_combination_indices,\n",
    "                                                random_state=42)\n",
    "    \n",
    "    return train_idx, val_idx\n",
    "\n",
    "train_idx, val_idx = get_train_val_idx()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fasttext-train.txt', 'w', encoding='utf-8') as target:\n",
    "    for text in list(dftrain['preprocessed_text']) + list(dftest['preprocessed_text']):\n",
    "        target.write(\"__label__0__\\t{0}\\n\".format(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fasttext skipgram -input fasttext-train.txt -output fasttext-vector-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordset = set(chain(*map(lambda val: val.split(' '), dftrain['preprocessed_text']))) | set(chain(*map(lambda val: val.split(' '), dftest['preprocessed_text'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('fasttext-words.txt', 'w', encoding='utf-8') as target:\n",
    "    for word in wordset:\n",
    "        target.write(\"{0}\\n\".format(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fasttext print-word-vectors fasttext-vector-model.bin < fasttext-words.txt > fasttext-word-vectors.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "word2index = {}\n",
    "with open('fasttext-word-vectors.txt', 'r', encoding='utf-8') as src:\n",
    "    for row in filter(lambda row: len(row) > 0, map(lambda line: line.strip().split(' '), src)):\n",
    "        word = row[0]\n",
    "        vector = np.fromiter(map(float, row[1:]), dtype=np.float)\n",
    "        word2index[word] = len(word2index)\n",
    "        vectors.append(vector)\n",
    "vectors = np.array(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_sequence(text):\n",
    "    sequence = [word2index[word] for word in text.split(' ') if word in word2index]\n",
    "    return sequence\n",
    "\n",
    "\n",
    "def texts_to_sequence(texts):\n",
    "    return [text_to_sequence(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sequences = texts_to_sequence(np.array(dftrain['preprocessed_text'])[train_idx])\n",
    "val_sequences = texts_to_sequence(np.array(dftrain['preprocessed_text'])[val_idx])\n",
    "test_sequences = texts_to_sequence(np.array(dftest['preprocessed_text']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAXLEN = 100\n",
    "train_X = np.array(pad_sequences(train_sequences, maxlen=MAXLEN))\n",
    "val_X = np.array(pad_sequences(val_sequences, maxlen=MAXLEN))\n",
    "test_X = np.array(pad_sequences(test_sequences, maxlen=MAXLEN))\n",
    "del train_sequences\n",
    "del val_sequences\n",
    "del test_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = np.array(dftrain[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']])[train_idx]\n",
    "val_y = np.array(dftrain[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']])[val_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(trainable=False):\n",
    "    embedding = Embedding(len(word2index), 100, weights=[vectors], trainable=trainable)\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion(y_true, y_pred):\n",
    "    false_positive_count = np.logical_and(y_true == 0, y_pred == 1).sum()\n",
    "    false_negative_count = np.logical_and(y_true == 1, y_pred == 0).sum()\n",
    "    true_positive_count = np.logical_and(y_true == 1, y_pred == 1).sum()\n",
    "    true_negative_count = np.logical_and(y_true == 0, y_pred == 0).sum()\n",
    "    \n",
    "    false_positive_rate = false_positive_count / (false_positive_count + true_negative_count)\n",
    "    false_negative_rate = false_negative_count / (false_negative_count + true_positive_count)\n",
    "    true_negative_rate = true_negative_count / (false_positive_count + true_negative_count)\n",
    "    true_positive_rate = true_positive_count / (false_negative_count + true_positive_count)\n",
    "    \n",
    "    return np.array([\n",
    "        [true_negative_rate, false_positive_rate],\n",
    "        [false_negative_rate, true_positive_rate],\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_result(model):\n",
    "    val_prediction = model.predict(val_X, verbose=True)\n",
    "    losses = []\n",
    "    for i, label in enumerate(['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']):\n",
    "        print(label)\n",
    "        loss = log_loss(val_y[:, i], val_prediction[:, i])\n",
    "        losses.append(loss)\n",
    "        print('loss: ', loss)\n",
    "        print(confusion(val_y[:, i], 1.0 * (val_prediction[:, i] > 0.5)))\n",
    "    print('Total loss: ', np.array(losses).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 100, 100)     72162000    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 100, 50)      5050        embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 99, 50)       10050       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 50)           0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 50)           0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 50)           0           global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 50)           0           add_1[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          5100        dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 100)          0           dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 6)            606         dropout_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 72,182,806\n",
      "Trainable params: 20,806\n",
      "Non-trainable params: 72,162,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def get_model():\n",
    "    input = Input(shape=(MAXLEN,), dtype='int32')\n",
    "    embedding = get_embedding() (input)\n",
    "    \n",
    "    conv1 = Conv1D(50, 1, activation='relu') (embedding)\n",
    "    pool1 = GlobalMaxPool1D() (conv1)\n",
    "    \n",
    "    conv2 = Conv1D(50, 2, activation='relu') (embedding)\n",
    "    pool2 = GlobalMaxPool1D() (conv2)\n",
    "    \n",
    "    merge = Add() ([pool1, pool2])\n",
    "    drop1 = Dropout(0.3) (merge)\n",
    "    fc1 = Dense(100, activation='relu') (drop1)\n",
    "    drop2 = Dropout(0.3) (fc1)\n",
    "    fc2 = Dense(6, activation='sigmoid') (drop2)\n",
    "    \n",
    "    model = Model(input, fc2)\n",
    "    \n",
    "    return model\n",
    "\n",
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('nadam', 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71888 samples, validate on 23963 samples\n",
      "Epoch 1/200\n",
      "71888/71888 [==============================] - 39s 549us/step - loss: 0.0711 - val_loss: 0.0551\n",
      "Epoch 2/200\n",
      "71888/71888 [==============================] - 27s 381us/step - loss: 0.0579 - val_loss: 0.0564\n",
      "Epoch 3/200\n",
      "71888/71888 [==============================] - 28s 389us/step - loss: 0.0557 - val_loss: 0.0552\n",
      "Epoch 4/200\n",
      "71888/71888 [==============================] - 28s 396us/step - loss: 0.0539 - val_loss: 0.0544\n",
      "Epoch 5/200\n",
      "71888/71888 [==============================] - 29s 409us/step - loss: 0.0531 - val_loss: 0.0540\n",
      "Epoch 6/200\n",
      "71888/71888 [==============================] - 27s 372us/step - loss: 0.0525 - val_loss: 0.0526\n",
      "Epoch 7/200\n",
      "71888/71888 [==============================] - 26s 357us/step - loss: 0.0518 - val_loss: 0.0553\n",
      "Epoch 8/200\n",
      "71888/71888 [==============================] - 28s 392us/step - loss: 0.0513 - val_loss: 0.0524\n",
      "Epoch 9/200\n",
      "71888/71888 [==============================] - 26s 358us/step - loss: 0.0510 - val_loss: 0.0549\n",
      "Epoch 10/200\n",
      "71888/71888 [==============================] - 25s 352us/step - loss: 0.0501 - val_loss: 0.0530\n",
      "Epoch 11/200\n",
      "71888/71888 [==============================] - 27s 376us/step - loss: 0.0496 - val_loss: 0.0520\n",
      "Epoch 12/200\n",
      "71888/71888 [==============================] - 26s 368us/step - loss: 0.0492 - val_loss: 0.0521\n",
      "Epoch 13/200\n",
      "71888/71888 [==============================] - 28s 386us/step - loss: 0.0491 - val_loss: 0.0554\n",
      "Epoch 14/200\n",
      "71888/71888 [==============================] - 26s 364us/step - loss: 0.0492 - val_loss: 0.0536\n",
      "Epoch 15/200\n",
      "71888/71888 [==============================] - 28s 395us/step - loss: 0.0484 - val_loss: 0.0513\n",
      "Epoch 16/200\n",
      "71888/71888 [==============================] - 25s 354us/step - loss: 0.0481 - val_loss: 0.0526\n",
      "Epoch 17/200\n",
      "71888/71888 [==============================] - 25s 345us/step - loss: 0.0480 - val_loss: 0.0540\n",
      "Epoch 18/200\n",
      "71888/71888 [==============================] - 26s 360us/step - loss: 0.0474 - val_loss: 0.0556\n",
      "Epoch 19/200\n",
      "71888/71888 [==============================] - 28s 388us/step - loss: 0.0474 - val_loss: 0.0537\n",
      "Epoch 20/200\n",
      "71888/71888 [==============================] - 25s 348us/step - loss: 0.0470 - val_loss: 0.0584\n",
      "Epoch 21/200\n",
      "71888/71888 [==============================] - 26s 361us/step - loss: 0.0468 - val_loss: 0.0566\n",
      "Epoch 22/200\n",
      "71888/71888 [==============================] - 26s 367us/step - loss: 0.0466 - val_loss: 0.0547\n",
      "Epoch 23/200\n",
      "71888/71888 [==============================] - 27s 371us/step - loss: 0.0464 - val_loss: 0.0542\n",
      "Epoch 24/200\n",
      "71888/71888 [==============================] - 25s 345us/step - loss: 0.0460 - val_loss: 0.0528\n",
      "Epoch 25/200\n",
      "71888/71888 [==============================] - 25s 347us/step - loss: 0.0458 - val_loss: 0.0553\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25f7c6d5a58>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_y, \n",
    "          epochs=200,\n",
    "          validation_data=(val_X, val_y), \n",
    "          verbose=True, \n",
    "          callbacks=[\n",
    "              ModelCheckpoint('model-conv.h5', save_best_only=True),\n",
    "              EarlyStopping(patience=10),\n",
    "          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model-conv.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23963/23963 [==============================] - 2s 74us/step\n",
      "toxic\n",
      "loss:  0.107912785592\n",
      "[[ 0.98965454  0.01034546]\n",
      " [ 0.3301601   0.6698399 ]]\n",
      "severe_toxic\n",
      "loss:  0.0261312943296\n",
      "[[  9.99957847e-01   4.21531847e-05]\n",
      " [  9.95833333e-01   4.16666667e-03]]\n",
      "obscene\n",
      "loss:  0.0616836553112\n",
      "[[ 0.99387287  0.00612713]\n",
      " [ 0.32654659  0.67345341]]\n",
      "threat\n",
      "loss:  0.0122763452377\n",
      "[[ 1.  0.]\n",
      " [ 1.  0.]]\n",
      "insult\n",
      "loss:  0.0746462835164\n",
      "[[ 0.98787932  0.01212068]\n",
      " [ 0.38674497  0.61325503]]\n",
      "identity_hate\n",
      "loss:  0.0256580353838\n",
      "[[  9.99536998e-01   4.63001936e-04]\n",
      " [  8.09756098e-01   1.90243902e-01]]\n",
      "Total loss:  0.0513847332284\n"
     ]
    }
   ],
   "source": [
    "val_result(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_loss(y_true, y_pred):\n",
    "    eps = 1e-10\n",
    "    toxic_pos_weight         = 1.0\n",
    "    toxic_neg_weight         = 1.0\n",
    "    severe_toxic_pos_weight  = 1.5\n",
    "    severe_toxic_neg_weight  = 1.0\n",
    "    obscene_pos_weight       = 1.0\n",
    "    obscene_neg_weight       = 1.0\n",
    "    threat_pos_weight        = 1.5\n",
    "    threat_neg_weight        = 1.0\n",
    "    insult_pos_weight        = 1.0\n",
    "    insult_neg_weight        = 1.0\n",
    "    identity_hate_pos_weight = 1.5\n",
    "    identity_hate_neg_weight = 1.0\n",
    "    toxic         =         toxic_pos_weight *      y_true[:, 0]  * K.log(    y_pred[:, 0] + eps) + \\\n",
    "                            toxic_neg_weight * (1 - y_true[:, 0]) * K.log(1 - y_pred[:, 0] + eps)\n",
    "    severe_toxic  =  severe_toxic_pos_weight *      y_true[:, 1]  * K.log(    y_pred[:, 1] + eps) + \\\n",
    "                     severe_toxic_neg_weight * (1 - y_true[:, 1]) * K.log(1 - y_pred[:, 1] + eps)\n",
    "    obscene       =       obscene_pos_weight *      y_true[:, 2]  * K.log(    y_pred[:, 2] + eps) + \\\n",
    "                          obscene_neg_weight * (1 - y_true[:, 2]) * K.log(1 - y_pred[:, 2] + eps)\n",
    "    threat        =        threat_pos_weight *      y_true[:, 3]  * K.log(    y_pred[:, 3] + eps) + \\\n",
    "                           threat_neg_weight * (1 - y_true[:, 3]) * K.log(1 - y_pred[:, 3] + eps)\n",
    "    insult        =        insult_pos_weight *      y_true[:, 4]  * K.log(    y_pred[:, 4] + eps) + \\\n",
    "                           insult_neg_weight * (1 - y_true[:, 4]) * K.log(1 - y_pred[:, 4] + eps)\n",
    "    identity_hate = identity_hate_pos_weight *      y_true[:, 5]  * K.log(    y_pred[:, 5] + eps) + \\\n",
    "                    identity_hate_neg_weight * (1 - y_true[:, 5]) * K.log(1 - y_pred[:, 5] + eps)\n",
    "    return - (toxic + severe_toxic + obscene + threat + insult + identity_hate) / 6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 100)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 100, 100)     72162000    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_3 (Conv1D)               (None, 100, 50)      5050        embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_4 (Conv1D)               (None, 99, 50)       10050       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_3 (GlobalM (None, 50)           0           conv1d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_4 (GlobalM (None, 50)           0           conv1d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 50)           0           global_max_pooling1d_3[0][0]     \n",
      "                                                                 global_max_pooling1d_4[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_3 (Dropout)             (None, 50)           0           add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 100)          5100        dropout_3[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dropout_4 (Dropout)             (None, 100)          0           dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 6)            606         dropout_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 72,182,806\n",
      "Trainable params: 20,806\n",
      "Non-trainable params: 72,162,000\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('nadam', custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71888 samples, validate on 23963 samples\n",
      "Epoch 1/200\n",
      "71888/71888 [==============================] - 44s 612us/step - loss: 0.0726 - val_loss: 0.0584\n",
      "Epoch 2/200\n",
      "71888/71888 [==============================] - 36s 502us/step - loss: 0.0602 - val_loss: 0.0566\n",
      "Epoch 3/200\n",
      "71888/71888 [==============================] - 38s 525us/step - loss: 0.0577 - val_loss: 0.0547\n",
      "Epoch 4/200\n",
      "71888/71888 [==============================] - 36s 495us/step - loss: 0.0562 - val_loss: 0.0556\n",
      "Epoch 5/200\n",
      "71888/71888 [==============================] - 34s 474us/step - loss: 0.0556 - val_loss: 0.0548\n",
      "Epoch 6/200\n",
      "71888/71888 [==============================] - 36s 500us/step - loss: 0.0549 - val_loss: 0.0545\n",
      "Epoch 7/200\n",
      "71888/71888 [==============================] - 34s 468us/step - loss: 0.0533 - val_loss: 0.0549\n",
      "Epoch 8/200\n",
      "71888/71888 [==============================] - 34s 467us/step - loss: 0.0533 - val_loss: 0.0556\n",
      "Epoch 9/200\n",
      "71888/71888 [==============================] - 37s 510us/step - loss: 0.0529 - val_loss: 0.0542\n",
      "Epoch 10/200\n",
      "71888/71888 [==============================] - 38s 526us/step - loss: 0.0518 - val_loss: 0.0537\n",
      "Epoch 11/200\n",
      "71888/71888 [==============================] - 38s 525us/step - loss: 0.0515 - val_loss: 0.0544\n",
      "Epoch 12/200\n",
      "71888/71888 [==============================] - 38s 530us/step - loss: 0.0510 - val_loss: 0.0545\n",
      "Epoch 13/200\n",
      "71888/71888 [==============================] - 34s 473us/step - loss: 0.0507 - val_loss: 0.0554\n",
      "Epoch 14/200\n",
      "71888/71888 [==============================] - 34s 473us/step - loss: 0.0499 - val_loss: 0.0557\n",
      "Epoch 15/200\n",
      "71888/71888 [==============================] - 35s 487us/step - loss: 0.0499 - val_loss: 0.0557\n",
      "Epoch 16/200\n",
      "71888/71888 [==============================] - 34s 475us/step - loss: 0.0496 - val_loss: 0.0545\n",
      "Epoch 17/200\n",
      "71888/71888 [==============================] - 33s 462us/step - loss: 0.0490 - val_loss: 0.0563\n",
      "Epoch 18/200\n",
      "71888/71888 [==============================] - 33s 462us/step - loss: 0.0488 - val_loss: 0.0555\n",
      "Epoch 19/200\n",
      "71888/71888 [==============================] - 33s 461us/step - loss: 0.0481 - val_loss: 0.0580\n",
      "Epoch 20/200\n",
      "71888/71888 [==============================] - 35s 480us/step - loss: 0.0483 - val_loss: 0.0560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25f7c8a4a90>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_y, \n",
    "          batch_size=32,\n",
    "          epochs=200,\n",
    "          validation_data=(val_X, val_y), \n",
    "          verbose=True, \n",
    "          callbacks=[\n",
    "              ModelCheckpoint('model-conv.h5', save_best_only=True),\n",
    "              EarlyStopping(patience=10),\n",
    "          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23963/23963 [==============================] - 2s 78us/step\n",
      "toxic\n",
      "loss:  0.106000858547\n",
      "[[ 0.9826344   0.0173656 ]\n",
      " [ 0.26006058  0.73993942]]\n",
      "severe_toxic\n",
      "loss:  0.0250515600914\n",
      "[[  9.99957847e-01   4.21531847e-05]\n",
      " [  1.00000000e+00   0.00000000e+00]]\n",
      "obscene\n",
      "loss:  0.0574519323598\n",
      "[[ 0.99030239  0.00969761]\n",
      " [ 0.23884103  0.76115897]]\n",
      "threat\n",
      "loss:  0.012510347684\n",
      "[[  9.99916272e-01   8.37275506e-05]\n",
      " [  9.86842105e-01   1.31578947e-02]]\n",
      "insult\n",
      "loss:  0.0741042633897\n",
      "[[ 0.98831848  0.01168152]\n",
      " [ 0.375       0.625     ]]\n",
      "identity_hate\n",
      "loss:  0.0254444991554\n",
      "[[  9.99831636e-01   1.68364340e-04]\n",
      " [  8.39024390e-01   1.60975610e-01]]\n",
      "Total loss:  0.0500939102045\n"
     ]
    }
   ],
   "source": [
    "model.load_weights('model-conv.h5')\n",
    "\n",
    "val_result(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    return Sequential([\n",
    "        InputLayer(input_shape=(MAXLEN,), dtype='int32'),\n",
    "        get_embedding(),\n",
    "        Bidirectional(LSTM(50, return_sequences=True)),\n",
    "        GlobalMaxPool1D(),\n",
    "        Dropout(0.3),\n",
    "        Dense(50, activation='relu'),\n",
    "        Dropout(0.3),\n",
    "        Dense(6, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 100, 100)          72162000  \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 100, 100)          60400     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_5 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 6)                 306       \n",
      "=================================================================\n",
      "Total params: 72,227,756\n",
      "Trainable params: 65,756\n",
      "Non-trainable params: 72,162,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile('nadam', 'binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71888 samples, validate on 23963 samples\n",
      "Epoch 1/5\n",
      "71888/71888 [==============================] - 1447s 20ms/step - loss: 0.0646 - val_loss: 0.0510\n",
      "Epoch 2/5\n",
      "71888/71888 [==============================] - 1448s 20ms/step - loss: 0.0517 - val_loss: 0.0501\n",
      "Epoch 3/5\n",
      "71888/71888 [==============================] - 1454s 20ms/step - loss: 0.0492 - val_loss: 0.0487\n",
      "Epoch 4/5\n",
      "71888/71888 [==============================] - 1410s 20ms/step - loss: 0.0473 - val_loss: 0.0477\n",
      "Epoch 5/5\n",
      "71888/71888 [==============================] - 1430s 20ms/step - loss: 0.0455 - val_loss: 0.0470\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25f7cc084a8>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_y, \n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(val_X, val_y), \n",
    "          verbose=True, \n",
    "          callbacks=[\n",
    "              ModelCheckpoint('model-rnn.h5', save_best_only=True),\n",
    "          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model-rnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23963/23963 [==============================] - 96s 4ms/step\n",
      "toxic\n",
      "loss:  0.096308736771\n",
      "[[ 0.98425088  0.01574912]\n",
      " [ 0.24145392  0.75854608]]\n",
      "severe_toxic\n",
      "loss:  0.0236998996393\n",
      "[[  9.99536315e-01   4.63685031e-04]\n",
      " [  9.29166667e-01   7.08333333e-02]]\n",
      "obscene\n",
      "loss:  0.0546462138876\n",
      "[[ 0.98994975  0.01005025]\n",
      " [ 0.19655442  0.80344558]]\n",
      "threat\n",
      "loss:  0.0119974299074\n",
      "[[ 1.  0.]\n",
      " [ 1.  0.]]\n",
      "insult\n",
      "loss:  0.0703548359798\n",
      "[[ 0.98634228  0.01365772]\n",
      " [ 0.32885906  0.67114094]]\n",
      "identity_hate\n",
      "loss:  0.0248463182712\n",
      "[[  9.99536998e-01   4.63001936e-04]\n",
      " [  8.48780488e-01   1.51219512e-01]]\n",
      "Total loss:  0.0469755724094\n"
     ]
    }
   ],
   "source": [
    "val_result(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226998/226998 [==============================] - 916s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "test_prediction = model.predict(test_X, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('input/sample_submission.csv')\n",
    "submission[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']] = test_prediction\n",
    "submission.to_csv('output.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.compile('nadam', custom_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 71888 samples, validate on 23963 samples\n",
      "Epoch 1/5\n",
      "71888/71888 [==============================] - 1341s 19ms/step - loss: 0.0698 - val_loss: 0.0544\n",
      "Epoch 2/5\n",
      "71888/71888 [==============================] - 1267s 18ms/step - loss: 0.0556 - val_loss: 0.0527\n",
      "Epoch 3/5\n",
      "71888/71888 [==============================] - 1208s 17ms/step - loss: 0.0529 - val_loss: 0.0524\n",
      "Epoch 4/5\n",
      "71888/71888 [==============================] - 1185s 16ms/step - loss: 0.0515 - val_loss: 0.0499\n",
      "Epoch 5/5\n",
      "71888/71888 [==============================] - 1164s 16ms/step - loss: 0.0496 - val_loss: 0.0493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25f7d0b4da0>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_y, \n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          validation_data=(val_X, val_y), \n",
    "          verbose=True, \n",
    "          callbacks=[\n",
    "              ModelCheckpoint('model-rnn.h5', save_best_only=True),\n",
    "          ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('model-rnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23963/23963 [==============================] - 84s 4ms/step\n",
      "toxic\n",
      "loss:  0.094682783082\n",
      "[[ 0.98859228  0.01140772]\n",
      " [ 0.26568585  0.73431415]]\n",
      "severe_toxic\n",
      "loss:  0.0244011336883\n",
      "[[  9.99241243e-01   7.58757324e-04]\n",
      " [  8.79166667e-01   1.20833333e-01]]\n",
      "obscene\n",
      "loss:  0.054559119224\n",
      "[[ 0.99175703  0.00824297]\n",
      " [ 0.22787784  0.77212216]]\n",
      "threat\n",
      "loss:  0.0127841209976\n",
      "[[ 1.  0.]\n",
      " [ 1.  0.]]\n",
      "insult\n",
      "loss:  0.0683301590677\n",
      "[[ 0.98585921  0.01414079]\n",
      " [ 0.31040268  0.68959732]]\n",
      "identity_hate\n",
      "loss:  0.025187609588\n",
      "[[ 0.99844263  0.00155737]\n",
      " [ 0.68292683  0.31707317]]\n",
      "Total loss:  0.046657487608\n"
     ]
    }
   ],
   "source": [
    "val_result(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "226998/226998 [==============================] - 820s 4ms/step\n"
     ]
    }
   ],
   "source": [
    "test_prediction = model.predict(test_X, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('input/sample_submission.csv')\n",
    "submission[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']] = test_prediction\n",
    "submission.to_csv('output.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kaggle-toxic]",
   "language": "python",
   "name": "conda-env-kaggle-toxic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
