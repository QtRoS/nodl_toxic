{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from itertools import chain\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "from crossvalidation import multilabel_label_combinations, multilabel_cross_validation\n",
    "from transform_pipeline import TransformPipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import editdistance\n",
    "from functools import lru_cache\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from scipy.sparse import csr_matrix\n",
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "from math import ceil\n",
    "\n",
    "from nltk.stem.snowball import EnglishStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain = pd.read_csv(\"input/train.csv\")\n",
    "dftrain['comment_text'] = dftrain['comment_text'].apply(str)\n",
    "dftest = pd.read_csv(\"input/test.csv\")\n",
    "dftest['comment_text'] = dftest['comment_text'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    delimeter = \"([?\\\\/.,`~!@#4%^&*-+\\[\\]{}<>'\\\"]*[ \\s\\n\\t\\r]+)\"\n",
    "    tokens = re.split(delimeter, text + \" \")\n",
    "    stripped_tokens = map(str.strip, tokens)\n",
    "    noempty_tokens = filter(bool, stripped_tokens)\n",
    "    return list(noempty_tokens)\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.strip()\n",
    "    text = text.replace('\\\\n', ' ').replace('\\\\r', ' ').replace('\\\\t', ' ')\n",
    "    text = text.lower()\n",
    "    return \" \".join(tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain['preprocessed_text'] = dftrain['comment_text'].apply(preprocess)\n",
    "dftest['preprocessed_text'] = dftest['comment_text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>preprocessed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nonsense ? kiss off , geek . what i said is tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" please do not vandalize pages , as you did w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" \"\"points of interest \"\" i removed the \"\"poin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>asking some his nationality is a racial offenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>the reader here is not going by my say so for ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  toxic  \\\n",
       "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
       "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
       "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
       "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4  79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                   preprocessed_text  \n",
       "0  nonsense ? kiss off , geek . what i said is tr...  \n",
       "1  \" please do not vandalize pages , as you did w...  \n",
       "2  \" \"\"points of interest \"\" i removed the \"\"poin...  \n",
       "3  asking some his nationality is a racial offenc...  \n",
       "4  the reader here is not going by my say so for ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.array(dftrain[targets])\n",
    "labelcombination_y = np.zeros([len(dftrain)], dtype=np.int)\n",
    "for i, row in enumerate(multilabel_label_combinations(y, 2)):\n",
    "    labelcombination_y[np.all(y == row, axis=1)] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_texts = np.array(dftrain['preprocessed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_idx, val_idx, _, _ = train_test_split(np.arange(len(dftrain), dtype=np.int), \n",
    "                                            labelcombination_y, \n",
    "                                            stratify=labelcombination_y,\n",
    "                                            random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def toptoxic_words(texts, y):\n",
    "    clf = Pipeline([\n",
    "        ('vec', TfidfVectorizer(tokenizer=lambda s: s.split(' '), binary=True, min_df=5, max_df=0.9)),\n",
    "        ('clf', LogisticRegression(penalty='l1'))    \n",
    "    ])\n",
    "    clf.fit(texts, (1 * (y.sum(axis=1) > 1)))\n",
    "    weights = np.abs(clf.steps[1][1].coef_[0])\n",
    "    topwords_idx = weights > 0.025\n",
    "    topwords = np.array(clf.steps[0][1].get_feature_names())[topwords_idx]\n",
    "    return topwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "topwords = toptoxic_words(preprocessed_texts[train_idx], y[train_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_similarity(word1, word2):\n",
    "    meanlen = (len(word1) + len(word2)) / 2\n",
    "    return 1 - editdistance.eval(word1, word2) / meanlen\n",
    "\n",
    "def sentence_word_similarity_features(texts, feature_words):\n",
    "    def _chunks(l, n):\n",
    "        for i in range(0, len(l), n):\n",
    "            yield l[i:i + n]\n",
    "            \n",
    "    def _text_similarity(words, wordset, similarities, text):\n",
    "        text_words = set(text.split(' '))\n",
    "        idx = [words.index(word) for word in text_words if word in wordset]\n",
    "        return similarities[idx].max()\n",
    "    \n",
    "    countvec = CountVectorizer(binary=True, tokenizer=lambda s: s.split(' '))\n",
    "    counts = countvec.fit_transform(texts)\n",
    "    words = countvec.get_feature_names()\n",
    "    word_chunk_size = 128\n",
    "    similarities_progress = IntProgress(min=0, max=ceil(len(words) / word_chunk_size))\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    for i, feature_word in enumerate(tqdm(feature_words)):\n",
    "        if i == 0:\n",
    "            display(similarities_progress)\n",
    "        \n",
    "        similarities_progress.value = 0\n",
    "        similarities = np.zeros([len(words)])\n",
    "        k = 0\n",
    "        for j, words_chunk in enumerate(_chunks(words, word_chunk_size)):\n",
    "            for word in words_chunk:\n",
    "                similarities[k] = word_similarity(word, feature_word)\n",
    "                k += 1\n",
    "            similarities_progress.value += 128\n",
    "        similarities_progress.value = similarities_progress.max\n",
    "        \n",
    "        count_similarities = counts.multiply(csr_matrix(similarities))\n",
    "        feature_similarities = np.array([\n",
    "            count_similarities[j].max()\n",
    "            for j in range(len(texts))\n",
    "        ])\n",
    "        result[feature_word] = feature_similarities\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0ce7e7673fb45c3afbdcc386f4b1e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=545), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc7e6dbae142472b8699ead8e03a71c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>IntProgress</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "IntProgress(value=0, max=5229)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "topword_similarities = sentence_word_similarity_features(list(dftrain['preprocessed_text']) + list(dftest['preprocessed_text']),\n",
    "                                                         topwords)\n",
    "for key, similarities in topword_similarities.items():\n",
    "    dftrain['lev_' + key] = similarities[:len(dftrain)]\n",
    "    dftest['lev_' + key] = similarities[-len(dftest):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['lev_!',\n",
       " 'lev_!!',\n",
       " 'lev_!!!',\n",
       " 'lev_!!!!',\n",
       " 'lev_!!!!!!',\n",
       " 'lev_!!!!!!!',\n",
       " 'lev_!!!!!!!!!',\n",
       " 'lev_!!!!!!!!!!!!',\n",
       " 'lev_!\"',\n",
       " 'lev_\"',\n",
       " 'lev_\"\"fuck',\n",
       " 'lev_\"\"fucking',\n",
       " \"lev_''''''\",\n",
       " 'lev_(',\n",
       " 'lev_(talk)',\n",
       " 'lev_(utc)',\n",
       " 'lev_)',\n",
       " 'lev_***',\n",
       " 'lev_-',\n",
       " 'lev_.',\n",
       " 'lev_.\"',\n",
       " 'lev_...',\n",
       " 'lev_/',\n",
       " 'lev_2008',\n",
       " 'lev_3',\n",
       " 'lev_==',\n",
       " 'lev_?',\n",
       " 'lev_???',\n",
       " 'lev_]]',\n",
       " 'lev_a',\n",
       " 'lev_about',\n",
       " 'lev_abuse',\n",
       " 'lev_account',\n",
       " 'lev_actions',\n",
       " 'lev_added',\n",
       " 'lev_admins',\n",
       " 'lev_adolf',\n",
       " 'lev_after',\n",
       " 'lev_again',\n",
       " 'lev_agree',\n",
       " \"lev_ain't\",\n",
       " 'lev_all',\n",
       " 'lev_alone',\n",
       " 'lev_also',\n",
       " 'lev_am',\n",
       " 'lev_anal',\n",
       " 'lev_and',\n",
       " 'lev_answer',\n",
       " 'lev_anti-semite',\n",
       " 'lev_anti-vandalism',\n",
       " 'lev_anus',\n",
       " 'lev_any',\n",
       " 'lev_anyway',\n",
       " 'lev_arab',\n",
       " 'lev_are',\n",
       " 'lev_arrogant',\n",
       " 'lev_arse',\n",
       " 'lev_article',\n",
       " 'lev_articles',\n",
       " 'lev_as',\n",
       " 'lev_ask',\n",
       " 'lev_ass',\n",
       " 'lev_asshole',\n",
       " 'lev_assholes',\n",
       " 'lev_at',\n",
       " 'lev_away',\n",
       " 'lev_back',\n",
       " 'lev_bag',\n",
       " 'lev_balls',\n",
       " 'lev_ban',\n",
       " 'lev_barnstar',\n",
       " 'lev_bastard',\n",
       " 'lev_bastards',\n",
       " 'lev_be',\n",
       " 'lev_been',\n",
       " 'lev_being',\n",
       " 'lev_believe',\n",
       " 'lev_best',\n",
       " 'lev_better',\n",
       " 'lev_big',\n",
       " 'lev_bit',\n",
       " 'lev_bitch',\n",
       " 'lev_bitches',\n",
       " 'lev_bitching',\n",
       " 'lev_black',\n",
       " 'lev_blood',\n",
       " 'lev_bloody',\n",
       " 'lev_boy',\n",
       " 'lev_brain',\n",
       " 'lev_brains',\n",
       " 'lev_bugger',\n",
       " 'lev_bullshit',\n",
       " 'lev_bully',\n",
       " 'lev_bum',\n",
       " 'lev_bunch',\n",
       " 'lev_but',\n",
       " 'lev_butt',\n",
       " 'lev_call',\n",
       " 'lev_calling',\n",
       " 'lev_can',\n",
       " \"lev_can't\",\n",
       " 'lev_cancer',\n",
       " 'lev_cant',\n",
       " 'lev_care',\n",
       " 'lev_cares',\n",
       " 'lev_certain',\n",
       " 'lev_chance',\n",
       " 'lev_change',\n",
       " 'lev_changed',\n",
       " 'lev_check',\n",
       " 'lev_cheers',\n",
       " 'lev_cheese',\n",
       " 'lev_child',\n",
       " 'lev_claim',\n",
       " 'lev_cock',\n",
       " 'lev_cocksucker',\n",
       " 'lev_come',\n",
       " 'lev_coming',\n",
       " 'lev_considered',\n",
       " 'lev_content',\n",
       " 'lev_correct',\n",
       " 'lev_cougar',\n",
       " 'lev_could',\n",
       " 'lev_coward',\n",
       " 'lev_cowards',\n",
       " 'lev_crap',\n",
       " 'lev_crazy',\n",
       " 'lev_cretin',\n",
       " 'lev_criminal',\n",
       " 'lev_cry',\n",
       " 'lev_cum',\n",
       " 'lev_cunt',\n",
       " 'lev_cunts',\n",
       " 'lev_current',\n",
       " 'lev_cut',\n",
       " 'lev_dammit',\n",
       " 'lev_damn',\n",
       " 'lev_dare',\n",
       " 'lev_day',\n",
       " 'lev_dead',\n",
       " 'lev_death',\n",
       " 'lev_deleted',\n",
       " 'lev_deserve',\n",
       " 'lev_destroy',\n",
       " 'lev_dick',\n",
       " 'lev_dickhead',\n",
       " 'lev_dicks',\n",
       " 'lev_did',\n",
       " \"lev_didn't\",\n",
       " 'lev_die',\n",
       " 'lev_dipshit',\n",
       " 'lev_dirty',\n",
       " 'lev_discussion',\n",
       " 'lev_disgrace',\n",
       " 'lev_disgusting',\n",
       " 'lev_dog',\n",
       " \"lev_don't\",\n",
       " 'lev_dont',\n",
       " 'lev_doosh',\n",
       " 'lev_douche',\n",
       " 'lev_douchebag',\n",
       " 'lev_down',\n",
       " 'lev_drop',\n",
       " 'lev_dumb',\n",
       " 'lev_dumbass',\n",
       " 'lev_dumbest',\n",
       " 'lev_e',\n",
       " 'lev_eat',\n",
       " 'lev_edit',\n",
       " 'lev_editing',\n",
       " 'lev_edits',\n",
       " 'lev_encyclopedia',\n",
       " 'lev_even',\n",
       " 'lev_ever',\n",
       " 'lev_every',\n",
       " 'lev_evidence',\n",
       " 'lev_evil',\n",
       " 'lev_example',\n",
       " 'lev_f',\n",
       " 'lev_f**k',\n",
       " 'lev_f*cking',\n",
       " 'lev_face',\n",
       " 'lev_fag',\n",
       " 'lev_faggot',\n",
       " 'lev_faggots',\n",
       " 'lev_fags',\n",
       " 'lev_family',\n",
       " 'lev_fart',\n",
       " 'lev_fat',\n",
       " 'lev_fatty',\n",
       " 'lev_fcuking',\n",
       " 'lev_feed',\n",
       " 'lev_feel',\n",
       " 'lev_finally',\n",
       " 'lev_find',\n",
       " 'lev_fire',\n",
       " 'lev_first',\n",
       " 'lev_fool',\n",
       " 'lev_foolish',\n",
       " 'lev_for',\n",
       " 'lev_forever',\n",
       " 'lev_fraud',\n",
       " 'lev_freak',\n",
       " 'lev_freaking',\n",
       " 'lev_friend',\n",
       " 'lev_from',\n",
       " 'lev_fu',\n",
       " 'lev_fuck',\n",
       " 'lev_fucked',\n",
       " 'lev_fucken',\n",
       " 'lev_fucker',\n",
       " 'lev_fuckers',\n",
       " 'lev_fuckhead',\n",
       " 'lev_fuckin',\n",
       " 'lev_fucking',\n",
       " 'lev_fucks',\n",
       " 'lev_fucktard',\n",
       " 'lev_fuckwit',\n",
       " 'lev_fuk',\n",
       " 'lev_g',\n",
       " 'lev_gay',\n",
       " 'lev_get',\n",
       " 'lev_given',\n",
       " 'lev_go',\n",
       " 'lev_goddamn',\n",
       " 'lev_going',\n",
       " 'lev_gonna',\n",
       " 'lev_good',\n",
       " 'lev_graceful',\n",
       " 'lev_guess',\n",
       " 'lev_guts',\n",
       " 'lev_guy',\n",
       " 'lev_guys',\n",
       " 'lev_ha',\n",
       " 'lev_hahaha',\n",
       " 'lev_hang',\n",
       " 'lev_happy',\n",
       " 'lev_harassment',\n",
       " 'lev_harder',\n",
       " 'lev_has',\n",
       " 'lev_hate',\n",
       " 'lev_have',\n",
       " \"lev_haven't\",\n",
       " 'lev_he',\n",
       " \"lev_he's\",\n",
       " 'lev_head',\n",
       " 'lev_hell',\n",
       " 'lev_help',\n",
       " 'lev_here',\n",
       " 'lev_herpes',\n",
       " 'lev_hes',\n",
       " 'lev_hey',\n",
       " 'lev_hi',\n",
       " 'lev_his',\n",
       " 'lev_hitler',\n",
       " 'lev_hole',\n",
       " 'lev_homo',\n",
       " 'lev_homosexual',\n",
       " 'lev_homosexuals',\n",
       " 'lev_how',\n",
       " 'lev_however',\n",
       " 'lev_hungry',\n",
       " 'lev_hypocrite',\n",
       " \"lev_i've\",\n",
       " 'lev_idiocy',\n",
       " 'lev_idiot',\n",
       " 'lev_idiotic',\n",
       " 'lev_idiots',\n",
       " 'lev_if',\n",
       " 'lev_ignorant',\n",
       " 'lev_illiterate',\n",
       " 'lev_imbecile',\n",
       " 'lev_in',\n",
       " 'lev_information',\n",
       " 'lev_insane',\n",
       " 'lev_insult',\n",
       " 'lev_interested',\n",
       " 'lev_involved',\n",
       " 'lev_issue',\n",
       " 'lev_it',\n",
       " 'lev_jack',\n",
       " 'lev_jackass',\n",
       " 'lev_jeff',\n",
       " 'lev_jerk',\n",
       " 'lev_jerkoff',\n",
       " 'lev_jerks',\n",
       " 'lev_jew',\n",
       " 'lev_jews',\n",
       " 'lev_job',\n",
       " 'lev_just',\n",
       " 'lev_k',\n",
       " 'lev_kick',\n",
       " 'lev_kill',\n",
       " 'lev_kleargear',\n",
       " 'lev_knob',\n",
       " 'lev_know',\n",
       " 'lev_known',\n",
       " 'lev_language',\n",
       " 'lev_last',\n",
       " 'lev_lazy',\n",
       " 'lev_leave',\n",
       " 'lev_liar',\n",
       " 'lev_life',\n",
       " 'lev_like',\n",
       " 'lev_likes',\n",
       " 'lev_link',\n",
       " 'lev_listen',\n",
       " 'lev_little',\n",
       " 'lev_long',\n",
       " 'lev_looks',\n",
       " 'lev_looser',\n",
       " 'lev_loser',\n",
       " 'lev_losers',\n",
       " 'lev_lot',\n",
       " 'lev_love',\n",
       " 'lev_loves',\n",
       " 'lev_lulz',\n",
       " 'lev_made',\n",
       " 'lev_maggot',\n",
       " 'lev_mama',\n",
       " 'lev_man',\n",
       " 'lev_may',\n",
       " 'lev_maybe',\n",
       " 'lev_me',\n",
       " 'lev_men',\n",
       " 'lev_mentioned',\n",
       " 'lev_might',\n",
       " 'lev_mitt',\n",
       " 'lev_mom',\n",
       " 'lev_mommy',\n",
       " 'lev_monkey',\n",
       " 'lev_more',\n",
       " 'lev_moron',\n",
       " 'lev_morons',\n",
       " 'lev_most',\n",
       " 'lev_mother',\n",
       " 'lev_motherfucker',\n",
       " 'lev_motherfuckers',\n",
       " 'lev_motherfucking',\n",
       " 'lev_mouth',\n",
       " 'lev_much',\n",
       " 'lev_murder',\n",
       " 'lev_must',\n",
       " 'lev_my',\n",
       " 'lev_n',\n",
       " 'lev_name',\n",
       " 'lev_nasty',\n",
       " 'lev_nazi',\n",
       " 'lev_needs',\n",
       " 'lev_nerd',\n",
       " 'lev_nerds',\n",
       " 'lev_new',\n",
       " 'lev_nigga',\n",
       " 'lev_nigger',\n",
       " 'lev_niggers',\n",
       " 'lev_not',\n",
       " 'lev_note',\n",
       " 'lev_nothing',\n",
       " 'lev_nuts',\n",
       " 'lev_obviously',\n",
       " 'lev_of',\n",
       " 'lev_off',\n",
       " 'lev_oh',\n",
       " 'lev_ok',\n",
       " 'lev_old',\n",
       " 'lev_on',\n",
       " 'lev_one',\n",
       " 'lev_only',\n",
       " 'lev_or',\n",
       " 'lev_original',\n",
       " 'lev_out',\n",
       " 'lev_page',\n",
       " 'lev_paid',\n",
       " 'lev_part',\n",
       " 'lev_pathetic',\n",
       " 'lev_pee',\n",
       " 'lev_penis',\n",
       " 'lev_people',\n",
       " 'lev_personally',\n",
       " 'lev_pervert',\n",
       " 'lev_piece',\n",
       " 'lev_pig',\n",
       " 'lev_pigs',\n",
       " 'lev_piss',\n",
       " 'lev_please',\n",
       " 'lev_point',\n",
       " 'lev_poop',\n",
       " 'lev_possible',\n",
       " 'lev_pov',\n",
       " 'lev_prick',\n",
       " 'lev_prostitute',\n",
       " 'lev_psycho',\n",
       " 'lev_pussies',\n",
       " 'lev_pussy',\n",
       " 'lev_put',\n",
       " 'lev_quit',\n",
       " 'lev_r',\n",
       " 'lev_racist',\n",
       " 'lev_rape',\n",
       " 'lev_raped',\n",
       " 'lev_raping',\n",
       " 'lev_rat',\n",
       " 'lev_rather',\n",
       " 'lev_read',\n",
       " 'lev_really',\n",
       " 'lev_reason',\n",
       " 'lev_redirect',\n",
       " 'lev_reference',\n",
       " 'lev_regarding',\n",
       " 'lev_regards',\n",
       " 'lev_remove',\n",
       " 'lev_removed',\n",
       " 'lev_reply',\n",
       " 'lev_report',\n",
       " 'lev_request',\n",
       " 'lev_response',\n",
       " 'lev_retard',\n",
       " 'lev_retarded',\n",
       " 'lev_retards',\n",
       " 'lev_revert',\n",
       " 'lev_reverted',\n",
       " 'lev_right',\n",
       " 'lev_s',\n",
       " 'lev_sack',\n",
       " 'lev_sad',\n",
       " 'lev_same',\n",
       " 'lev_save',\n",
       " 'lev_saying',\n",
       " 'lev_scared',\n",
       " 'lev_screwing',\n",
       " 'lev_scum',\n",
       " 'lev_scumbag',\n",
       " 'lev_section',\n",
       " 'lev_see',\n",
       " 'lev_seems',\n",
       " 'lev_send',\n",
       " 'lev_sex',\n",
       " 'lev_shameless',\n",
       " \"lev_she's\",\n",
       " 'lev_shit',\n",
       " 'lev_shoot',\n",
       " 'lev_shot',\n",
       " 'lev_shut',\n",
       " 'lev_sick',\n",
       " 'lev_sir',\n",
       " 'lev_sissy',\n",
       " 'lev_sit',\n",
       " 'lev_slave',\n",
       " 'lev_sleep',\n",
       " 'lev_slick',\n",
       " 'lev_slut',\n",
       " 'lev_smell',\n",
       " 'lev_smoker',\n",
       " 'lev_so',\n",
       " 'lev_sockpuppet',\n",
       " 'lev_something',\n",
       " 'lev_sorry',\n",
       " 'lev_source',\n",
       " 'lev_sources',\n",
       " 'lev_speech',\n",
       " 'lev_start',\n",
       " 'lev_stinky',\n",
       " 'lev_stop',\n",
       " 'lev_stupid',\n",
       " 'lev_stupidity',\n",
       " 'lev_suck',\n",
       " 'lev_sucker',\n",
       " 'lev_sucking',\n",
       " 'lev_sucks',\n",
       " 'lev_sure',\n",
       " 'lev_talk',\n",
       " 'lev_talking',\n",
       " 'lev_terrible',\n",
       " 'lev_testicles',\n",
       " 'lev_than',\n",
       " 'lev_thank',\n",
       " 'lev_thanks',\n",
       " 'lev_that',\n",
       " 'lev_thats',\n",
       " 'lev_the',\n",
       " 'lev_there',\n",
       " 'lev_these',\n",
       " 'lev_thief',\n",
       " 'lev_think',\n",
       " 'lev_this',\n",
       " 'lev_tits',\n",
       " 'lev_to',\n",
       " 'lev_toilet',\n",
       " 'lev_touch',\n",
       " 'lev_troll',\n",
       " 'lev_trying',\n",
       " 'lev_twat',\n",
       " 'lev_twit',\n",
       " 'lev_u',\n",
       " 'lev_ugly',\n",
       " 'lev_understand',\n",
       " 'lev_uneducated',\n",
       " 'lev_unsigned',\n",
       " 'lev_up',\n",
       " 'lev_ur',\n",
       " 'lev_us',\n",
       " 'lev_used',\n",
       " 'lev_useless',\n",
       " 'lev_users',\n",
       " 'lev_using',\n",
       " 'lev_vagina',\n",
       " 'lev_vandalise',\n",
       " 'lev_very',\n",
       " 'lev_wanker',\n",
       " 'lev_wankers',\n",
       " 'lev_wanna',\n",
       " 'lev_want',\n",
       " 'lev_warning',\n",
       " 'lev_was',\n",
       " 'lev_watch',\n",
       " 'lev_way',\n",
       " 'lev_we',\n",
       " 'lev_welcome',\n",
       " 'lev_well',\n",
       " 'lev_what',\n",
       " \"lev_what's\",\n",
       " 'lev_whats',\n",
       " 'lev_which',\n",
       " 'lev_white',\n",
       " 'lev_who',\n",
       " 'lev_whore',\n",
       " 'lev_wikipedia',\n",
       " 'lev_will',\n",
       " 'lev_wipe',\n",
       " 'lev_work',\n",
       " 'lev_working',\n",
       " 'lev_world',\n",
       " 'lev_would',\n",
       " 'lev_wrong',\n",
       " 'lev_wtf',\n",
       " 'lev_y',\n",
       " 'lev_yeah',\n",
       " 'lev_yes',\n",
       " 'lev_yo',\n",
       " 'lev_you',\n",
       " \"lev_you're\",\n",
       " 'lev_your',\n",
       " 'lev_youre',\n",
       " 'lev_}}',\n",
       " 'lev_•']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "levenshtein_features = ['lev_' + word for word in topwords]\n",
    "levenshtein_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>preprocessed_text</th>\n",
       "      <th>lev_!</th>\n",
       "      <th>...</th>\n",
       "      <th>lev_y</th>\n",
       "      <th>lev_yeah</th>\n",
       "      <th>lev_yes</th>\n",
       "      <th>lev_yo</th>\n",
       "      <th>lev_you</th>\n",
       "      <th>lev_you're</th>\n",
       "      <th>lev_your</th>\n",
       "      <th>lev_youre</th>\n",
       "      <th>lev_}}</th>\n",
       "      <th>lev_•</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22256635</td>\n",
       "      <td>Nonsense?  kiss off, geek. what I said is true...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>nonsense ? kiss off , geek . what i said is tr...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27450690</td>\n",
       "      <td>\"\\n\\n Please do not vandalize pages, as you di...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" please do not vandalize pages , as you did w...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>54037174</td>\n",
       "      <td>\"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>\" \"\"points of interest \"\" i removed the \"\"poin...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77493077</td>\n",
       "      <td>Asking some his nationality is a Racial offenc...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>asking some his nationality is a racial offenc...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>79357270</td>\n",
       "      <td>The reader here is not going by my say so for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>the reader here is not going by my say so for ...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.714286</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 554 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                       comment_text  toxic  \\\n",
       "0  22256635  Nonsense?  kiss off, geek. what I said is true...      1   \n",
       "1  27450690  \"\\n\\n Please do not vandalize pages, as you di...      0   \n",
       "2  54037174  \"\\n\\n \"\"Points of interest\"\" \\n\\nI removed the...      0   \n",
       "3  77493077  Asking some his nationality is a Racial offenc...      0   \n",
       "4  79357270  The reader here is not going by my say so for ...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "0             0        0       0       0              0   \n",
       "1             0        0       0       0              0   \n",
       "2             0        0       0       0              0   \n",
       "3             0        0       0       0              0   \n",
       "4             0        0       0       0              0   \n",
       "\n",
       "                                   preprocessed_text  lev_!  ...       lev_y  \\\n",
       "0  nonsense ? kiss off , geek . what i said is tr...    0.0  ...    0.000000   \n",
       "1  \" please do not vandalize pages , as you did w...    0.0  ...    0.000000   \n",
       "2  \" \"\"points of interest \"\" i removed the \"\"poin...    0.0  ...    0.333333   \n",
       "3  asking some his nationality is a racial offenc...    0.0  ...    0.000000   \n",
       "4  the reader here is not going by my say so for ...    0.0  ...    0.333333   \n",
       "\n",
       "   lev_yeah   lev_yes    lev_yo   lev_you  lev_you're  lev_your  lev_youre  \\\n",
       "0      0.25  0.200000  0.333333  0.714286    0.600000  1.000000   0.777778   \n",
       "1      0.25  0.333333  0.600000  1.000000    0.333333  0.714286   0.500000   \n",
       "2      0.50  0.333333  0.600000  1.000000    0.600000  1.000000   0.777778   \n",
       "3      0.25  0.333333  0.333333  0.714286    0.600000  1.000000   0.777778   \n",
       "4      0.25  0.333333  0.600000  1.000000    0.384615  0.714286   0.500000   \n",
       "\n",
       "   lev_}}  lev_•  \n",
       "0     0.0    0.0  \n",
       "1     0.0    0.0  \n",
       "2     1.0    0.0  \n",
       "3     0.0    0.0  \n",
       "4     0.0    0.0  \n",
       "\n",
       "[5 rows x 554 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftrain.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftrain.to_csv('train.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftest.to_csv('test.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = EnglishStemmer()\n",
    "\n",
    "@lru_cache(30000)\n",
    "def stem_word(word):\n",
    "    return stemmer.stem(word)\n",
    "\n",
    "def stem_text(text):\n",
    "    tokens = text.split(' ')\n",
    "    return ' '.join(map(stem_word, tokens))\n",
    "\n",
    "def stem_texts(texts):\n",
    "    return [stem_text(text) for text in tqdm(texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c066f2a059644519256936cd37d5bdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=95851), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37e5afb098144ff4835e15fef77a6638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=226998), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dftrain['stemmed_text'] = stem_texts(dftrain['preprocessed_text'])\n",
    "dftest['stemmed_text'] = stem_texts(dftest['preprocessed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_texts = np.array(dftrain['stemmed_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic loss:  0.110571544541\n",
      "severe_toxic loss:  0.0285683947691\n",
      "obscene loss:  0.0617498290975\n",
      "threat loss:  0.0121844556649\n",
      "insult loss:  0.082138605912\n",
      "identity_hate loss:  0.0263262219214\n",
      "Total loss:  0.0535898419844\n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "for i, target in enumerate(targets):\n",
    "    clf = Pipeline([\n",
    "        ('vec', TfidfVectorizer(binary=True, min_df=3, tokenizer=lambda s: s.split(' '))),\n",
    "        ('clf', LogisticRegression(penalty='l1'))\n",
    "    ])\n",
    "    clf.fit(stemmed_texts[train_idx], y[train_idx][:, i])\n",
    "    error = log_loss(y[val_idx][:, i], clf.predict_proba(stemmed_texts[val_idx])[:, 1])\n",
    "    print('{0} loss: '.format(target), error)\n",
    "    errors.append(error)\n",
    "print('Total loss: ', np.array(errors).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic loss:  0.109430148933\n",
      "severe_toxic loss:  0.026859392916\n",
      "obscene loss:  0.0607447117394\n",
      "threat loss:  0.0118839611236\n",
      "insult loss:  0.0781660489122\n",
      "identity_hate loss:  0.0260515706028\n",
      "Total loss:  0.0521893057045\n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "for i, target in enumerate(targets):\n",
    "    clf = Pipeline([\n",
    "        ('vec', FeatureUnion([\n",
    "            ('tfidf', TransformPipeline([\n",
    "                ('text', FunctionTransformer(\n",
    "                    lambda X: X[:, 0],\n",
    "                    validate=False\n",
    "                )),\n",
    "                ('vec', TfidfVectorizer(binary=True, min_df=3, tokenizer=lambda s: s.split(' ')))\n",
    "            ])),\n",
    "            ('levenshtein', TransformPipeline([\n",
    "                ('features', FunctionTransformer(\n",
    "                    lambda X: np.array(X[:, 1:1+len(levenshtein_features)]).astype(np.float),\n",
    "                    validate=False\n",
    "                )),\n",
    "            ])),\n",
    "        ])),\n",
    "        ('clf', LogisticRegression(penalty='l1'))\n",
    "    ])\n",
    "    X = np.array(dftrain[['stemmed_text'] + levenshtein_features])\n",
    "    clf.fit(X[train_idx], y[train_idx][:, i])\n",
    "    error = log_loss(y[val_idx][:, i], clf.predict_proba(X[val_idx])[:, 1])\n",
    "    print('{0} loss: '.format(target), error)\n",
    "    errors.append(error)\n",
    "print('Total loss: ', np.array(errors).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic loss:  0.107130806604\n",
      "severe_toxic loss:  0.0263669424172\n",
      "obscene loss:  0.0586768704664\n",
      "threat loss:  0.0119581965625\n",
      "insult loss:  0.0776366380974\n",
      "identity_hate loss:  0.0255054268334\n",
      "Total loss:  0.0512124801636\n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "for i, target in enumerate(targets):\n",
    "    clf = Pipeline([\n",
    "        ('vec', FeatureUnion([\n",
    "            ('tfidf', TransformPipeline([\n",
    "                ('text', FunctionTransformer(\n",
    "                    lambda X: X[:, 0],\n",
    "                    validate=False\n",
    "                )),\n",
    "                ('vec', TfidfVectorizer(binary=True, min_df=3, tokenizer=lambda s: s.split(' ')))\n",
    "            ])),\n",
    "            ('levenshtein', TransformPipeline([\n",
    "                ('features', FunctionTransformer(\n",
    "                    lambda X: np.array(X[:, 1:1+len(levenshtein_features)]).astype(np.float),\n",
    "                    validate=False\n",
    "                )),\n",
    "                ('idf', TfidfTransformer()),\n",
    "            ])),\n",
    "        ])),\n",
    "        ('clf', LogisticRegression(penalty='l1'))\n",
    "    ])\n",
    "    X = np.array(dftrain[['stemmed_text'] + levenshtein_features])\n",
    "    clf.fit(X[train_idx], y[train_idx][:, i])\n",
    "    error = log_loss(y[val_idx][:, i], clf.predict_proba(X[val_idx])[:, 1])\n",
    "    print('{0} loss: '.format(target), error)\n",
    "    errors.append(error)\n",
    "print('Total loss: ', np.array(errors).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toxic loss:  0.100878550564\n",
      "severe_toxic loss:  0.0255812745147\n",
      "obscene loss:  0.0567030156597\n",
      "threat loss:  0.0117274833456\n",
      "insult loss:  0.0754305249903\n",
      "identity_hate loss:  0.0250575666985\n",
      "Total loss:  0.0492297359622\n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "for i, target in enumerate(targets):\n",
    "    clf = Pipeline([\n",
    "        ('vec', FeatureUnion([\n",
    "            ('tfidf', TransformPipeline([\n",
    "                ('text', FunctionTransformer(\n",
    "                    lambda X: X[:, 0],\n",
    "                    validate=False\n",
    "                )),\n",
    "                ('vec', TfidfVectorizer(binary=True, min_df=3, tokenizer=lambda s: s.split(' ')))\n",
    "            ])),\n",
    "            ('chars', TransformPipeline([\n",
    "                ('text', FunctionTransformer(lambda X: X[:, 1], validate=False)),\n",
    "                ('vec', TfidfVectorizer(analyzer='char', ngram_range=(1, 3), lowercase=False)),\n",
    "            ])),\n",
    "            ('levenshtein', TransformPipeline([\n",
    "                ('features', FunctionTransformer(\n",
    "                    lambda X: np.array(X[:, 2:2+len(levenshtein_features)]).astype(np.float),\n",
    "                    validate=False\n",
    "                )),\n",
    "                ('idf', TfidfTransformer()),\n",
    "            ])),\n",
    "        ])),\n",
    "        ('clf', LogisticRegression(penalty='l1'))\n",
    "    ])\n",
    "    X = np.array(dftrain[['stemmed_text', 'comment_text'] + levenshtein_features])\n",
    "    clf.fit(X[train_idx], y[train_idx][:, i])\n",
    "    error = log_loss(y[val_idx][:, i], clf.predict_proba(X[val_idx])[:, 1])\n",
    "    print('{0} loss: '.format(target), error)\n",
    "    errors.append(error)\n",
    "print('Total loss: ', np.array(errors).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:kaggle-toxic]",
   "language": "python",
   "name": "conda-env-kaggle-toxic-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
